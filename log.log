04/10 17:32:11:  ======================== New Round =============================
04/10 17:32:11:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 17:32:11:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 17:41:12:  #RES: f1:48.771, precision:49.749, recall:47.830, loss:5.942 at 2
04/10 17:41:12:  Best: f1:48.771, precision:49.749, recall:47.830, loss:5.942 at 2
04/10 17:41:12:  ---------
04/10 17:50:10:  #RES: f1:50.370, precision:48.442, recall:52.459, loss:8.647 at 4
04/10 17:50:10:  Best: f1:50.370, precision:48.442, recall:52.459, loss:8.647 at 4
04/10 17:50:10:  ---------
04/10 17:59:08:  #RES: f1:50.763, precision:50.236, recall:51.302, loss:9.801 at 6
04/10 17:59:08:  Best: f1:50.763, precision:50.236, recall:51.302, loss:9.801 at 6
04/10 17:59:08:  ---------
04/10 18:08:06:  #RES: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:08:06:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:08:06:  ---------
04/10 18:14:43:  #RES: f1:43.884, precision:42.534, recall:45.323, loss:12.933 at 11
04/10 18:14:43:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:14:43:  ---------
04/10 18:18:27:  #RES: f1:46.227, precision:45.514, recall:46.962, loss:14.191 at 13
04/10 18:18:27:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:18:27:  ---------
04/10 18:22:11:  #RES: f1:60.079, precision:61.284, recall:58.920, loss:17.107 at 15
04/10 18:22:11:  Best: f1:60.079, precision:61.284, recall:58.920, loss:17.107 at 15
04/10 18:22:11:  ---------
04/10 18:25:55:  #RES: f1:62.553, precision:60.708, recall:64.513, loss:15.253 at 18
04/10 18:25:55:  Best: f1:62.553, precision:60.708, recall:64.513, loss:15.253 at 18
04/10 18:25:55:  ---------
04/10 18:29:40:  #RES: f1:65.911, precision:64.570, recall:67.310, loss:18.799 at 20
04/10 18:29:40:  Best: f1:65.911, precision:64.570, recall:67.310, loss:18.799 at 20
04/10 18:29:40:  ---------
04/10 18:33:24:  #RES: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:33:24:  Best: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:33:24:  ---------
04/10 18:37:08:  #RES: f1:64.921, precision:64.098, recall:65.767, loss:18.911 at 25
04/10 18:37:08:  Best: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:37:08:  ---------
04/10 18:40:53:  #RES: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:40:53:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:40:53:  ---------
04/10 18:44:37:  #RES: f1:65.839, precision:65.246, recall:66.442, loss:20.193 at 29
04/10 18:44:37:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:44:37:  ---------
04/10 18:48:21:  #RES: f1:66.321, precision:64.825, recall:67.888, loss:15.137 at 31
04/10 18:48:21:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:48:21:  ---------
04/10 18:52:06:  #RES: f1:67.137, precision:65.593, recall:68.756, loss:41.115 at 34
04/10 18:52:06:  Best: f1:67.137, precision:65.593, recall:68.756, loss:41.115 at 34
04/10 18:52:06:  ---------
04/10 18:55:50:  #RES: f1:67.325, precision:65.688, recall:69.045, loss:27.434 at 36
04/10 18:55:50:  Best: f1:67.325, precision:65.688, recall:69.045, loss:27.434 at 36
04/10 18:55:50:  ---------
04/10 18:59:35:  #RES: f1:67.482, precision:65.901, recall:69.142, loss:141.030 at 38
04/10 18:59:35:  Best: f1:67.482, precision:65.901, recall:69.142, loss:141.030 at 38
04/10 18:59:35:  ---------
04/10 19:03:19:  #RES: f1:67.485, precision:66.081, recall:68.949, loss:59.071 at 40
04/10 19:03:19:  Best: f1:67.485, precision:66.081, recall:68.949, loss:59.071 at 40
04/10 19:03:19:  ---------
04/10 19:07:03:  #RES: f1:67.958, precision:66.636, recall:69.335, loss:105.362 at 43
04/10 19:07:03:  Best: f1:67.958, precision:66.636, recall:69.335, loss:105.362 at 43
04/10 19:07:03:  ---------
04/10 19:10:47:  #RES: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:10:47:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:10:47:  ---------
04/10 19:14:49:  #RES: f1:67.045, precision:65.860, recall:68.274, loss:39.363 at 47
04/10 19:14:49:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:14:49:  ---------
04/10 19:18:12:  ======================== New Round =============================
04/10 19:18:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:18:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:19:05:  ======================== New Round =============================
04/10 19:19:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:19:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:19:34:  #RES: f1:67.551, precision:66.387, recall:68.756, loss:182.905 at 50
04/10 19:19:34:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:19:34:  ---------
04/10 19:23:19:  #RES: f1:66.698, precision:65.549, recall:67.888, loss:267.859 at 52
04/10 19:23:19:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:23:19:  ---------
04/10 19:27:03:  #RES: f1:66.540, precision:65.424, recall:67.695, loss:1039.897 at 54
04/10 19:27:03:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:27:03:  ---------
04/10 19:30:48:  #RES: f1:63.218, precision:62.243, recall:64.224, loss:262.333 at 56
04/10 19:30:48:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:30:48:  ---------
04/10 19:32:24:  ======================== New Round =============================
04/10 19:32:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:32:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:03:  ======================== New Round =============================
04/10 19:33:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:35:  ======================== New Round =============================
04/10 19:33:35:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:35:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:56:  ======================== New Round =============================
04/10 19:33:56:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:56:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:34:44:  #RES: f1:66.667, precision:65.760, recall:67.599, loss:646.981 at 59
04/10 19:34:44:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:34:44:  ---------
04/10 19:36:15:  ======================== New Round =============================
04/10 19:36:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:36:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:36:45:  ======================== New Round =============================
04/10 19:36:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:36:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:37:14:  ======================== New Round =============================
04/10 19:37:14:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:37:14:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:37:36:  ======================== New Round =============================
04/10 19:37:36:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:37:36:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:38:41:  #RES: f1:66.255, precision:65.323, recall:67.213, loss:352.420 at 61
04/10 19:38:41:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:38:41:  ---------
04/10 19:39:20:  ======================== New Round =============================
04/10 19:39:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:39:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:40:53:  ======================== New Round =============================
04/10 19:40:53:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:40:53:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:43:56:  #RES: f1:66.540, precision:65.604, recall:67.502, loss:344.437 at 63
04/10 19:43:56:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:43:56:  ---------
04/10 19:43:57:  ======================== New Round =============================
04/10 19:43:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:43:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:46:14:  #RES: f1:50.212, precision:49.079, recall:51.398, loss:5.935 at 2
04/10 19:46:14:  Best: f1:50.212, precision:49.079, recall:51.398, loss:5.935 at 2
04/10 19:46:14:  ---------
04/10 19:47:17:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 19:47:17:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 19:47:17:  ---------
04/10 19:50:34:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:50:34:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:50:34:  ---------
04/10 19:53:35:  #RES: f1:51.637, precision:50.136, recall:53.230, loss:7.654 at 4
04/10 19:53:35:  Best: f1:51.637, precision:50.136, recall:53.230, loss:7.654 at 4
04/10 19:53:35:  ---------
04/10 19:53:43:  ======================== New Round =============================
04/10 19:53:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:53:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:53:53:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/10 19:53:53:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:53:53:  ---------
04/10 19:55:27:  #RES: f1:66.475, precision:66.126, recall:66.827, loss:504.793 at 65
04/10 19:55:27:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:55:27:  ---------
04/10 19:55:37:  ======================== New Round =============================
04/10 19:55:37:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:55:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:56:06:  ======================== New Round =============================
04/10 19:56:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:56:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:57:12:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/10 19:57:12:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:57:12:  ---------
04/10 19:57:45:  ======================== New Round =============================
04/10 19:57:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:57:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:58:58:  ======================== New Round =============================
04/10 19:58:58:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:58:58:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:00:33:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:00:33:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:00:33:  ---------
04/10 20:00:59:  #RES: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:00:59:  Best: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:00:59:  ---------
04/10 20:03:49:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/10 20:03:49:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:03:49:  ---------
04/10 20:05:07:  ======================== New Round =============================
04/10 20:05:07:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:05:07:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:05:54:  ======================== New Round =============================
04/10 20:05:54:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:05:54:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:06:34:  ======================== New Round =============================
04/10 20:06:34:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:06:34:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:07:11:  #RES: f1:66.604, precision:64.927, recall:68.370, loss:3636.930 at 68
04/10 20:07:11:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:07:11:  ---------
04/10 20:07:15:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/10 20:07:15:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:07:15:  ---------
04/10 20:07:44:  ======================== New Round =============================
04/10 20:07:44:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:07:44:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:08:27:  #RES: f1:54.267, precision:53.956, recall:54.581, loss:9.251 at 9
04/10 20:08:27:  Best: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:08:27:  ---------
04/10 20:10:34:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/10 20:10:34:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:10:34:  ---------
04/10 20:13:52:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/10 20:13:52:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:13:52:  ---------
04/10 20:15:48:  #RES: f1:56.303, precision:54.570, recall:58.149, loss:10.840 at 11
04/10 20:15:48:  Best: f1:56.303, precision:54.570, recall:58.149, loss:10.840 at 11
04/10 20:15:48:  ---------
04/10 20:17:08:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/10 20:17:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:17:08:  ---------
04/10 20:17:28:  ======================== New Round =============================
04/10 20:17:28:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:17:28:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:18:12:  ======================== New Round =============================
04/10 20:18:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:18:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:18:48:  #RES: f1:66.089, precision:62.662, recall:69.913, loss:592.917 at 70
04/10 20:18:48:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:18:48:  ---------
04/10 20:20:28:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/10 20:20:28:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/10 20:20:28:  ---------
04/10 20:21:04:  ======================== New Round =============================
04/10 20:21:04:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:21:04:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:23:16:  #RES: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:23:16:  Best: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:23:16:  ---------
04/10 20:23:20:  ======================== New Round =============================
04/10 20:23:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:23:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:23:51:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:23:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:23:51:  ---------
04/10 20:24:41:  ======================== New Round =============================
04/10 20:24:41:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:24:41:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:25:13:  ======================== New Round =============================
04/10 20:25:13:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:25:13:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:25:58:  ======================== New Round =============================
04/10 20:25:58:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:25:58:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:27:17:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/10 20:27:17:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:27:17:  ---------
04/10 20:29:15:  ======================== New Round =============================
04/10 20:29:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:29:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:29:49:  ======================== New Round =============================
04/10 20:29:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:29:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:18:  ======================== New Round =============================
04/10 20:30:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:30:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:35:  #RES: f1:58.624, precision:57.327, recall:59.981, loss:912.953 at 72
04/10 20:30:35:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:30:35:  ---------
04/10 20:30:39:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/10 20:30:39:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:30:39:  ---------
04/10 20:30:40:  ======================== New Round =============================
04/10 20:30:40:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:30:40:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:47:  #RES: f1:59.664, precision:59.351, recall:59.981, loss:14.277 at 15
04/10 20:30:47:  Best: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:30:47:  ---------
04/10 20:31:57:  #RES: f1:61.657, precision:61.598, recall:61.716, loss:0.183 at 2
04/10 20:31:57:  Best: f1:61.657, precision:61.598, recall:61.716, loss:0.183 at 2
04/10 20:31:57:  ---------
04/10 20:33:13:  #RES: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:33:13:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:33:13:  ---------
04/10 20:33:57:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/10 20:33:57:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:33:57:  ---------
04/10 20:34:30:  #RES: f1:62.620, precision:62.560, recall:62.681, loss:0.276 at 6
04/10 20:34:30:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:34:30:  ---------
04/10 20:35:46:  #RES: f1:64.004, precision:63.881, recall:64.127, loss:0.333 at 9
04/10 20:35:46:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:35:46:  ---------
04/10 20:37:03:  #RES: f1:64.779, precision:64.470, recall:65.092, loss:0.389 at 11
04/10 20:37:03:  Best: f1:64.779, precision:64.470, recall:65.092, loss:0.389 at 11
04/10 20:37:03:  ---------
04/10 20:37:14:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/10 20:37:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:37:14:  ---------
04/10 20:38:06:  #RES: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:38:06:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:38:06:  ---------
04/10 20:38:20:  #RES: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:38:20:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:38:20:  ---------
04/10 20:39:37:  #RES: f1:64.821, precision:64.178, recall:65.477, loss:0.437 at 15
04/10 20:39:37:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:39:37:  ---------
04/10 20:40:31:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/10 20:40:31:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:40:31:  ---------
04/10 20:40:54:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.435 at 18
04/10 20:40:54:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:40:54:  ---------
04/10 20:42:07:  #RES: f1:61.458, precision:60.372, recall:62.584, loss:412.248 at 75
04/10 20:42:07:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:42:07:  ---------
04/10 20:42:12:  #RES: f1:65.361, precision:64.773, recall:65.959, loss:0.469 at 20
04/10 20:42:12:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:42:12:  ---------
04/10 20:43:29:  #RES: f1:66.763, precision:66.699, recall:66.827, loss:0.457 at 22
04/10 20:43:29:  Best: f1:66.763, precision:66.699, recall:66.827, loss:0.457 at 22
04/10 20:43:29:  ---------
04/10 20:43:47:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/10 20:43:47:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:43:47:  ---------
04/10 20:44:46:  #RES: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:44:46:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:44:46:  ---------
04/10 20:45:24:  #RES: f1:64.279, precision:65.435, recall:63.163, loss:18.707 at 20
04/10 20:45:24:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:45:24:  ---------
04/10 20:46:03:  #RES: f1:66.412, precision:65.814, recall:67.020, loss:0.460 at 27
04/10 20:46:03:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:46:03:  ---------
04/10 20:47:03:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/10 20:47:03:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/10 20:47:03:  ---------
04/10 20:47:20:  #RES: f1:66.378, precision:66.124, recall:66.635, loss:0.489 at 29
04/10 20:47:20:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:47:20:  ---------
04/10 20:48:37:  #RES: f1:66.825, precision:65.886, recall:67.792, loss:0.491 at 31
04/10 20:48:37:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:48:37:  ---------
04/10 20:49:54:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.501 at 34
04/10 20:49:54:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:49:54:  ---------
04/10 20:50:21:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:50:21:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:50:21:  ---------
04/10 20:51:11:  #RES: f1:66.216, precision:66.376, recall:66.056, loss:0.476 at 36
04/10 20:51:11:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:51:11:  ---------
04/10 20:52:29:  #RES: f1:65.357, precision:64.952, recall:65.767, loss:0.500 at 38
04/10 20:52:29:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:52:29:  ---------
04/10 20:52:43:  #RES: f1:63.962, precision:62.604, recall:65.381, loss:16.208 at 22
04/10 20:52:43:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:52:43:  ---------
04/10 20:53:37:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/10 20:53:37:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:53:37:  ---------
04/10 20:53:39:  #RES: f1:64.389, precision:64.265, recall:64.513, loss:115.893 at 77
04/10 20:53:39:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:53:39:  ---------
04/10 20:53:46:  #RES: f1:66.088, precision:66.120, recall:66.056, loss:0.507 at 40
04/10 20:53:46:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:53:46:  ---------
04/10 20:55:03:  #RES: f1:65.811, precision:64.916, recall:66.731, loss:0.526 at 43
04/10 20:55:03:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:55:03:  ---------
04/10 20:56:23:  #RES: f1:66.030, precision:65.436, recall:66.635, loss:0.525 at 45
04/10 20:56:23:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:56:23:  ---------
04/10 20:56:58:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 20:56:58:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 20:56:58:  ---------
04/10 20:57:48:  #RES: f1:66.311, precision:66.764, recall:65.863, loss:0.554 at 47
04/10 20:57:48:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:57:48:  ---------
04/10 20:59:09:  #RES: f1:66.828, precision:67.121, recall:66.538, loss:0.539 at 49
04/10 20:59:09:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:59:09:  ---------
04/10 21:00:12:  #RES: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:00:12:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:00:12:  ---------
04/10 21:00:21:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/10 21:00:21:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:00:21:  ---------
04/10 21:00:32:  #RES: f1:67.144, precision:66.415, recall:67.888, loss:0.524 at 52
04/10 21:00:32:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:00:32:  ---------
04/10 21:01:49:  #RES: f1:66.408, precision:66.764, recall:66.056, loss:0.542 at 54
04/10 21:01:49:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:01:49:  ---------
04/10 21:03:13:  #RES: f1:66.088, precision:66.120, recall:66.056, loss:0.540 at 56
04/10 21:03:13:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:03:13:  ---------
04/10 21:03:47:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/10 21:03:47:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:03:47:  ---------
04/10 21:04:30:  #RES: f1:67.113, precision:66.635, recall:67.599, loss:0.562 at 59
04/10 21:04:30:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:04:30:  ---------
04/10 21:05:24:  #RES: f1:66.382, precision:65.388, recall:67.406, loss:484.777 at 79
04/10 21:05:24:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:05:24:  ---------
04/10 21:05:47:  #RES: f1:67.015, precision:66.073, recall:67.985, loss:0.546 at 61
04/10 21:05:47:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:05:47:  ---------
04/10 21:07:11:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/10 21:07:11:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:07:11:  ---------
04/10 21:07:12:  #RES: f1:66.093, precision:65.560, recall:66.635, loss:0.567 at 63
04/10 21:07:12:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:07:12:  ---------
04/10 21:07:43:  #RES: f1:64.353, precision:62.648, recall:66.152, loss:22.313 at 27
04/10 21:07:43:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:07:43:  ---------
04/10 21:08:37:  #RES: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:08:37:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:08:37:  ---------
04/10 21:10:03:  #RES: f1:66.603, precision:66.379, recall:66.827, loss:0.542 at 68
04/10 21:10:03:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:10:03:  ---------
04/10 21:10:40:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/10 21:10:40:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:10:40:  ---------
04/10 21:11:34:  #RES: f1:65.538, precision:63.945, recall:67.213, loss:0.572 at 70
04/10 21:11:34:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:11:34:  ---------
04/10 21:13:06:  #RES: f1:65.897, precision:65.739, recall:66.056, loss:0.589 at 72
04/10 21:13:06:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:13:06:  ---------
04/10 21:14:02:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/10 21:14:02:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:14:02:  ---------
04/10 21:14:37:  #RES: f1:66.346, precision:66.250, recall:66.442, loss:0.552 at 74
04/10 21:14:37:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:14:37:  ---------
04/10 21:15:20:  #RES: f1:64.389, precision:64.265, recall:64.513, loss:24.936 at 29
04/10 21:15:20:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:15:20:  ---------
04/10 21:16:08:  #RES: f1:66.060, precision:65.589, recall:66.538, loss:0.563 at 77
04/10 21:16:08:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:16:08:  ---------
04/10 21:17:10:  #RES: f1:66.350, precision:65.327, recall:67.406, loss:362.054 at 81
04/10 21:17:10:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:17:10:  ---------
04/10 21:17:21:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:17:21:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:17:21:  ---------
04/10 21:17:39:  #RES: f1:65.995, precision:65.649, recall:66.345, loss:0.559 at 79
04/10 21:17:39:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:17:39:  ---------
04/10 21:19:09:  #RES: f1:66.924, precision:66.924, recall:66.924, loss:0.585 at 81
04/10 21:19:09:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:19:09:  ---------
04/10 21:20:39:  #RES: f1:66.828, precision:67.022, recall:66.635, loss:0.585 at 84
04/10 21:20:39:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:20:39:  ---------
04/10 21:20:41:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/10 21:20:41:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:20:41:  ---------
04/10 21:22:10:  #RES: f1:66.890, precision:66.476, recall:67.310, loss:0.584 at 86
04/10 21:22:10:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:22:10:  ---------
04/10 21:22:43:  #RES: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:22:43:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:22:43:  ---------
04/10 21:23:41:  #RES: f1:65.968, precision:65.221, recall:66.731, loss:0.576 at 88
04/10 21:23:41:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:23:41:  ---------
04/10 21:24:00:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/10 21:24:00:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:24:00:  ---------
04/10 21:25:11:  #RES: f1:66.380, precision:65.752, recall:67.020, loss:0.566 at 90
04/10 21:25:11:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:25:11:  ---------
04/10 21:26:32:  #RES: f1:67.344, precision:67.573, recall:67.117, loss:0.566 at 93
04/10 21:26:32:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:26:32:  ---------
04/10 21:27:18:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/10 21:27:18:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:27:18:  ---------
04/10 21:27:49:  #RES: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:27:49:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:27:49:  ---------
04/10 21:28:42:  #RES: f1:65.966, precision:65.403, recall:66.538, loss:820.967 at 84
04/10 21:28:42:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:28:42:  ---------
04/10 21:29:06:  #RES: f1:67.212, precision:67.115, recall:67.310, loss:0.575 at 97
04/10 21:29:06:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:29:06:  ---------
04/10 21:30:02:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:13.487 at 34
04/10 21:30:02:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:30:02:  ---------
04/10 21:30:23:  #RES: f1:67.084, precision:66.955, recall:67.213, loss:0.574 at 99
04/10 21:30:23:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:30:23:  ---------
04/10 21:30:34:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/10 21:30:34:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:30:34:  ---------
04/10 21:34:01:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:34:01:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:34:01:  ---------
04/10 21:37:33:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/10 21:37:33:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:37:33:  ---------
04/10 21:37:46:  #RES: f1:65.748, precision:64.794, recall:66.731, loss:274.803 at 36
04/10 21:37:46:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:37:46:  ---------
04/10 21:40:25:  #RES: f1:58.049, precision:57.854, recall:58.245, loss:685.411 at 86
04/10 21:40:25:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:40:25:  ---------
04/10 21:41:00:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/10 21:41:00:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:41:00:  ---------
04/10 21:44:28:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/10 21:44:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:44:28:  ---------
04/10 21:45:26:  #RES: f1:65.717, precision:64.733, recall:66.731, loss:54.442 at 38
04/10 21:45:26:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:45:26:  ---------
04/10 21:47:53:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/10 21:47:53:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:47:53:  ---------
04/10 21:51:21:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/10 21:51:21:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:51:21:  ---------
04/10 21:52:09:  #RES: f1:64.193, precision:64.162, recall:64.224, loss:300.165 at 88
04/10 21:52:09:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:52:09:  ---------
04/10 21:53:03:  #RES: f1:65.781, precision:64.766, recall:66.827, loss:776.498 at 40
04/10 21:53:03:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:53:03:  ---------
04/10 21:54:46:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/10 21:54:46:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:54:46:  ---------
04/10 21:58:15:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/10 21:58:15:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:58:15:  ---------
04/10 22:00:45:  #RES: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:00:45:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:00:45:  ---------
04/10 22:01:44:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/10 22:01:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:01:44:  ---------
04/10 22:03:55:  #RES: f1:67.077, precision:65.831, recall:68.370, loss:312.426 at 90
04/10 22:03:55:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:03:55:  ---------
04/10 22:05:13:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/10 22:05:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:05:13:  ---------
04/10 22:08:34:  #RES: f1:65.564, precision:64.259, recall:66.924, loss:98.427 at 45
04/10 22:08:34:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:08:34:  ---------
04/10 22:08:44:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/10 22:08:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:08:44:  ---------
04/10 22:14:10:  #RES: f1:67.268, precision:66.292, recall:68.274, loss:317.774 at 93
04/10 22:14:10:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:14:10:  ---------
04/10 22:14:32:  #RES: f1:64.672, precision:63.704, recall:65.670, loss:192.335 at 47
04/10 22:14:32:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:14:32:  ---------
04/10 22:19:10:  #RES: f1:64.133, precision:63.202, recall:65.092, loss:171.023 at 50
04/10 22:19:10:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:19:10:  ---------
04/10 22:21:18:  #RES: f1:67.299, precision:66.170, recall:68.467, loss:1150.949 at 95
04/10 22:21:18:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:21:18:  ---------
04/10 22:23:47:  #RES: f1:64.458, precision:63.290, recall:65.670, loss:165.365 at 52
04/10 22:23:47:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:23:47:  ---------
04/10 22:28:24:  #RES: f1:67.047, precision:66.043, recall:68.081, loss:429.561 at 97
04/10 22:28:24:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:28:24:  ---------
04/10 22:28:25:  #RES: f1:63.671, precision:62.488, recall:64.899, loss:276.744 at 54
04/10 22:28:25:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:28:25:  ---------
04/10 22:33:03:  #RES: f1:63.758, precision:63.394, recall:64.127, loss:1108.264 at 56
04/10 22:33:03:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:33:03:  ---------
04/10 22:35:52:  #RES: f1:65.318, precision:65.255, recall:65.381, loss:270.836 at 59
04/10 22:35:52:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:35:52:  ---------
04/10 22:37:36:  #RES: f1:58.055, precision:56.593, recall:59.595, loss:320.622 at 61
04/10 22:37:36:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:37:36:  ---------
04/10 22:39:20:  #RES: f1:63.394, precision:62.677, recall:64.127, loss:325.205 at 63
04/10 22:39:20:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:39:20:  ---------
04/10 22:41:04:  #RES: f1:64.995, precision:63.788, recall:66.249, loss:235.938 at 65
04/10 22:41:04:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:41:04:  ---------
04/10 22:42:47:  #RES: f1:64.348, precision:64.472, recall:64.224, loss:354.782 at 68
04/10 22:42:47:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:42:47:  ---------
04/10 22:44:32:  #RES: f1:63.504, precision:61.905, recall:65.188, loss:360.767 at 70
04/10 22:44:32:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:44:32:  ---------
04/10 22:46:17:  #RES: f1:64.596, precision:64.015, recall:65.188, loss:266.620 at 72
04/10 22:46:17:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:46:17:  ---------
04/10 22:48:02:  #RES: f1:63.524, precision:62.747, recall:64.320, loss:1834.763 at 75
04/10 22:48:02:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:48:02:  ---------
04/10 22:49:48:  #RES: f1:64.865, precision:63.806, recall:65.959, loss:398.005 at 77
04/10 22:49:48:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:49:48:  ---------
04/10 22:51:33:  #RES: f1:62.782, precision:61.228, recall:64.417, loss:1043.555 at 79
04/10 22:51:33:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:51:33:  ---------
04/10 22:53:18:  #RES: f1:63.163, precision:62.595, recall:63.742, loss:485.795 at 81
04/10 22:53:18:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:53:18:  ---------
04/10 22:55:03:  #RES: f1:62.846, precision:62.252, recall:63.452, loss:281.170 at 84
04/10 22:55:03:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:55:03:  ---------
04/10 22:56:48:  #RES: f1:64.538, precision:64.757, recall:64.320, loss:234.235 at 86
04/10 22:56:48:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:56:48:  ---------
04/10 22:58:33:  #RES: f1:64.697, precision:64.981, recall:64.417, loss:227.272 at 88
04/10 22:58:33:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:58:33:  ---------
04/10 23:00:19:  #RES: f1:64.906, precision:64.627, recall:65.188, loss:230.439 at 91
04/10 23:00:19:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:00:19:  ---------
04/10 23:02:04:  #RES: f1:65.167, precision:64.672, recall:65.670, loss:211.422 at 93
04/10 23:02:04:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:02:04:  ---------
04/10 23:03:49:  #RES: f1:64.748, precision:63.401, recall:66.152, loss:900.709 at 95
04/10 23:03:49:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:03:49:  ---------
04/10 23:05:34:  #RES: f1:64.194, precision:63.321, recall:65.092, loss:200.223 at 97
04/10 23:05:34:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:05:34:  ---------
04/10 23:46:47:  ======================== New Round =============================
04/10 23:46:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 23:46:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:47:30:  ======================== New Round =============================
04/10 23:47:30:  2015, add_gan:True, add_gan_loss: True, add_gpt: False, text_model deberta
04/10 23:47:30:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=True, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:48:18:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 23:48:18:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 23:48:18:  ---------
04/10 23:49:55:  ======================== New Round =============================
04/10 23:49:55:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 23:49:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:50:06:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:50:06:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:50:06:  ---------
04/10 23:50:45:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/10 23:50:45:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/10 23:50:45:  ---------
04/10 23:51:35:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/10 23:51:35:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/10 23:51:35:  ---------
04/10 23:52:24:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:52:24:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:52:24:  ---------
04/10 23:53:05:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/10 23:53:05:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:53:05:  ---------
04/10 23:53:13:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/10 23:53:13:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:53:13:  ---------
04/10 23:54:03:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/10 23:54:03:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/10 23:54:03:  ---------
04/10 23:54:53:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/10 23:54:53:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/10 23:54:53:  ---------
04/10 23:55:43:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:55:43:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:55:43:  ---------
04/10 23:56:05:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/10 23:56:05:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:56:05:  ---------
04/10 23:56:32:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/10 23:56:32:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:56:32:  ---------
04/10 23:57:22:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/10 23:57:22:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/10 23:57:22:  ---------
04/10 23:58:12:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/10 23:58:12:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/10 23:58:12:  ---------
04/10 23:59:01:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:01:  ---------
04/10 23:59:05:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 23:59:05:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 23:59:05:  ---------
04/10 23:59:51:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/10 23:59:51:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:51:  ---------
04/11 00:00:40:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/11 00:00:40:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:00:40:  ---------
04/11 00:01:30:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/11 00:01:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:01:30:  ---------
04/11 00:02:04:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/11 00:02:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:02:04:  ---------
04/11 00:02:19:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/11 00:02:19:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:02:19:  ---------
04/11 00:03:09:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/11 00:03:09:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:03:09:  ---------
04/11 00:03:59:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/11 00:03:59:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:03:59:  ---------
04/11 00:04:49:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/11 00:04:49:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:04:49:  ---------
04/11 00:05:04:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/11 00:05:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:05:04:  ---------
04/11 00:05:37:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/11 00:05:37:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:05:37:  ---------
04/11 00:06:26:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/11 00:06:26:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:06:26:  ---------
04/11 00:07:14:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/11 00:07:14:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:07:14:  ---------
04/11 00:07:54:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.211 at 2
04/11 00:07:54:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.211 at 2
04/11 00:07:54:  ---------
04/11 00:07:59:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/11 00:07:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:07:59:  ---------
04/11 00:08:02:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/11 00:08:02:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:08:02:  ---------
04/11 00:08:52:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/11 00:08:52:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:08:52:  ---------
04/11 00:09:42:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/11 00:09:42:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:09:42:  ---------
04/11 00:10:31:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/11 00:10:31:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:10:31:  ---------
04/11 00:10:59:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/11 00:10:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:10:59:  ---------
04/11 00:11:21:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/11 00:11:21:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:11:21:  ---------
04/11 00:12:11:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/11 00:12:11:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:12:11:  ---------
04/11 00:13:00:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/11 00:13:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:13:00:  ---------
04/11 00:13:50:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/11 00:13:50:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:13:50:  ---------
04/11 00:13:59:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/11 00:13:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:13:59:  ---------
04/11 00:14:40:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/11 00:14:40:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:14:40:  ---------
04/11 00:15:30:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/11 00:15:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:15:30:  ---------
04/11 00:16:20:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:16:20:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:16:20:  ---------
04/11 00:16:58:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/11 00:16:58:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/11 00:16:58:  ---------
04/11 00:17:09:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/11 00:17:09:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:17:09:  ---------
04/11 00:17:59:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/11 00:17:59:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:17:59:  ---------
04/11 00:18:48:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/11 00:18:48:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:18:48:  ---------
04/11 00:19:38:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/11 00:19:38:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:19:38:  ---------
04/11 00:19:58:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:19:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:19:58:  ---------
04/11 00:20:28:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:20:28:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:20:28:  ---------
04/11 00:21:17:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/11 00:21:17:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:21:17:  ---------
04/11 00:22:07:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/11 00:22:07:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:22:07:  ---------
04/11 00:22:56:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/11 00:22:56:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:22:56:  ---------
04/11 00:22:58:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/11 00:22:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:22:58:  ---------
04/11 00:23:46:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/11 00:23:46:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:23:46:  ---------
04/11 00:24:36:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/11 00:24:36:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:24:36:  ---------
04/11 00:25:26:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/11 00:25:26:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:25:26:  ---------
04/11 00:25:57:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/11 00:25:57:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:25:57:  ---------
04/11 00:26:15:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/11 00:26:15:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:26:15:  ---------
04/11 00:27:42:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/11 00:27:42:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:27:42:  ---------
04/11 00:28:19:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:28:19:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:28:19:  ---------
04/11 00:29:19:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/11 00:29:19:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:29:19:  ---------
04/11 00:30:59:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/11 00:30:59:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:30:59:  ---------
04/11 00:32:38:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/11 00:32:38:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:32:38:  ---------
04/11 00:34:17:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/11 00:34:17:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/11 00:34:17:  ---------
04/11 00:35:56:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:35:56:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:35:56:  ---------
04/11 00:37:35:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/11 00:37:35:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:37:35:  ---------
04/11 00:39:11:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:39:11:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:39:11:  ---------
04/11 00:39:31:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.337 at 6
04/11 00:39:31:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:39:31:  ---------
04/11 00:40:50:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/11 00:40:50:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:40:50:  ---------
04/11 00:42:28:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/11 00:42:28:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:42:28:  ---------
04/11 00:44:08:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/11 00:44:08:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:44:08:  ---------
04/11 00:45:46:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/11 00:45:46:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:45:46:  ---------
04/11 00:47:25:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/11 00:47:25:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:47:25:  ---------
04/11 00:49:04:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:49:04:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:49:04:  ---------
04/11 00:50:40:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/11 00:50:40:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:50:40:  ---------
04/11 00:50:42:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.408 at 9
04/11 00:50:42:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:50:42:  ---------
04/11 00:52:19:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/11 00:52:19:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:52:19:  ---------
04/11 00:53:58:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/11 00:53:58:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:53:58:  ---------
04/11 00:55:37:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/11 00:55:37:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:55:37:  ---------
04/11 00:57:17:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:57:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:57:17:  ---------
04/11 00:58:56:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/11 00:58:56:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:58:56:  ---------
04/11 01:00:35:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/11 01:00:35:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:00:35:  ---------
04/11 01:01:54:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:01:54:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:01:54:  ---------
04/11 01:02:10:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/11 01:02:10:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:02:10:  ---------
04/11 01:03:49:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/11 01:03:49:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:03:49:  ---------
04/11 01:05:28:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/11 01:05:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:05:28:  ---------
04/11 01:07:07:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/11 01:07:07:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:07:07:  ---------
04/11 01:08:47:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/11 01:08:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:08:47:  ---------
04/11 01:10:26:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/11 01:10:26:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:10:26:  ---------
04/11 01:12:04:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/11 01:12:04:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:12:04:  ---------
04/11 01:13:08:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.445 at 13
04/11 01:13:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:13:08:  ---------
04/11 01:13:39:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/11 01:13:39:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:13:39:  ---------
04/11 01:17:14:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.508 at 15
04/11 01:17:14:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:17:14:  ---------
04/11 01:19:48:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.491 at 18
04/11 01:19:48:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:19:48:  ---------
04/11 01:22:22:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.533 at 20
04/11 01:22:22:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:22:22:  ---------
04/11 01:24:55:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.512 at 22
04/11 01:24:55:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:24:55:  ---------
04/11 01:27:28:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.521 at 25
04/11 01:27:28:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.521 at 25
04/11 01:27:28:  ---------
04/11 01:30:03:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:30:03:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:30:03:  ---------
04/11 01:32:37:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.488 at 29
04/11 01:32:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:32:37:  ---------
04/11 01:35:10:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.529 at 31
04/11 01:35:10:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:35:10:  ---------
04/11 01:37:43:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.551 at 34
04/11 01:37:43:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:37:43:  ---------
04/11 01:40:17:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.571 at 36
04/11 01:40:17:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:40:17:  ---------
04/11 01:42:51:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.547 at 38
04/11 01:42:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:42:51:  ---------
04/11 01:45:24:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.550 at 40
04/11 01:45:24:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:45:24:  ---------
04/11 01:47:56:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.561 at 43
04/11 01:47:56:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.561 at 43
04/11 01:47:56:  ---------
04/11 01:50:30:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:50:30:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:50:30:  ---------
04/11 01:53:03:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.581 at 47
04/11 01:53:03:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:53:03:  ---------
04/11 01:55:36:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:55:36:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:55:36:  ---------
04/11 01:58:09:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.609 at 52
04/11 01:58:09:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:58:09:  ---------
04/11 02:00:43:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.547 at 54
04/11 02:00:43:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:00:43:  ---------
04/11 02:03:17:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.602 at 56
04/11 02:03:17:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:03:17:  ---------
04/11 02:05:50:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.626 at 59
04/11 02:05:50:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:05:50:  ---------
04/11 02:08:24:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.604 at 61
04/11 02:08:24:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:08:24:  ---------
04/11 02:10:57:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:10:57:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:10:57:  ---------
04/11 02:13:31:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.572 at 65
04/11 02:13:31:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:13:31:  ---------
04/11 02:16:05:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.529 at 68
04/11 02:16:05:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:16:05:  ---------
04/11 02:18:39:  #RES: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:18:39:  Best: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:18:39:  ---------
04/11 02:21:12:  #RES: f1:67.519, precision:66.326, recall:68.756, loss:0.592 at 72
04/11 02:21:12:  Best: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:21:12:  ---------
04/11 02:23:46:  #RES: f1:68.251, precision:67.291, recall:69.238, loss:0.588 at 75
04/11 02:23:46:  Best: f1:68.251, precision:67.291, recall:69.238, loss:0.588 at 75
04/11 02:23:46:  ---------
04/11 02:26:19:  #RES: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:26:19:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:26:19:  ---------
04/11 02:28:53:  #RES: f1:67.874, precision:67.107, recall:68.660, loss:0.600 at 79
04/11 02:28:53:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:28:53:  ---------
04/11 02:31:26:  #RES: f1:67.023, precision:67.515, recall:66.538, loss:0.618 at 81
04/11 02:31:26:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:31:26:  ---------
04/11 02:33:59:  #RES: f1:66.924, precision:66.924, recall:66.924, loss:0.621 at 84
04/11 02:33:59:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:33:59:  ---------
04/11 02:36:32:  #RES: f1:67.407, precision:67.505, recall:67.310, loss:0.618 at 86
04/11 02:36:32:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:36:32:  ---------
04/11 02:39:07:  #RES: f1:67.771, precision:66.543, recall:69.045, loss:0.642 at 88
04/11 02:39:07:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:39:07:  ---------
04/11 02:41:40:  #RES: f1:68.577, precision:67.928, recall:69.238, loss:0.629 at 90
04/11 02:41:40:  Best: f1:68.577, precision:67.928, recall:69.238, loss:0.629 at 90
04/11 02:41:40:  ---------
04/11 02:44:14:  #RES: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:44:14:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:44:14:  ---------
04/11 02:46:47:  #RES: f1:68.073, precision:67.684, recall:68.467, loss:0.635 at 95
04/11 02:46:47:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:46:47:  ---------
04/11 02:49:21:  #RES: f1:68.393, precision:68.034, recall:68.756, loss:0.639 at 97
04/11 02:49:21:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:49:21:  ---------
04/11 15:34:42:  ======================== New Round =============================
04/11 15:34:42:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:34:42:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:35:16:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:35:16:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:35:16:  ---------
04/11 15:35:57:  ======================== New Round =============================
04/11 15:35:57:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:35:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:36:37:  ======================== New Round =============================
04/11 15:36:37:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:36:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:02:  ======================== New Round =============================
04/11 15:37:02:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:37:02:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:17:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:37:17:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:37:17:  ---------
04/11 15:37:47:  ======================== New Round =============================
04/11 15:37:47:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:37:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:57:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/11 15:37:57:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/11 15:37:57:  ---------
04/11 15:38:03:  #RES: f1:48.740, precision:48.978, recall:48.505, loss:7.979 at 2
04/11 15:38:03:  Best: f1:48.740, precision:48.978, recall:48.505, loss:7.979 at 2
04/11 15:38:03:  ---------
04/11 15:38:43:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:38:43:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:38:43:  ---------
04/11 15:39:28:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/11 15:39:28:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:39:28:  ---------
04/11 15:40:11:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/11 15:40:11:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/11 15:40:11:  ---------
04/11 15:40:57:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/11 15:40:57:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/11 15:40:57:  ---------
04/11 15:41:24:  #RES: f1:20.489, precision:56.034, recall:12.536, loss:7.979 at 2
04/11 15:41:24:  Best: f1:20.489, precision:56.034, recall:12.536, loss:7.979 at 2
04/11 15:41:24:  ---------
04/11 15:41:37:  #RES: f1:50.121, precision:50.291, recall:49.952, loss:6.328 at 4
04/11 15:41:37:  Best: f1:50.121, precision:50.291, recall:49.952, loss:6.328 at 4
04/11 15:41:37:  ---------
04/11 15:41:41:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:41:41:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:41:41:  ---------
04/11 15:42:27:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/11 15:42:27:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:42:27:  ---------
04/11 15:43:08:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/11 15:43:08:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/11 15:43:08:  ---------
04/11 15:43:45:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/11 15:43:45:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/11 15:43:45:  ---------
04/11 15:44:23:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:23:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:23:  ---------
04/11 15:44:33:  #RES: f1:50.168, precision:49.905, recall:50.434, loss:6.328 at 4
04/11 15:44:33:  Best: f1:50.168, precision:49.905, recall:50.434, loss:6.328 at 4
04/11 15:44:33:  ---------
04/11 15:44:44:  #RES: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:44:44:  Best: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:44:44:  ---------
04/11 15:44:59:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/11 15:44:59:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:59:  ---------
04/11 15:45:37:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/11 15:45:37:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:45:37:  ---------
04/11 15:46:15:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/11 15:46:15:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:46:15:  ---------
04/11 15:46:53:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/11 15:46:53:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:46:53:  ---------
04/11 15:47:27:  #RES: f1:50.422, precision:49.043, recall:51.880, loss:6.807 at 6
04/11 15:47:27:  Best: f1:50.422, precision:49.043, recall:51.880, loss:6.807 at 6
04/11 15:47:27:  ---------
04/11 15:47:30:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/11 15:47:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:47:30:  ---------
04/11 15:47:37:  #RES: f1:47.245, precision:46.952, recall:47.541, loss:7.158 at 9
04/11 15:47:37:  Best: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:47:37:  ---------
04/11 15:48:07:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/11 15:48:07:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:48:07:  ---------
04/11 15:48:45:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/11 15:48:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:48:45:  ---------
04/11 15:49:22:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/11 15:49:22:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:49:22:  ---------
04/11 15:50:00:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/11 15:50:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:50:00:  ---------
04/11 15:50:20:  #RES: f1:51.659, precision:50.792, recall:52.555, loss:7.158 at 9
04/11 15:50:20:  Best: f1:51.659, precision:50.792, recall:52.555, loss:7.158 at 9
04/11 15:50:20:  ---------
04/11 15:50:31:  #RES: f1:56.057, precision:55.243, recall:56.895, loss:7.211 at 11
04/11 15:50:31:  Best: f1:56.057, precision:55.243, recall:56.895, loss:7.211 at 11
04/11 15:50:31:  ---------
04/11 15:50:36:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/11 15:50:36:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:50:36:  ---------
04/11 15:51:14:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/11 15:51:14:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:51:14:  ---------
04/11 15:51:52:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/11 15:51:52:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:51:52:  ---------
04/11 15:52:29:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/11 15:52:29:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:52:29:  ---------
04/11 15:53:06:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/11 15:53:06:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:53:06:  ---------
04/11 15:53:14:  #RES: f1:56.159, precision:54.991, recall:57.377, loss:7.211 at 11
04/11 15:53:14:  Best: f1:56.159, precision:54.991, recall:57.377, loss:7.211 at 11
04/11 15:53:14:  ---------
04/11 15:53:26:  #RES: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:53:26:  Best: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:53:26:  ---------
04/11 15:53:43:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/11 15:53:43:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:53:43:  ---------
04/11 15:54:20:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/11 15:54:20:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:54:20:  ---------
04/11 15:54:58:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/11 15:54:58:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:54:58:  ---------
04/11 15:55:35:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/11 15:55:35:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:55:35:  ---------
04/11 15:56:10:  #RES: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:56:10:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:56:10:  ---------
04/11 15:56:12:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/11 15:56:12:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:56:12:  ---------
04/11 15:56:22:  #RES: f1:58.611, precision:57.840, recall:59.402, loss:9.189 at 15
04/11 15:56:22:  Best: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:56:22:  ---------
04/11 15:56:49:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/11 15:56:49:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:56:49:  ---------
04/11 15:57:26:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:57:26:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:57:26:  ---------
04/11 15:58:04:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/11 15:58:04:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:58:04:  ---------
04/11 15:58:41:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/11 15:58:41:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:58:41:  ---------
04/11 15:59:05:  #RES: f1:58.812, precision:57.959, recall:59.691, loss:9.189 at 15
04/11 15:59:05:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:59:05:  ---------
04/11 15:59:17:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/11 15:59:17:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:59:17:  ---------
04/11 15:59:17:  #RES: f1:61.816, precision:60.976, recall:62.681, loss:9.993 at 18
04/11 15:59:17:  Best: f1:61.816, precision:60.976, recall:62.681, loss:9.993 at 18
04/11 15:59:17:  ---------
04/11 15:59:55:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/11 15:59:55:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:59:55:  ---------
04/11 16:00:32:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:00:32:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:00:32:  ---------
04/11 16:01:10:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/11 16:01:10:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:01:10:  ---------
04/11 16:01:47:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/11 16:01:47:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:01:47:  ---------
04/11 16:01:59:  #RES: f1:60.837, precision:59.981, recall:61.716, loss:9.993 at 18
04/11 16:01:59:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 16:01:59:  ---------
04/11 16:02:12:  #RES: f1:62.732, precision:60.538, recall:65.092, loss:11.214 at 20
04/11 16:02:12:  Best: f1:62.732, precision:60.538, recall:65.092, loss:11.214 at 20
04/11 16:02:12:  ---------
04/11 16:02:24:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/11 16:02:24:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:02:24:  ---------
04/11 16:03:01:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/11 16:03:01:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:03:01:  ---------
04/11 16:03:39:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/11 16:03:39:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:03:39:  ---------
04/11 16:04:16:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/11 16:04:16:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:04:16:  ---------
04/11 16:04:53:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/11 16:04:53:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:04:53:  ---------
04/11 16:04:54:  #RES: f1:61.424, precision:59.353, recall:63.645, loss:11.214 at 20
04/11 16:04:54:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 16:04:54:  ---------
04/11 16:04:58:  #RES: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:04:58:  Best: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:04:58:  ---------
04/11 16:06:01:  #RES: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:06:01:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:06:01:  ---------
04/11 16:06:05:  #RES: f1:64.620, precision:63.333, recall:65.959, loss:13.393 at 24
04/11 16:06:05:  Best: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:06:05:  ---------
04/11 16:07:09:  #RES: f1:64.110, precision:62.977, recall:65.284, loss:13.393 at 24
04/11 16:07:09:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:07:09:  ---------
04/11 16:07:13:  #RES: f1:66.046, precision:63.874, recall:68.370, loss:14.056 at 27
04/11 16:07:13:  Best: f1:66.046, precision:63.874, recall:68.370, loss:14.056 at 27
04/11 16:07:13:  ---------
04/11 16:08:17:  #RES: f1:64.832, precision:62.782, recall:67.020, loss:14.056 at 27
04/11 16:08:17:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:08:17:  ---------
04/11 16:08:21:  #RES: f1:66.099, precision:64.842, recall:67.406, loss:14.220 at 29
04/11 16:08:21:  Best: f1:66.099, precision:64.842, recall:67.406, loss:14.220 at 29
04/11 16:08:21:  ---------
04/11 16:09:25:  #RES: f1:65.751, precision:64.618, recall:66.924, loss:14.220 at 29
04/11 16:09:25:  Best: f1:65.751, precision:64.618, recall:66.924, loss:14.220 at 29
04/11 16:09:25:  ---------
04/11 16:09:28:  #RES: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:09:28:  Best: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:09:28:  ---------
04/11 16:10:33:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:10:33:  Best: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:10:33:  ---------
04/11 16:10:36:  #RES: f1:65.101, precision:63.462, recall:66.827, loss:16.341 at 34
04/11 16:10:36:  Best: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:10:36:  ---------
04/11 16:11:41:  #RES: f1:63.756, precision:62.123, recall:65.477, loss:16.341 at 34
04/11 16:11:41:  Best: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:11:41:  ---------
04/11 16:11:44:  #RES: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:11:44:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:11:44:  ---------
04/11 16:12:49:  #RES: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:12:49:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:12:49:  ---------
04/11 16:12:51:  #RES: f1:64.309, precision:62.140, recall:66.635, loss:17.053 at 38
04/11 16:12:51:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:12:51:  ---------
04/11 16:13:57:  #RES: f1:63.492, precision:61.538, recall:65.574, loss:17.053 at 38
04/11 16:13:57:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:13:57:  ---------
04/11 16:13:59:  #RES: f1:65.419, precision:63.545, recall:67.406, loss:16.265 at 40
04/11 16:13:59:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:13:59:  ---------
04/11 16:15:05:  #RES: f1:64.981, precision:63.148, recall:66.924, loss:16.265 at 40
04/11 16:15:05:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:15:05:  ---------
04/11 16:15:07:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:16.379 at 43
04/11 16:15:07:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:15:07:  ---------
04/11 16:16:13:  #RES: f1:66.950, precision:65.587, recall:68.370, loss:16.379 at 43
04/11 16:16:13:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:16:13:  ---------
04/11 16:16:14:  #RES: f1:66.319, precision:65.177, recall:67.502, loss:17.415 at 45
04/11 16:16:14:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:16:14:  ---------
04/11 16:17:21:  #RES: f1:65.751, precision:64.618, recall:66.924, loss:17.415 at 45
04/11 16:17:21:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:17:21:  ---------
04/11 16:17:22:  #RES: f1:66.889, precision:66.102, recall:67.695, loss:16.983 at 47
04/11 16:17:22:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:17:22:  ---------
04/11 16:18:29:  #RES: f1:66.476, precision:65.663, recall:67.310, loss:16.983 at 47
04/11 16:18:29:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:18:29:  ---------
04/11 16:18:30:  #RES: f1:65.621, precision:64.728, recall:66.538, loss:16.996 at 49
04/11 16:18:30:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:18:30:  ---------
04/11 16:19:37:  #RES: f1:65.527, precision:64.546, recall:66.538, loss:16.996 at 49
04/11 16:19:37:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:19:37:  ---------
04/11 16:19:37:  #RES: f1:66.541, precision:64.982, recall:68.177, loss:17.661 at 52
04/11 16:19:37:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:19:37:  ---------
04/11 16:20:45:  #RES: f1:65.976, precision:64.430, recall:67.599, loss:17.661 at 52
04/11 16:20:45:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:20:45:  ---------
04/11 16:20:45:  #RES: f1:66.477, precision:65.482, recall:67.502, loss:17.699 at 54
04/11 16:20:45:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:20:45:  ---------
04/11 16:21:52:  #RES: f1:65.842, precision:64.977, recall:66.731, loss:17.699 at 54
04/11 16:21:52:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:21:52:  ---------
04/11 16:21:53:  #RES: f1:66.920, precision:65.979, recall:67.888, loss:17.879 at 56
04/11 16:21:53:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:21:53:  ---------
04/11 16:23:00:  #RES: f1:66.382, precision:65.388, recall:67.406, loss:17.879 at 56
04/11 16:23:00:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:23:00:  ---------
04/11 16:23:01:  #RES: f1:65.240, precision:63.040, recall:67.599, loss:18.649 at 59
04/11 16:23:01:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:23:01:  ---------
04/11 16:24:08:  #RES: f1:65.300, precision:63.153, recall:67.599, loss:18.649 at 59
04/11 16:24:08:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:24:08:  ---------
04/11 16:24:09:  #RES: f1:66.792, precision:65.374, recall:68.274, loss:18.159 at 61
04/11 16:24:09:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:24:09:  ---------
04/11 16:25:16:  #RES: f1:66.792, precision:65.285, recall:68.370, loss:18.159 at 61
04/11 16:25:16:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:25:16:  ---------
04/11 16:25:17:  #RES: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:25:17:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:25:17:  ---------
04/11 16:26:24:  #RES: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:26:24:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:26:24:  ---------
04/11 16:26:24:  #RES: f1:67.144, precision:66.509, recall:67.792, loss:18.260 at 65
04/11 16:26:24:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:26:24:  ---------
04/11 16:27:32:  #RES: f1:66.826, precision:66.350, recall:67.310, loss:18.260 at 65
04/11 16:27:32:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:27:32:  ---------
04/11 16:27:32:  #RES: f1:65.221, precision:63.779, recall:66.731, loss:18.695 at 68
04/11 16:27:32:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:27:32:  ---------
04/11 16:28:40:  #RES: f1:65.158, precision:63.745, recall:66.635, loss:18.695 at 68
04/11 16:28:40:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:28:40:  ---------
04/11 16:28:40:  #RES: f1:65.982, precision:63.924, recall:68.177, loss:19.493 at 70
04/11 16:28:40:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:28:40:  ---------
04/11 16:29:48:  #RES: f1:65.796, precision:63.743, recall:67.985, loss:19.493 at 70
04/11 16:29:48:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:29:48:  ---------
04/11 16:29:48:  #RES: f1:64.516, precision:62.613, recall:66.538, loss:19.424 at 72
04/11 16:29:48:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:29:48:  ---------
04/11 16:30:55:  #RES: f1:64.761, precision:62.990, recall:66.635, loss:19.424 at 72
04/11 16:30:55:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:30:55:  ---------
04/11 16:30:56:  #RES: f1:65.855, precision:64.026, recall:67.792, loss:19.553 at 74
04/11 16:30:56:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:30:56:  ---------
04/11 16:32:03:  #RES: f1:65.946, precision:64.286, recall:67.695, loss:19.553 at 74
04/11 16:32:03:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:32:03:  ---------
04/11 16:32:03:  #RES: f1:66.293, precision:64.253, recall:68.467, loss:19.903 at 77
04/11 16:32:03:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:32:03:  ---------
04/11 16:33:11:  #RES: f1:66.698, precision:64.674, recall:68.852, loss:19.925 at 79
04/11 16:33:11:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:33:11:  ---------
04/11 16:33:11:  #RES: f1:66.417, precision:64.487, recall:68.467, loss:19.903 at 77
04/11 16:33:11:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:33:11:  ---------
04/11 16:34:20:  #RES: f1:64.972, precision:63.477, recall:66.538, loss:20.388 at 81
04/11 16:34:20:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:34:20:  ---------
04/11 16:34:20:  #RES: f1:66.262, precision:64.279, recall:68.370, loss:19.925 at 79
04/11 16:34:20:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:34:20:  ---------
04/11 16:35:27:  #RES: f1:66.132, precision:64.728, recall:67.599, loss:20.226 at 84
04/11 16:35:27:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:35:27:  ---------
04/11 16:35:27:  #RES: f1:65.068, precision:63.486, recall:66.731, loss:20.388 at 81
04/11 16:35:27:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:35:27:  ---------
04/11 16:36:35:  #RES: f1:66.792, precision:65.023, recall:68.660, loss:19.997 at 86
04/11 16:36:35:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:36:35:  ---------
04/11 16:36:36:  #RES: f1:65.600, precision:64.062, recall:67.213, loss:20.226 at 84
04/11 16:36:36:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:36:36:  ---------
04/11 16:37:43:  #RES: f1:66.444, precision:65.877, recall:67.020, loss:19.767 at 88
04/11 16:37:43:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:37:43:  ---------
04/11 16:37:43:  #RES: f1:66.635, precision:64.813, recall:68.563, loss:19.997 at 86
04/11 16:37:43:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:37:43:  ---------
04/11 16:38:51:  #RES: f1:65.912, precision:64.483, recall:67.406, loss:20.123 at 90
04/11 16:38:51:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:38:51:  ---------
04/11 16:38:51:  #RES: f1:66.158, precision:65.501, recall:66.827, loss:19.767 at 88
04/11 16:38:51:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:38:51:  ---------
04/11 16:39:58:  #RES: f1:65.546, precision:63.529, recall:67.695, loss:20.496 at 93
04/11 16:39:58:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:39:58:  ---------
04/11 16:39:59:  #RES: f1:66.102, precision:64.581, recall:67.695, loss:20.123 at 90
04/11 16:39:59:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:39:59:  ---------
04/11 16:41:06:  #RES: f1:66.134, precision:64.555, recall:67.792, loss:20.380 at 95
04/11 16:41:06:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:41:06:  ---------
04/11 16:41:07:  #RES: f1:65.329, precision:63.291, recall:67.502, loss:20.496 at 93
04/11 16:41:07:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:41:07:  ---------
04/11 16:42:14:  #RES: f1:66.103, precision:64.410, recall:67.888, loss:20.513 at 97
04/11 16:42:14:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:42:14:  ---------
04/11 16:42:15:  #RES: f1:66.291, precision:64.679, recall:67.985, loss:20.380 at 95
04/11 16:42:15:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:42:15:  ---------
04/11 16:43:21:  #RES: f1:66.479, precision:64.776, recall:68.274, loss:20.407 at 99
04/11 16:43:21:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:43:21:  ---------
04/11 16:43:22:  #RES: f1:65.885, precision:64.168, recall:67.695, loss:20.513 at 97
04/11 16:43:22:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:43:22:  ---------
04/11 16:43:52:  #RES: f1:66.354, precision:64.625, recall:68.177, loss:20.407 at 99
04/11 16:43:52:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:43:52:  ---------
04/11 17:25:54:  ======================== New Round =============================
04/11 17:25:54:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:25:54:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:26:20:  ======================== New Round =============================
04/11 17:26:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:26:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:26:43:  ======================== New Round =============================
04/11 17:26:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:26:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:28:52:  #RES: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:28:52:  Best: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:28:52:  ---------
04/11 17:31:03:  #RES: f1:58.889, precision:57.052, recall:60.849, loss:0.321 at 4
04/11 17:31:03:  Best: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:31:03:  ---------
04/11 17:33:14:  #RES: f1:62.518, precision:61.610, recall:63.452, loss:0.315 at 6
04/11 17:33:14:  Best: f1:62.518, precision:61.610, recall:63.452, loss:0.315 at 6
04/11 17:33:14:  ---------
04/11 17:35:26:  #RES: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:35:26:  Best: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:35:26:  ---------
04/11 17:37:38:  #RES: f1:61.635, precision:61.457, recall:61.813, loss:0.448 at 11
04/11 17:37:38:  Best: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:37:38:  ---------
04/11 17:39:49:  #RES: f1:65.855, precision:64.026, recall:67.792, loss:0.470 at 13
04/11 17:39:49:  Best: f1:65.855, precision:64.026, recall:67.792, loss:0.470 at 13
04/11 17:39:49:  ---------
04/11 17:42:01:  #RES: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:42:01:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:42:01:  ---------
04/11 17:44:12:  #RES: f1:64.818, precision:65.422, recall:64.224, loss:0.482 at 18
04/11 17:44:12:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:44:12:  ---------
04/11 17:46:24:  #RES: f1:65.481, precision:65.292, recall:65.670, loss:0.492 at 20
04/11 17:46:24:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:46:24:  ---------
04/11 17:48:35:  #RES: f1:65.161, precision:64.943, recall:65.381, loss:0.494 at 22
04/11 17:48:35:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:48:35:  ---------
04/11 17:50:47:  #RES: f1:66.320, precision:65.000, recall:67.695, loss:0.533 at 25
04/11 17:50:47:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:50:47:  ---------
04/11 17:52:59:  #RES: f1:66.379, precision:66.031, recall:66.731, loss:0.546 at 27
04/11 17:52:59:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:52:59:  ---------
04/11 17:55:10:  #RES: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:55:10:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:55:10:  ---------
04/11 17:57:22:  #RES: f1:64.602, precision:63.842, recall:65.381, loss:0.516 at 31
04/11 17:57:22:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:57:22:  ---------
04/11 17:59:33:  #RES: f1:66.508, precision:65.634, recall:67.406, loss:0.532 at 34
04/11 17:59:33:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:59:33:  ---------
04/11 18:01:45:  #RES: f1:65.676, precision:65.300, recall:66.056, loss:0.530 at 36
04/11 18:01:45:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:01:45:  ---------
04/11 18:03:56:  #RES: f1:64.672, precision:64.734, recall:64.609, loss:0.581 at 38
04/11 18:03:56:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:03:56:  ---------
04/11 18:06:08:  #RES: f1:65.906, precision:65.009, recall:66.827, loss:0.560 at 40
04/11 18:06:08:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:06:08:  ---------
04/11 18:08:19:  #RES: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:08:19:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:08:19:  ---------
04/11 18:10:31:  #RES: f1:66.798, precision:68.068, recall:65.574, loss:0.527 at 45
04/11 18:10:31:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:10:31:  ---------
04/11 18:12:43:  #RES: f1:64.703, precision:63.764, recall:65.670, loss:0.568 at 47
04/11 18:12:43:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:12:43:  ---------
04/11 18:14:54:  #RES: f1:65.601, precision:65.920, recall:65.284, loss:0.566 at 50
04/11 18:14:54:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:14:54:  ---------
04/11 18:17:06:  #RES: f1:65.901, precision:65.370, recall:66.442, loss:0.570 at 52
04/11 18:17:06:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:17:06:  ---------
04/11 18:19:18:  #RES: f1:66.188, precision:65.655, recall:66.731, loss:0.535 at 54
04/11 18:19:18:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:19:18:  ---------
04/11 18:21:30:  #RES: f1:66.731, precision:67.023, recall:66.442, loss:0.566 at 56
04/11 18:21:30:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:21:30:  ---------
04/11 18:23:42:  #RES: f1:65.316, precision:65.347, recall:65.284, loss:0.572 at 59
04/11 18:23:42:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:23:42:  ---------
04/11 18:25:53:  #RES: f1:65.081, precision:64.225, recall:65.959, loss:0.585 at 61
04/11 18:25:53:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:25:53:  ---------
04/11 18:28:05:  #RES: f1:65.460, precision:64.689, recall:66.249, loss:0.595 at 63
04/11 18:28:05:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:28:05:  ---------
04/11 18:30:17:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.564 at 65
04/11 18:30:17:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:30:17:  ---------
04/11 18:32:28:  #RES: f1:66.921, precision:66.257, recall:67.599, loss:0.551 at 68
04/11 18:32:28:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:32:28:  ---------
04/11 18:34:40:  #RES: f1:65.406, precision:64.133, recall:66.731, loss:0.585 at 70
04/11 18:34:40:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:34:40:  ---------
04/11 18:36:51:  #RES: f1:66.667, precision:66.991, recall:66.345, loss:0.590 at 72
04/11 18:36:51:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:36:51:  ---------
04/11 18:39:03:  #RES: f1:66.635, precision:65.789, recall:67.502, loss:0.570 at 75
04/11 18:39:03:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:39:03:  ---------
04/11 18:41:14:  #RES: f1:65.527, precision:64.546, recall:66.538, loss:0.582 at 77
04/11 18:41:14:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:41:14:  ---------
04/11 18:43:26:  #RES: f1:65.803, precision:65.458, recall:66.152, loss:0.574 at 79
04/11 18:43:26:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:43:26:  ---------
04/11 18:45:37:  #RES: f1:66.667, precision:67.189, recall:66.152, loss:0.567 at 81
04/11 18:45:37:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:45:37:  ---------
04/11 18:47:49:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.569 at 84
04/11 18:47:49:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:47:49:  ---------
04/11 18:50:01:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.577 at 86
04/11 18:50:01:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:50:01:  ---------
04/11 18:52:12:  #RES: f1:66.827, precision:66.539, recall:67.117, loss:0.585 at 88
04/11 18:52:12:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:52:12:  ---------
04/11 18:54:24:  #RES: f1:65.672, precision:65.577, recall:65.767, loss:0.593 at 90
04/11 18:54:24:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:54:24:  ---------
04/11 18:56:35:  #RES: f1:65.420, precision:65.076, recall:65.767, loss:0.597 at 93
04/11 18:56:35:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:56:35:  ---------
04/11 18:58:47:  #RES: f1:65.862, precision:65.957, recall:65.767, loss:0.600 at 95
04/11 18:58:47:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:58:47:  ---------
04/11 19:00:58:  #RES: f1:66.089, precision:65.931, recall:66.249, loss:0.599 at 97
04/11 19:00:58:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 19:00:58:  ---------
04/12 10:26:31:  ======================== New Round =============================
04/12 10:26:31:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:26:31:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:41:08:  ======================== New Round =============================
04/12 10:41:08:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:41:08:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:42:06:  ======================== New Round =============================
04/12 10:42:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:42:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:46:17:  ======================== New Round =============================
04/12 10:46:17:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:46:17:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:47:38:  ======================== New Round =============================
04/12 10:47:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:47:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:56:23:  ======================== New Round =============================
04/12 10:56:23:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:56:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:56:44:  #RES: f1:46.065, precision:46.132, recall:45.998, loss:0.287 at 0
04/12 10:56:44:  Best: f1:46.065, precision:46.132, recall:45.998, loss:0.287 at 0
04/12 10:56:44:  ---------
04/12 10:57:01:  ======================== New Round =============================
04/12 10:57:01:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:57:01:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:57:06:  #RES: f1:49.351, precision:49.232, recall:49.470, loss:0.256 at 0
04/12 10:57:06:  Best: f1:49.351, precision:49.232, recall:49.470, loss:0.256 at 0
04/12 10:57:06:  ---------
04/12 10:57:28:  #RES: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:28:  Best: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:28:  ---------
04/12 10:57:50:  #RES: f1:54.537, precision:55.183, recall:53.905, loss:0.268 at 0
04/12 10:57:50:  Best: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:50:  ---------
04/12 10:58:06:  ======================== New Round =============================
04/12 10:58:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:58:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:58:13:  #RES: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:13:  Best: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:13:  ---------
04/12 10:58:34:  #RES: f1:55.996, precision:54.857, recall:57.184, loss:0.243 at 0
04/12 10:58:34:  Best: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:34:  ---------
04/12 10:58:56:  #RES: f1:59.166, precision:59.512, recall:58.824, loss:0.228 at 0
04/12 10:58:56:  Best: f1:59.166, precision:59.512, recall:58.824, loss:0.228 at 0
04/12 10:58:56:  ---------
04/12 10:59:18:  #RES: f1:63.245, precision:63.521, recall:62.970, loss:0.231 at 1
04/12 10:59:18:  Best: f1:63.245, precision:63.521, recall:62.970, loss:0.231 at 1
04/12 10:59:18:  ---------
04/12 10:59:40:  #RES: f1:63.610, precision:63.671, recall:63.549, loss:0.241 at 1
04/12 10:59:40:  Best: f1:63.610, precision:63.671, recall:63.549, loss:0.241 at 1
04/12 10:59:40:  ---------
04/12 11:00:03:  #RES: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:03:  Best: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:03:  ---------
04/12 11:00:24:  #RES: f1:65.373, precision:65.756, recall:64.995, loss:0.250 at 1
04/12 11:00:24:  Best: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:24:  ---------
04/12 11:01:05:  ======================== New Round =============================
04/12 11:01:05:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:01:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:01:27:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/12 11:01:27:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/12 11:01:27:  ---------
04/12 11:01:31:  ======================== New Round =============================
04/12 11:01:31:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:01:31:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:02:07:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/12 11:02:07:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/12 11:02:07:  ---------
04/12 11:02:50:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:02:50:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:02:50:  ---------
04/12 11:03:34:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/12 11:03:34:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:03:34:  ---------
04/12 11:04:17:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 11:04:17:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 11:04:17:  ---------
04/12 11:04:18:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/12 11:04:18:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/12 11:04:18:  ---------
04/12 11:05:02:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/12 11:05:02:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/12 11:05:02:  ---------
04/12 11:05:47:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:05:47:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:05:47:  ---------
04/12 11:06:27:  ======================== New Round =============================
04/12 11:06:27:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:06:27:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:06:32:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/12 11:06:32:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:06:32:  ---------
04/12 11:07:02:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:07:02:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:07:02:  ---------
04/12 11:07:16:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/12 11:07:16:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/12 11:07:16:  ---------
04/12 11:07:27:  ======================== New Round =============================
04/12 11:07:27:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:07:27:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:08:02:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/12 11:08:02:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/12 11:08:02:  ---------
04/12 11:08:10:  ======================== New Round =============================
04/12 11:08:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:08:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:08:47:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:08:47:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:08:47:  ---------
04/12 11:09:24:  ======================== New Round =============================
04/12 11:09:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:09:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:09:33:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/12 11:09:33:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:09:33:  ---------
04/12 11:09:48:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/12 11:09:48:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:09:48:  ---------
04/12 11:10:17:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/12 11:10:17:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:10:17:  ---------
04/12 11:10:38:  ======================== New Round =============================
04/12 11:10:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:10:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:11:03:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/12 11:11:03:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:11:03:  ---------
04/12 11:11:47:  ======================== New Round =============================
04/12 11:11:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:11:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:11:48:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/12 11:11:48:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:11:48:  ---------
04/12 11:12:33:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/12 11:12:33:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:12:33:  ---------
04/12 11:12:35:  ======================== New Round =============================
04/12 11:12:35:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:12:35:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:12:36:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/12 11:12:36:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:12:36:  ---------
04/12 11:13:12:  ======================== New Round =============================
04/12 11:13:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:13:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:13:19:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/12 11:13:19:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:13:19:  ---------
04/12 11:14:04:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/12 11:14:04:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:14:04:  ---------
04/12 11:14:48:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/12 11:14:48:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:14:48:  ---------
04/12 11:15:21:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:15:21:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:15:21:  ---------
04/12 11:15:32:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/12 11:15:32:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:15:32:  ---------
04/12 11:16:16:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/12 11:16:16:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:16:16:  ---------
04/12 11:17:01:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/12 11:17:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:17:01:  ---------
04/12 11:17:46:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/12 11:17:46:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:17:46:  ---------
04/12 11:18:05:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/12 11:18:05:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:18:05:  ---------
04/12 11:18:30:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/12 11:18:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:18:30:  ---------
04/12 11:19:06:  ======================== New Round =============================
04/12 11:19:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:19:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:19:15:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/12 11:19:15:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:19:15:  ---------
04/12 11:19:50:  ======================== New Round =============================
04/12 11:19:50:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:19:50:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:20:01:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/12 11:20:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:20:01:  ---------
04/12 11:20:45:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/12 11:20:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:20:45:  ---------
04/12 11:20:51:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/12 11:20:51:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:20:51:  ---------
04/12 11:21:10:  ======================== New Round =============================
04/12 11:21:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:21:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:21:31:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/12 11:21:31:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:21:31:  ---------
04/12 11:21:32:  ======================== New Round =============================
04/12 11:21:32:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:21:32:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:22:16:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/12 11:22:16:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:22:16:  ---------
04/12 11:23:00:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/12 11:23:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:23:00:  ---------
04/12 11:23:20:  ======================== New Round =============================
04/12 11:23:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:23:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:23:37:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/12 11:23:37:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:23:37:  ---------
04/12 11:23:45:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/12 11:23:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:23:45:  ---------
04/12 11:24:30:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:24:30:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:24:30:  ---------
04/12 11:24:33:  ======================== New Round =============================
04/12 11:24:33:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:24:33:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:25:15:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/12 11:25:15:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:25:15:  ---------
04/12 11:25:47:  ======================== New Round =============================
04/12 11:25:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:25:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:26:00:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/12 11:26:00:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:26:00:  ---------
04/12 11:26:22:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/12 11:26:22:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:26:22:  ---------
04/12 11:26:44:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/12 11:26:44:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:26:44:  ---------
04/12 11:26:57:  ======================== New Round =============================
04/12 11:26:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:26:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:27:29:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/12 11:27:29:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:27:29:  ---------
04/12 11:28:14:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:14:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:14:  ---------
04/12 11:28:50:  ======================== New Round =============================
04/12 11:28:50:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:28:50:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:28:59:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/12 11:28:59:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:59:  ---------
04/12 11:29:08:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/12 11:29:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:29:08:  ---------
04/12 11:29:32:  ======================== New Round =============================
04/12 11:29:32:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:29:32:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:29:46:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/12 11:29:46:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:29:46:  ---------
04/12 11:30:10:  ======================== New Round =============================
04/12 11:30:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:30:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:30:39:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/12 11:30:39:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:30:39:  ---------
04/12 11:31:13:  #RES: f1:45.164, precision:45.296, recall:45.034, loss:0.256 at 2
04/12 11:31:13:  Best: f1:45.164, precision:45.296, recall:45.034, loss:0.256 at 2
04/12 11:31:13:  ---------
04/12 11:31:45:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/12 11:31:45:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:31:45:  ---------
04/12 11:32:13:  #RES: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:32:13:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:32:13:  ---------
04/12 11:32:43:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 11:32:43:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 11:32:43:  ---------
04/12 11:32:49:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/12 11:32:49:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:32:49:  ---------
04/12 11:33:11:  #RES: f1:48.702, precision:47.689, recall:49.759, loss:0.319 at 6
04/12 11:33:11:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:33:11:  ---------
04/12 11:33:55:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/12 11:33:55:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:33:55:  ---------
04/12 11:34:11:  #RES: f1:47.145, precision:47.727, recall:46.577, loss:0.346 at 9
04/12 11:34:11:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:34:11:  ---------
04/12 11:35:00:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/12 11:35:00:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:35:00:  ---------
04/12 11:35:06:  #RES: f1:49.544, precision:49.331, recall:49.759, loss:0.388 at 11
04/12 11:35:06:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:35:06:  ---------
04/12 11:35:45:  #RES: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:35:45:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:35:45:  ---------
04/12 11:36:07:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:36:07:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:36:07:  ---------
04/12 11:36:23:  #RES: f1:51.755, precision:50.273, recall:53.327, loss:0.452 at 15
04/12 11:36:23:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:36:23:  ---------
04/12 11:37:02:  #RES: f1:47.428, precision:48.207, recall:46.673, loss:0.485 at 18
04/12 11:37:02:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:37:02:  ---------
04/12 11:37:40:  #RES: f1:47.105, precision:47.952, recall:46.287, loss:0.514 at 20
04/12 11:37:40:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:37:40:  ---------
04/12 11:38:19:  #RES: f1:53.282, precision:53.333, recall:53.230, loss:0.512 at 22
04/12 11:38:19:  Best: f1:53.282, precision:53.333, recall:53.230, loss:0.512 at 22
04/12 11:38:19:  ---------
04/12 11:38:45:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/12 11:38:45:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:38:45:  ---------
04/12 11:38:57:  #RES: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:38:57:  Best: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:38:57:  ---------
04/12 11:39:35:  #RES: f1:51.785, precision:51.128, recall:52.459, loss:0.559 at 27
04/12 11:39:35:  Best: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:39:35:  ---------
04/12 11:40:14:  #RES: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:14:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:14:  ---------
04/12 11:40:53:  #RES: f1:53.180, precision:52.751, recall:53.616, loss:0.582 at 31
04/12 11:40:53:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:53:  ---------
04/12 11:41:22:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/12 11:41:22:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:41:22:  ---------
04/12 11:41:31:  #RES: f1:52.261, precision:51.598, recall:52.941, loss:0.603 at 34
04/12 11:41:31:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:41:31:  ---------
04/12 11:42:10:  #RES: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:10:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:10:  ---------
04/12 11:42:49:  #RES: f1:53.748, precision:54.995, recall:52.555, loss:0.595 at 38
04/12 11:42:49:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:49:  ---------
04/12 11:43:27:  #RES: f1:53.462, precision:53.696, recall:53.230, loss:0.596 at 40
04/12 11:43:27:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:43:27:  ---------
04/12 11:44:00:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/12 11:44:00:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:44:00:  ---------
04/12 11:44:05:  #RES: f1:53.861, precision:54.609, recall:53.134, loss:0.625 at 43
04/12 11:44:05:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:44:05:  ---------
04/12 11:44:44:  #RES: f1:52.915, precision:53.785, recall:52.073, loss:0.618 at 45
04/12 11:44:44:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:44:44:  ---------
04/12 11:45:23:  #RES: f1:53.173, precision:53.020, recall:53.327, loss:0.616 at 47
04/12 11:45:23:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:45:23:  ---------
04/12 11:46:01:  #RES: f1:53.531, precision:54.858, recall:52.266, loss:0.629 at 49
04/12 11:46:01:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:46:01:  ---------
04/12 11:46:37:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/12 11:46:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:46:37:  ---------
04/12 11:46:39:  #RES: f1:53.944, precision:54.474, recall:53.423, loss:0.633 at 52
04/12 11:46:39:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:46:39:  ---------
04/12 11:47:18:  #RES: f1:52.967, precision:53.892, recall:52.073, loss:0.642 at 54
04/12 11:47:18:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:47:18:  ---------
04/12 11:47:57:  #RES: f1:52.715, precision:54.536, recall:51.013, loss:0.634 at 56
04/12 11:47:57:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:47:57:  ---------
04/12 11:48:36:  #RES: f1:53.556, precision:54.491, recall:52.652, loss:0.658 at 59
04/12 11:48:36:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:48:36:  ---------
04/12 11:49:13:  #RES: f1:52.243, precision:54.076, recall:50.530, loss:0.656 at 61
04/12 11:49:13:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:49:13:  ---------
04/12 11:49:14:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/12 11:49:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:49:14:  ---------
04/12 11:49:52:  #RES: f1:53.786, precision:54.154, recall:53.423, loss:0.657 at 63
04/12 11:49:52:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:49:52:  ---------
04/12 11:50:31:  #RES: f1:53.346, precision:54.059, recall:52.652, loss:0.646 at 65
04/12 11:50:31:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:50:31:  ---------
04/12 11:51:10:  #RES: f1:54.339, precision:55.600, recall:53.134, loss:0.678 at 68
04/12 11:51:10:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:51:10:  ---------
04/12 11:51:48:  #RES: f1:52.746, precision:54.167, recall:51.398, loss:0.659 at 70
04/12 11:51:48:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:51:48:  ---------
04/12 11:51:51:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/12 11:51:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:51:51:  ---------
04/12 11:52:27:  #RES: f1:53.552, precision:55.225, recall:51.977, loss:0.677 at 72
04/12 11:52:27:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:52:27:  ---------
04/12 11:53:05:  #RES: f1:53.193, precision:52.964, recall:53.423, loss:0.643 at 74
04/12 11:53:05:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:53:05:  ---------
04/12 11:53:44:  #RES: f1:53.281, precision:53.725, recall:52.845, loss:0.652 at 77
04/12 11:53:44:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:53:44:  ---------
04/12 11:54:22:  #RES: f1:54.267, precision:53.956, recall:54.581, loss:0.667 at 79
04/12 11:54:22:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:54:22:  ---------
04/12 11:54:27:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 11:54:27:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 11:54:27:  ---------
04/12 11:55:01:  #RES: f1:53.673, precision:55.705, recall:51.784, loss:0.668 at 81
04/12 11:55:01:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:55:01:  ---------
04/12 11:55:39:  #RES: f1:54.537, precision:55.489, recall:53.616, loss:0.664 at 84
04/12 11:55:39:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:55:39:  ---------
04/12 11:56:18:  #RES: f1:54.492, precision:55.193, recall:53.809, loss:0.675 at 86
04/12 11:56:18:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:56:18:  ---------
04/12 11:56:57:  #RES: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:56:57:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:56:57:  ---------
04/12 11:57:05:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:57:05:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:57:05:  ---------
04/12 11:57:35:  #RES: f1:54.448, precision:54.902, recall:54.002, loss:0.682 at 90
04/12 11:57:35:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:57:35:  ---------
04/12 11:58:14:  #RES: f1:54.652, precision:55.217, recall:54.098, loss:0.678 at 93
04/12 11:58:14:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:58:14:  ---------
04/12 11:58:52:  #RES: f1:54.599, precision:55.412, recall:53.809, loss:0.677 at 95
04/12 11:58:52:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:58:52:  ---------
04/12 11:59:31:  #RES: f1:54.608, precision:55.533, recall:53.713, loss:0.679 at 97
04/12 11:59:31:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:59:31:  ---------
04/12 11:59:42:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/12 11:59:42:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:59:42:  ---------
04/12 12:00:09:  #RES: f1:54.465, precision:55.445, recall:53.520, loss:0.679 at 99
04/12 12:00:09:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 12:00:09:  ---------
04/12 12:01:12:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:01:12:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:01:12:  ---------
04/12 12:02:27:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/12 12:02:27:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:02:27:  ---------
04/12 12:03:43:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/12 12:03:43:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:03:43:  ---------
04/12 12:04:59:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/12 12:04:59:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:04:59:  ---------
04/12 12:06:15:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/12 12:06:15:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:06:15:  ---------
04/12 12:07:31:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/12 12:07:31:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:07:31:  ---------
04/12 12:08:47:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:08:47:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:08:47:  ---------
04/12 12:10:03:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/12 12:10:03:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:10:03:  ---------
04/12 12:11:18:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/12 12:11:18:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:11:18:  ---------
04/12 12:12:34:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/12 12:12:34:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:12:34:  ---------
04/12 12:13:50:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/12 12:13:50:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:13:50:  ---------
04/12 12:15:06:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:15:06:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:15:06:  ---------
04/12 12:16:22:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/12 12:16:22:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:16:22:  ---------
04/12 12:17:38:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/12 12:17:38:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:17:38:  ---------
04/12 12:18:54:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/12 12:18:54:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:18:54:  ---------
04/12 12:20:10:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/12 12:20:10:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:20:10:  ---------
04/12 12:21:26:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/12 12:21:26:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:21:26:  ---------
04/12 12:22:41:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/12 12:22:41:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:22:41:  ---------
04/12 12:23:57:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/12 12:23:57:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:23:57:  ---------
04/12 12:25:13:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/12 12:25:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:25:13:  ---------
04/12 12:26:29:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/12 12:26:29:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:26:29:  ---------
04/12 12:27:44:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/12 12:27:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:27:44:  ---------
04/12 15:58:19:  ======================== New Round =============================
04/12 15:58:19:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 15:58:19:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 15:59:36:  ======================== New Round =============================
04/12 15:59:36:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 15:59:36:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:04:45:  ======================== New Round =============================
04/12 16:04:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:04:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:06:03:  ======================== New Round =============================
04/12 16:06:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:06:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:06:36:  #RES: f1:61.531, precision:59.302, recall:63.934, loss:0.181 at 2
04/12 16:06:36:  Best: f1:61.531, precision:59.302, recall:63.934, loss:0.181 at 2
04/12 16:06:36:  ---------
04/12 16:07:09:  #RES: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:09:  Best: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:09:  ---------
04/12 16:07:43:  #RES: f1:64.369, precision:64.809, recall:63.934, loss:0.293 at 6
04/12 16:07:43:  Best: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:43:  ---------
04/12 16:08:16:  #RES: f1:66.444, precision:65.693, recall:67.213, loss:0.327 at 9
04/12 16:08:16:  Best: f1:66.444, precision:65.693, recall:67.213, loss:0.327 at 9
04/12 16:08:16:  ---------
04/12 16:08:49:  #RES: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:08:49:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:08:49:  ---------
04/12 16:09:23:  #RES: f1:64.793, precision:66.297, recall:63.356, loss:0.409 at 13
04/12 16:09:23:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:09:23:  ---------
04/12 16:09:56:  #RES: f1:66.446, precision:65.242, recall:67.695, loss:0.425 at 15
04/12 16:09:56:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:09:56:  ---------
04/12 16:10:30:  #RES: f1:66.029, precision:65.527, recall:66.538, loss:0.432 at 18
04/12 16:10:30:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:10:30:  ---------
04/12 16:11:03:  #RES: f1:66.539, precision:66.252, recall:66.827, loss:0.464 at 20
04/12 16:11:03:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:11:03:  ---------
04/12 16:11:37:  #RES: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:11:37:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:11:37:  ---------
04/12 16:12:10:  #RES: f1:66.376, precision:66.699, recall:66.056, loss:0.492 at 25
04/12 16:12:10:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:12:10:  ---------
04/12 16:12:44:  #RES: f1:66.951, precision:65.858, recall:68.081, loss:0.506 at 27
04/12 16:12:44:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:12:44:  ---------
04/12 16:13:17:  #RES: f1:66.218, precision:65.996, recall:66.442, loss:0.514 at 29
04/12 16:13:17:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:13:17:  ---------
04/12 16:13:51:  #RES: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:13:51:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:13:51:  ---------
04/12 16:14:24:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 34
04/12 16:14:24:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:14:24:  ---------
04/12 16:14:58:  #RES: f1:64.786, precision:63.211, recall:66.442, loss:0.530 at 36
04/12 16:14:58:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:14:58:  ---------
04/12 16:15:31:  #RES: f1:66.603, precision:65.728, recall:67.502, loss:0.514 at 38
04/12 16:15:31:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:15:31:  ---------
04/12 16:16:05:  #RES: f1:67.148, precision:66.987, recall:67.310, loss:0.529 at 40
04/12 16:16:05:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:16:05:  ---------
04/12 16:16:38:  #RES: f1:66.033, precision:65.075, recall:67.020, loss:0.543 at 43
04/12 16:16:38:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:16:38:  ---------
04/12 16:17:12:  #RES: f1:66.542, precision:64.636, recall:68.563, loss:0.560 at 45
04/12 16:17:12:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:17:12:  ---------
04/12 16:17:45:  #RES: f1:66.572, precision:65.485, recall:67.695, loss:0.581 at 47
04/12 16:17:45:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:17:45:  ---------
04/12 16:18:19:  #RES: f1:66.730, precision:65.610, recall:67.888, loss:0.573 at 50
04/12 16:18:19:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:18:19:  ---------
04/12 16:18:52:  #RES: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:18:52:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:18:52:  ---------
04/12 16:19:25:  #RES: f1:66.407, precision:67.060, recall:65.767, loss:0.586 at 54
04/12 16:19:25:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:19:25:  ---------
04/12 16:19:59:  #RES: f1:65.875, precision:64.948, recall:66.827, loss:0.584 at 56
04/12 16:19:59:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:19:59:  ---------
04/12 16:20:33:  #RES: f1:66.212, precision:66.963, recall:65.477, loss:0.559 at 59
04/12 16:20:33:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:20:33:  ---------
04/12 16:21:06:  #RES: f1:65.938, precision:64.981, recall:66.924, loss:0.573 at 61
04/12 16:21:06:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:21:06:  ---------
04/12 16:21:40:  #RES: f1:65.880, precision:64.510, recall:67.310, loss:0.592 at 63
04/12 16:21:40:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:21:40:  ---------
04/12 16:22:13:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.592 at 65
04/12 16:22:13:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:22:13:  ---------
04/12 16:22:47:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.592 at 68
04/12 16:22:47:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:22:47:  ---------
04/12 16:23:20:  #RES: f1:65.967, precision:65.312, recall:66.635, loss:0.598 at 70
04/12 16:23:20:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:23:20:  ---------
04/12 16:23:54:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.610 at 72
04/12 16:23:54:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:23:54:  ---------
04/12 16:24:27:  #RES: f1:66.603, precision:66.004, recall:67.213, loss:0.591 at 75
04/12 16:24:27:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:24:27:  ---------
04/12 16:25:01:  #RES: f1:66.130, precision:64.991, recall:67.310, loss:0.611 at 77
04/12 16:25:01:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:25:01:  ---------
04/12 16:25:34:  #RES: f1:66.318, precision:65.446, recall:67.213, loss:0.597 at 79
04/12 16:25:34:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:25:34:  ---------
04/12 16:26:08:  #RES: f1:66.982, precision:65.829, recall:68.177, loss:0.600 at 81
04/12 16:26:08:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:26:08:  ---------
04/12 16:26:41:  #RES: f1:67.430, precision:66.698, recall:68.177, loss:0.598 at 84
04/12 16:26:41:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:26:41:  ---------
04/12 16:27:15:  #RES: f1:65.934, precision:65.341, recall:66.538, loss:0.602 at 86
04/12 16:27:15:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:27:15:  ---------
04/12 16:27:48:  #RES: f1:65.952, precision:63.784, recall:68.274, loss:0.649 at 88
04/12 16:27:48:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:27:48:  ---------
04/12 16:28:22:  #RES: f1:66.761, precision:65.138, recall:68.467, loss:0.624 at 90
04/12 16:28:22:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:28:22:  ---------
04/12 16:28:55:  #RES: f1:66.540, precision:65.335, recall:67.792, loss:0.619 at 93
04/12 16:28:55:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:28:55:  ---------
04/12 16:29:28:  #RES: f1:66.161, precision:65.051, recall:67.310, loss:0.612 at 95
04/12 16:29:28:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:29:28:  ---------
04/12 16:30:02:  #RES: f1:66.635, precision:65.881, recall:67.406, loss:0.608 at 97
04/12 16:30:02:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:30:02:  ---------
04/12 16:32:05:  ======================== New Round =============================
04/12 16:32:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:32:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:32:28:  #RES: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 16:32:28:  Best: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 16:32:28:  ---------
04/12 16:32:52:  #RES: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 16:32:52:  Best: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 16:32:52:  ---------
04/12 16:33:15:  #RES: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:15:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:15:  ---------
04/12 16:33:38:  #RES: f1:63.234, precision:63.112, recall:63.356, loss:0.331 at 9
04/12 16:33:38:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:38:  ---------
04/12 16:34:02:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:02:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:02:  ---------
04/12 16:34:25:  #RES: f1:65.163, precision:64.852, recall:65.477, loss:0.395 at 13
04/12 16:34:25:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:25:  ---------
04/12 16:34:49:  #RES: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:34:49:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:34:49:  ---------
04/12 16:35:12:  #RES: f1:64.790, precision:64.117, recall:65.477, loss:0.419 at 18
04/12 16:35:12:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:12:  ---------
04/12 16:35:36:  #RES: f1:65.079, precision:64.313, recall:65.863, loss:0.438 at 20
04/12 16:35:36:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:36:  ---------
04/12 16:35:59:  #RES: f1:66.031, precision:65.345, recall:66.731, loss:0.446 at 22
04/12 16:35:59:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:59:  ---------
04/12 16:36:22:  #RES: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:22:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:22:  ---------
04/12 16:36:46:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.449 at 27
04/12 16:36:46:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:46:  ---------
04/12 16:37:09:  #RES: f1:66.635, precision:66.443, recall:66.827, loss:0.468 at 29
04/12 16:37:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:09:  ---------
04/12 16:37:33:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.462 at 31
04/12 16:37:33:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:33:  ---------
04/12 16:37:56:  #RES: f1:66.602, precision:66.959, recall:66.249, loss:0.506 at 34
04/12 16:37:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:56:  ---------
04/12 16:38:20:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 36
04/12 16:38:20:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:38:20:  ---------
04/12 16:38:43:  #RES: f1:66.988, precision:66.956, recall:67.020, loss:0.482 at 38
04/12 16:38:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:38:43:  ---------
04/12 16:39:07:  #RES: f1:67.281, precision:67.641, recall:66.924, loss:0.519 at 40
04/12 16:39:07:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:07:  ---------
04/12 16:39:30:  #RES: f1:66.256, precision:65.144, recall:67.406, loss:0.534 at 43
04/12 16:39:30:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:30:  ---------
04/12 16:39:54:  #RES: f1:66.731, precision:67.221, recall:66.249, loss:0.490 at 45
04/12 16:39:54:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:54:  ---------
04/12 16:40:17:  #RES: f1:67.021, precision:67.216, recall:66.827, loss:0.514 at 47
04/12 16:40:17:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:40:17:  ---------
04/12 16:40:41:  #RES: f1:66.860, precision:66.892, recall:66.827, loss:0.528 at 49
04/12 16:40:41:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:40:41:  ---------
04/12 16:41:04:  #RES: f1:66.794, precision:66.381, recall:67.213, loss:0.545 at 52
04/12 16:41:04:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:41:04:  ---------
04/12 16:41:28:  #RES: f1:65.755, precision:64.358, recall:67.213, loss:0.570 at 54
04/12 16:41:28:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:41:28:  ---------
04/12 16:43:56:  #RES: f1:65.928, precision:65.896, recall:65.959, loss:0.532 at 56
04/12 16:43:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:43:56:  ---------
04/12 16:44:19:  #RES: f1:65.961, precision:65.865, recall:66.056, loss:0.552 at 59
04/12 16:44:19:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:44:19:  ---------
04/12 16:44:43:  #RES: f1:67.429, precision:66.604, recall:68.274, loss:0.543 at 61
04/12 16:44:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:44:43:  ---------
04/12 16:45:06:  #RES: f1:67.153, precision:67.679, recall:66.635, loss:0.553 at 63
04/12 16:45:06:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:06:  ---------
04/12 16:45:29:  #RES: f1:66.921, precision:66.164, recall:67.695, loss:0.528 at 65
04/12 16:45:29:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:29:  ---------
04/12 16:45:53:  #RES: f1:67.016, precision:66.165, recall:67.888, loss:0.562 at 68
04/12 16:45:53:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:53:  ---------
04/12 16:46:16:  #RES: f1:65.325, precision:63.455, recall:67.310, loss:0.580 at 70
04/12 16:46:16:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:46:16:  ---------
04/12 16:46:40:  #RES: f1:66.889, precision:66.195, recall:67.599, loss:0.558 at 72
04/12 16:46:40:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:46:40:  ---------
04/12 16:47:03:  #RES: f1:66.381, precision:65.478, recall:67.310, loss:0.557 at 74
04/12 16:47:03:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:03:  ---------
04/12 16:47:27:  #RES: f1:65.783, precision:64.591, recall:67.020, loss:0.562 at 77
04/12 16:47:27:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:27:  ---------
04/12 16:47:50:  #RES: f1:67.234, precision:65.955, recall:68.563, loss:0.570 at 79
04/12 16:47:50:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:50:  ---------
04/12 16:48:14:  #RES: f1:67.323, precision:65.511, recall:69.238, loss:0.571 at 81
04/12 16:48:14:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:48:14:  ---------
04/12 16:48:37:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.566 at 84
04/12 16:48:37:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:48:37:  ---------
04/12 16:49:01:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:0.569 at 86
04/12 16:49:01:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:49:01:  ---------
04/12 16:49:24:  #RES: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 16:49:24:  Best: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 16:49:24:  ---------
04/12 16:49:48:  #RES: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 16:49:48:  Best: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 16:49:48:  ---------
04/12 16:50:12:  #RES: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:12:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:12:  ---------
04/12 16:50:35:  #RES: f1:67.173, precision:66.106, recall:68.274, loss:0.566 at 95
04/12 16:50:35:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:35:  ---------
04/12 16:50:59:  #RES: f1:66.952, precision:65.949, recall:67.985, loss:0.571 at 97
04/12 16:50:59:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:59:  ---------
04/12 16:51:22:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.571 at 99
04/12 16:51:22:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:51:22:  ---------
04/12 17:11:29:  ======================== New Round =============================
04/12 17:11:29:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 17:11:29:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 17:11:53:  #RES: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 17:11:53:  Best: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 17:11:53:  ---------
04/12 17:12:16:  #RES: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 17:12:16:  Best: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 17:12:16:  ---------
04/12 17:12:39:  #RES: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:12:39:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:12:39:  ---------
04/12 17:13:02:  #RES: f1:63.234, precision:63.112, recall:63.356, loss:0.331 at 9
04/12 17:13:02:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:13:02:  ---------
04/12 17:13:25:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:25:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:25:  ---------
04/12 17:13:49:  #RES: f1:65.163, precision:64.852, recall:65.477, loss:0.395 at 13
04/12 17:13:49:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:49:  ---------
04/12 17:14:12:  #RES: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:12:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:12:  ---------
04/12 17:14:36:  #RES: f1:64.790, precision:64.117, recall:65.477, loss:0.419 at 18
04/12 17:14:36:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:36:  ---------
04/12 17:14:59:  #RES: f1:65.079, precision:64.313, recall:65.863, loss:0.438 at 20
04/12 17:14:59:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:59:  ---------
04/12 17:15:23:  #RES: f1:66.031, precision:65.345, recall:66.731, loss:0.446 at 22
04/12 17:15:23:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:15:23:  ---------
04/12 17:15:46:  #RES: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:15:46:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:15:46:  ---------
04/12 17:16:09:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.449 at 27
04/12 17:16:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:09:  ---------
04/12 17:16:33:  #RES: f1:66.635, precision:66.443, recall:66.827, loss:0.468 at 29
04/12 17:16:33:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:33:  ---------
04/12 17:16:56:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.462 at 31
04/12 17:16:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:56:  ---------
04/12 17:17:20:  #RES: f1:66.602, precision:66.959, recall:66.249, loss:0.506 at 34
04/12 17:17:20:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:17:20:  ---------
04/12 17:17:43:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 36
04/12 17:17:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:17:43:  ---------
04/12 17:18:06:  #RES: f1:66.988, precision:66.956, recall:67.020, loss:0.482 at 38
04/12 17:18:06:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:06:  ---------
04/12 17:18:30:  #RES: f1:67.281, precision:67.641, recall:66.924, loss:0.519 at 40
04/12 17:18:30:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:30:  ---------
04/12 17:18:54:  #RES: f1:66.256, precision:65.144, recall:67.406, loss:0.534 at 43
04/12 17:18:54:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:54:  ---------
04/12 17:19:17:  #RES: f1:66.731, precision:67.221, recall:66.249, loss:0.490 at 45
04/12 17:19:17:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:19:17:  ---------
04/12 17:19:40:  #RES: f1:67.021, precision:67.216, recall:66.827, loss:0.514 at 47
04/12 17:19:40:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:19:40:  ---------
04/12 17:20:04:  #RES: f1:66.860, precision:66.892, recall:66.827, loss:0.528 at 49
04/12 17:20:04:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:04:  ---------
04/12 17:20:28:  #RES: f1:66.794, precision:66.381, recall:67.213, loss:0.545 at 52
04/12 17:20:28:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:28:  ---------
04/12 17:20:51:  #RES: f1:65.755, precision:64.358, recall:67.213, loss:0.570 at 54
04/12 17:20:51:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:51:  ---------
04/12 17:21:14:  #RES: f1:65.928, precision:65.896, recall:65.959, loss:0.532 at 56
04/12 17:21:14:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:21:14:  ---------
04/12 17:21:38:  #RES: f1:65.961, precision:65.865, recall:66.056, loss:0.552 at 59
04/12 17:21:38:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:21:38:  ---------
04/12 17:22:01:  #RES: f1:67.429, precision:66.604, recall:68.274, loss:0.543 at 61
04/12 17:22:01:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:01:  ---------
04/12 17:22:25:  #RES: f1:67.153, precision:67.679, recall:66.635, loss:0.553 at 63
04/12 17:22:25:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:25:  ---------
04/12 17:22:48:  #RES: f1:66.921, precision:66.164, recall:67.695, loss:0.528 at 65
04/12 17:22:48:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:48:  ---------
04/12 17:23:11:  #RES: f1:67.016, precision:66.165, recall:67.888, loss:0.562 at 68
04/12 17:23:11:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:11:  ---------
04/12 17:23:35:  #RES: f1:65.325, precision:63.455, recall:67.310, loss:0.580 at 70
04/12 17:23:35:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:35:  ---------
04/12 17:23:58:  #RES: f1:66.889, precision:66.195, recall:67.599, loss:0.558 at 72
04/12 17:23:58:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:58:  ---------
04/12 17:24:22:  #RES: f1:66.381, precision:65.478, recall:67.310, loss:0.557 at 74
04/12 17:24:22:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:24:22:  ---------
04/12 17:24:45:  #RES: f1:65.783, precision:64.591, recall:67.020, loss:0.562 at 77
04/12 17:24:45:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:24:45:  ---------
04/12 17:25:09:  #RES: f1:67.234, precision:65.955, recall:68.563, loss:0.570 at 79
04/12 17:25:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:09:  ---------
04/12 17:25:32:  #RES: f1:67.323, precision:65.511, recall:69.238, loss:0.571 at 81
04/12 17:25:32:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:32:  ---------
04/12 17:25:56:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.566 at 84
04/12 17:25:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:56:  ---------
04/12 17:26:19:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:0.569 at 86
04/12 17:26:19:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:26:19:  ---------
04/12 17:26:42:  #RES: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 17:26:42:  Best: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 17:26:42:  ---------
04/12 17:27:06:  #RES: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 17:27:06:  Best: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 17:27:06:  ---------
04/12 17:27:29:  #RES: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:29:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:29:  ---------
04/12 17:27:52:  #RES: f1:67.173, precision:66.106, recall:68.274, loss:0.566 at 95
04/12 17:27:52:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:52:  ---------
04/12 17:28:16:  #RES: f1:66.952, precision:65.949, recall:67.985, loss:0.571 at 97
04/12 17:28:16:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:28:16:  ---------
04/12 17:28:39:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.571 at 99
04/12 17:28:39:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:28:39:  ---------
04/12 18:18:15:  ======================== New Round =============================
04/12 18:18:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 18:18:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 18:19:57:  ======================== New Round =============================
04/12 18:19:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 18:19:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 18:21:12:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 18:21:12:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 18:21:12:  ---------
04/12 18:22:26:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:22:26:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:22:26:  ---------
04/12 18:23:41:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/12 18:23:41:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:23:41:  ---------
04/12 18:24:56:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/12 18:24:56:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:24:56:  ---------
04/12 18:26:11:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:26:11:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:26:11:  ---------
04/12 18:27:27:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/12 18:27:27:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:27:27:  ---------
04/12 18:28:42:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/12 18:28:42:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:28:42:  ---------
04/12 18:29:57:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/12 18:29:57:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:29:57:  ---------
04/12 18:31:12:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/12 18:31:12:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:31:12:  ---------
04/12 18:32:27:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/12 18:32:27:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:32:27:  ---------
04/12 18:33:42:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 18:33:42:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 18:33:42:  ---------
04/12 18:34:58:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:34:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:34:58:  ---------
04/12 18:36:13:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/12 18:36:13:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:36:13:  ---------
04/12 18:37:28:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/12 18:37:28:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:37:28:  ---------
04/12 18:38:43:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/12 18:38:43:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:38:43:  ---------
04/12 18:39:59:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/12 18:39:59:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:39:59:  ---------
04/12 18:41:14:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/12 18:41:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:41:14:  ---------
04/12 18:42:29:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/12 18:42:29:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:42:29:  ---------
04/12 18:43:44:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 18:43:44:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 18:43:44:  ---------
04/12 18:44:59:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:44:59:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:44:59:  ---------
04/12 18:46:14:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/12 18:46:14:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:46:14:  ---------
04/12 18:47:29:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:47:29:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:47:29:  ---------
04/12 18:48:45:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/12 18:48:45:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:48:45:  ---------
04/12 18:50:00:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/12 18:50:00:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:50:00:  ---------
04/12 18:51:15:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/12 18:51:15:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:51:15:  ---------
04/12 18:52:30:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/12 18:52:30:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:52:30:  ---------
04/12 18:53:46:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/12 18:53:46:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:53:46:  ---------
04/12 18:55:01:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:55:01:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:55:01:  ---------
04/12 18:56:16:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/12 18:56:16:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:56:16:  ---------
04/12 18:57:31:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/12 18:57:31:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:57:31:  ---------
04/12 18:58:46:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/12 18:58:46:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:58:46:  ---------
04/12 19:00:01:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/12 19:00:01:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 19:00:01:  ---------
04/12 19:01:17:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:01:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:01:17:  ---------
04/12 19:02:32:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/12 19:02:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:02:32:  ---------
04/12 19:03:47:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/12 19:03:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:03:47:  ---------
04/12 19:05:02:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/12 19:05:02:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:05:02:  ---------
04/12 19:06:17:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/12 19:06:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:06:17:  ---------
04/12 19:07:32:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/12 19:07:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:07:32:  ---------
04/12 19:08:47:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/12 19:08:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:08:47:  ---------
04/12 19:10:02:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/12 19:10:02:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:10:02:  ---------
04/12 19:11:17:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/12 19:11:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:11:17:  ---------
04/12 19:12:32:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/12 19:12:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:12:32:  ---------
04/12 19:13:47:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/12 19:13:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:13:47:  ---------
04/14 15:06:24:  ======================== New Round =============================
04/14 15:06:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:06:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:07:44:  ======================== New Round =============================
04/14 15:07:44:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:07:44:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:08:17:  ======================== New Round =============================
04/14 15:08:17:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:08:17:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:19:18:  ======================== New Round =============================
04/14 15:19:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:19:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:22:43:  ======================== New Round =============================
04/14 15:22:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:22:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:27:49:  ======================== New Round =============================
04/14 15:27:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:27:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:28:46:  ======================== New Round =============================
04/14 15:28:46:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:28:46:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:30:16:  ======================== New Round =============================
04/14 15:30:16:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:30:16:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:30:57:  ======================== New Round =============================
04/14 15:30:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:30:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:31:51:  ======================== New Round =============================
04/14 15:31:51:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:31:51:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:37:21:  ======================== New Round =============================
04/14 15:37:21:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:37:21:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:37:56:  ======================== New Round =============================
04/14 15:37:56:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:37:56:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:38:19:  ======================== New Round =============================
04/14 15:38:19:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:38:19:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:39:33:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/14 15:39:33:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/14 15:39:33:  ---------
04/14 15:40:47:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:40:47:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:40:47:  ---------
04/14 15:42:02:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/14 15:42:02:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:42:02:  ---------
04/14 15:43:18:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/14 15:43:18:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:43:18:  ---------
04/14 15:44:33:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:44:33:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:44:33:  ---------
04/14 15:45:48:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/14 15:45:48:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:45:48:  ---------
04/14 15:47:04:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/14 15:47:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:47:04:  ---------
04/14 15:48:19:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/14 15:48:19:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:48:19:  ---------
04/14 15:49:34:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/14 15:49:34:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:49:34:  ---------
04/14 15:50:50:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/14 15:50:50:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:50:50:  ---------
04/14 15:52:05:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/14 15:52:05:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/14 15:52:05:  ---------
04/14 15:53:20:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:53:20:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:53:20:  ---------
04/14 15:54:35:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/14 15:54:35:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:54:35:  ---------
04/14 15:55:51:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/14 15:55:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:55:51:  ---------
04/14 15:57:06:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/14 15:57:06:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:57:06:  ---------
04/14 15:58:21:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/14 15:58:21:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:58:21:  ---------
04/14 15:59:37:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/14 15:59:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:59:37:  ---------
04/14 16:00:52:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/14 16:00:52:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 16:00:52:  ---------
04/14 16:02:07:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/14 16:02:07:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/14 16:02:07:  ---------
04/14 16:03:22:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:03:22:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:03:22:  ---------
04/14 16:04:38:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/14 16:04:38:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:04:38:  ---------
04/14 16:05:53:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:05:53:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:05:53:  ---------
04/14 16:07:09:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/14 16:07:09:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:07:09:  ---------
04/14 16:08:24:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/14 16:08:24:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:08:24:  ---------
04/14 16:09:39:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/14 16:09:39:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:09:39:  ---------
04/14 16:10:54:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/14 16:10:54:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:10:54:  ---------
04/14 16:12:10:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/14 16:12:10:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:12:10:  ---------
04/14 16:13:25:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:13:25:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:13:25:  ---------
04/14 16:14:40:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/14 16:14:40:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:14:40:  ---------
04/14 16:15:56:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/14 16:15:56:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:15:56:  ---------
04/14 16:17:11:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/14 16:17:11:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:17:11:  ---------
04/14 16:18:26:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/14 16:18:26:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:18:26:  ---------
04/14 16:19:42:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:19:42:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:19:42:  ---------
04/14 16:20:57:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/14 16:20:57:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:20:57:  ---------
04/14 16:22:13:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/14 16:22:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:22:13:  ---------
04/14 16:23:28:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/14 16:23:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:23:28:  ---------
04/14 16:24:43:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/14 16:24:43:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:24:43:  ---------
04/14 16:25:58:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/14 16:25:58:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:25:58:  ---------
04/14 16:27:14:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/14 16:27:14:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:27:14:  ---------
04/14 16:28:29:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/14 16:28:29:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:28:29:  ---------
04/14 16:29:44:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/14 16:29:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:29:44:  ---------
04/14 16:30:59:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/14 16:30:59:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:30:59:  ---------
04/14 16:32:15:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/14 16:32:15:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:32:15:  ---------
