04/10 17:32:11:  ======================== New Round =============================
04/10 17:32:11:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 17:32:11:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 17:41:12:  #RES: f1:48.771, precision:49.749, recall:47.830, loss:5.942 at 2
04/10 17:41:12:  Best: f1:48.771, precision:49.749, recall:47.830, loss:5.942 at 2
04/10 17:41:12:  ---------
04/10 17:50:10:  #RES: f1:50.370, precision:48.442, recall:52.459, loss:8.647 at 4
04/10 17:50:10:  Best: f1:50.370, precision:48.442, recall:52.459, loss:8.647 at 4
04/10 17:50:10:  ---------
04/10 17:59:08:  #RES: f1:50.763, precision:50.236, recall:51.302, loss:9.801 at 6
04/10 17:59:08:  Best: f1:50.763, precision:50.236, recall:51.302, loss:9.801 at 6
04/10 17:59:08:  ---------
04/10 18:08:06:  #RES: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:08:06:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:08:06:  ---------
04/10 18:14:43:  #RES: f1:43.884, precision:42.534, recall:45.323, loss:12.933 at 11
04/10 18:14:43:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:14:43:  ---------
04/10 18:18:27:  #RES: f1:46.227, precision:45.514, recall:46.962, loss:14.191 at 13
04/10 18:18:27:  Best: f1:53.853, precision:53.146, recall:54.581, loss:10.062 at 9
04/10 18:18:27:  ---------
04/10 18:22:11:  #RES: f1:60.079, precision:61.284, recall:58.920, loss:17.107 at 15
04/10 18:22:11:  Best: f1:60.079, precision:61.284, recall:58.920, loss:17.107 at 15
04/10 18:22:11:  ---------
04/10 18:25:55:  #RES: f1:62.553, precision:60.708, recall:64.513, loss:15.253 at 18
04/10 18:25:55:  Best: f1:62.553, precision:60.708, recall:64.513, loss:15.253 at 18
04/10 18:25:55:  ---------
04/10 18:29:40:  #RES: f1:65.911, precision:64.570, recall:67.310, loss:18.799 at 20
04/10 18:29:40:  Best: f1:65.911, precision:64.570, recall:67.310, loss:18.799 at 20
04/10 18:29:40:  ---------
04/10 18:33:24:  #RES: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:33:24:  Best: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:33:24:  ---------
04/10 18:37:08:  #RES: f1:64.921, precision:64.098, recall:65.767, loss:18.911 at 25
04/10 18:37:08:  Best: f1:65.947, precision:64.201, recall:67.792, loss:20.570 at 22
04/10 18:37:08:  ---------
04/10 18:40:53:  #RES: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:40:53:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:40:53:  ---------
04/10 18:44:37:  #RES: f1:65.839, precision:65.246, recall:66.442, loss:20.193 at 29
04/10 18:44:37:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:44:37:  ---------
04/10 18:48:21:  #RES: f1:66.321, precision:64.825, recall:67.888, loss:15.137 at 31
04/10 18:48:21:  Best: f1:66.352, precision:65.060, recall:67.695, loss:20.446 at 27
04/10 18:48:21:  ---------
04/10 18:52:06:  #RES: f1:67.137, precision:65.593, recall:68.756, loss:41.115 at 34
04/10 18:52:06:  Best: f1:67.137, precision:65.593, recall:68.756, loss:41.115 at 34
04/10 18:52:06:  ---------
04/10 18:55:50:  #RES: f1:67.325, precision:65.688, recall:69.045, loss:27.434 at 36
04/10 18:55:50:  Best: f1:67.325, precision:65.688, recall:69.045, loss:27.434 at 36
04/10 18:55:50:  ---------
04/10 18:59:35:  #RES: f1:67.482, precision:65.901, recall:69.142, loss:141.030 at 38
04/10 18:59:35:  Best: f1:67.482, precision:65.901, recall:69.142, loss:141.030 at 38
04/10 18:59:35:  ---------
04/10 19:03:19:  #RES: f1:67.485, precision:66.081, recall:68.949, loss:59.071 at 40
04/10 19:03:19:  Best: f1:67.485, precision:66.081, recall:68.949, loss:59.071 at 40
04/10 19:03:19:  ---------
04/10 19:07:03:  #RES: f1:67.958, precision:66.636, recall:69.335, loss:105.362 at 43
04/10 19:07:03:  Best: f1:67.958, precision:66.636, recall:69.335, loss:105.362 at 43
04/10 19:07:03:  ---------
04/10 19:10:47:  #RES: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:10:47:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:10:47:  ---------
04/10 19:14:49:  #RES: f1:67.045, precision:65.860, recall:68.274, loss:39.363 at 47
04/10 19:14:49:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:14:49:  ---------
04/10 19:18:12:  ======================== New Round =============================
04/10 19:18:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:18:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:19:05:  ======================== New Round =============================
04/10 19:19:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:19:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:19:34:  #RES: f1:67.551, precision:66.387, recall:68.756, loss:182.905 at 50
04/10 19:19:34:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:19:34:  ---------
04/10 19:23:19:  #RES: f1:66.698, precision:65.549, recall:67.888, loss:267.859 at 52
04/10 19:23:19:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:23:19:  ---------
04/10 19:27:03:  #RES: f1:66.540, precision:65.424, recall:67.695, loss:1039.897 at 54
04/10 19:27:03:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:27:03:  ---------
04/10 19:30:48:  #RES: f1:63.218, precision:62.243, recall:64.224, loss:262.333 at 56
04/10 19:30:48:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:30:48:  ---------
04/10 19:32:24:  ======================== New Round =============================
04/10 19:32:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:32:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:03:  ======================== New Round =============================
04/10 19:33:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:35:  ======================== New Round =============================
04/10 19:33:35:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:35:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:33:56:  ======================== New Round =============================
04/10 19:33:56:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:33:56:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:34:44:  #RES: f1:66.667, precision:65.760, recall:67.599, loss:646.981 at 59
04/10 19:34:44:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:34:44:  ---------
04/10 19:36:15:  ======================== New Round =============================
04/10 19:36:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:36:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:36:45:  ======================== New Round =============================
04/10 19:36:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:36:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:37:14:  ======================== New Round =============================
04/10 19:37:14:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:37:14:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:37:36:  ======================== New Round =============================
04/10 19:37:36:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:37:36:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:38:41:  #RES: f1:66.255, precision:65.323, recall:67.213, loss:352.420 at 61
04/10 19:38:41:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:38:41:  ---------
04/10 19:39:20:  ======================== New Round =============================
04/10 19:39:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:39:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:40:53:  ======================== New Round =============================
04/10 19:40:53:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:40:53:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:43:56:  #RES: f1:66.540, precision:65.604, recall:67.502, loss:344.437 at 63
04/10 19:43:56:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:43:56:  ---------
04/10 19:43:57:  ======================== New Round =============================
04/10 19:43:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:43:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:46:14:  #RES: f1:50.212, precision:49.079, recall:51.398, loss:5.935 at 2
04/10 19:46:14:  Best: f1:50.212, precision:49.079, recall:51.398, loss:5.935 at 2
04/10 19:46:14:  ---------
04/10 19:47:17:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 19:47:17:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 19:47:17:  ---------
04/10 19:50:34:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:50:34:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:50:34:  ---------
04/10 19:53:35:  #RES: f1:51.637, precision:50.136, recall:53.230, loss:7.654 at 4
04/10 19:53:35:  Best: f1:51.637, precision:50.136, recall:53.230, loss:7.654 at 4
04/10 19:53:35:  ---------
04/10 19:53:43:  ======================== New Round =============================
04/10 19:53:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:53:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:53:53:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/10 19:53:53:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:53:53:  ---------
04/10 19:55:27:  #RES: f1:66.475, precision:66.126, recall:66.827, loss:504.793 at 65
04/10 19:55:27:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 19:55:27:  ---------
04/10 19:55:37:  ======================== New Round =============================
04/10 19:55:37:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:55:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:56:06:  ======================== New Round =============================
04/10 19:56:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:56:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:57:12:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/10 19:57:12:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 19:57:12:  ---------
04/10 19:57:45:  ======================== New Round =============================
04/10 19:57:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:57:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 19:58:58:  ======================== New Round =============================
04/10 19:58:58:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 19:58:58:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:00:33:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:00:33:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:00:33:  ---------
04/10 20:00:59:  #RES: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:00:59:  Best: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:00:59:  ---------
04/10 20:03:49:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/10 20:03:49:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:03:49:  ---------
04/10 20:05:07:  ======================== New Round =============================
04/10 20:05:07:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:05:07:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:05:54:  ======================== New Round =============================
04/10 20:05:54:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:05:54:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:06:34:  ======================== New Round =============================
04/10 20:06:34:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:06:34:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:07:11:  #RES: f1:66.604, precision:64.927, recall:68.370, loss:3636.930 at 68
04/10 20:07:11:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:07:11:  ---------
04/10 20:07:15:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/10 20:07:15:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:07:15:  ---------
04/10 20:07:44:  ======================== New Round =============================
04/10 20:07:44:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:07:44:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:08:27:  #RES: f1:54.267, precision:53.956, recall:54.581, loss:9.251 at 9
04/10 20:08:27:  Best: f1:55.977, precision:54.471, recall:57.570, loss:8.093 at 6
04/10 20:08:27:  ---------
04/10 20:10:34:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/10 20:10:34:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:10:34:  ---------
04/10 20:13:52:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/10 20:13:52:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:13:52:  ---------
04/10 20:15:48:  #RES: f1:56.303, precision:54.570, recall:58.149, loss:10.840 at 11
04/10 20:15:48:  Best: f1:56.303, precision:54.570, recall:58.149, loss:10.840 at 11
04/10 20:15:48:  ---------
04/10 20:17:08:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/10 20:17:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 20:17:08:  ---------
04/10 20:17:28:  ======================== New Round =============================
04/10 20:17:28:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:17:28:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:18:12:  ======================== New Round =============================
04/10 20:18:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:18:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:18:48:  #RES: f1:66.089, precision:62.662, recall:69.913, loss:592.917 at 70
04/10 20:18:48:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:18:48:  ---------
04/10 20:20:28:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/10 20:20:28:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/10 20:20:28:  ---------
04/10 20:21:04:  ======================== New Round =============================
04/10 20:21:04:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:21:04:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:23:16:  #RES: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:23:16:  Best: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:23:16:  ---------
04/10 20:23:20:  ======================== New Round =============================
04/10 20:23:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:23:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:23:51:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:23:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:23:51:  ---------
04/10 20:24:41:  ======================== New Round =============================
04/10 20:24:41:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:24:41:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:25:13:  ======================== New Round =============================
04/10 20:25:13:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:25:13:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:25:58:  ======================== New Round =============================
04/10 20:25:58:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:25:58:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:27:17:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/10 20:27:17:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:27:17:  ---------
04/10 20:29:15:  ======================== New Round =============================
04/10 20:29:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:29:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:29:49:  ======================== New Round =============================
04/10 20:29:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:29:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=1), device_id='cuda:1', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:18:  ======================== New Round =============================
04/10 20:30:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:30:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:35:  #RES: f1:58.624, precision:57.327, recall:59.981, loss:912.953 at 72
04/10 20:30:35:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:30:35:  ---------
04/10 20:30:39:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/10 20:30:39:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:30:39:  ---------
04/10 20:30:40:  ======================== New Round =============================
04/10 20:30:40:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 20:30:40:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 20:30:47:  #RES: f1:59.664, precision:59.351, recall:59.981, loss:14.277 at 15
04/10 20:30:47:  Best: f1:60.768, precision:59.054, recall:62.584, loss:13.456 at 13
04/10 20:30:47:  ---------
04/10 20:31:57:  #RES: f1:61.657, precision:61.598, recall:61.716, loss:0.183 at 2
04/10 20:31:57:  Best: f1:61.657, precision:61.598, recall:61.716, loss:0.183 at 2
04/10 20:31:57:  ---------
04/10 20:33:13:  #RES: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:33:13:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:33:13:  ---------
04/10 20:33:57:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/10 20:33:57:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:33:57:  ---------
04/10 20:34:30:  #RES: f1:62.620, precision:62.560, recall:62.681, loss:0.276 at 6
04/10 20:34:30:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:34:30:  ---------
04/10 20:35:46:  #RES: f1:64.004, precision:63.881, recall:64.127, loss:0.333 at 9
04/10 20:35:46:  Best: f1:64.338, precision:64.746, recall:63.934, loss:0.244 at 4
04/10 20:35:46:  ---------
04/10 20:37:03:  #RES: f1:64.779, precision:64.470, recall:65.092, loss:0.389 at 11
04/10 20:37:03:  Best: f1:64.779, precision:64.470, recall:65.092, loss:0.389 at 11
04/10 20:37:03:  ---------
04/10 20:37:14:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/10 20:37:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:37:14:  ---------
04/10 20:38:06:  #RES: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:38:06:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:38:06:  ---------
04/10 20:38:20:  #RES: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:38:20:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:38:20:  ---------
04/10 20:39:37:  #RES: f1:64.821, precision:64.178, recall:65.477, loss:0.437 at 15
04/10 20:39:37:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:39:37:  ---------
04/10 20:40:31:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/10 20:40:31:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:40:31:  ---------
04/10 20:40:54:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.435 at 18
04/10 20:40:54:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:40:54:  ---------
04/10 20:42:07:  #RES: f1:61.458, precision:60.372, recall:62.584, loss:412.248 at 75
04/10 20:42:07:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:42:07:  ---------
04/10 20:42:12:  #RES: f1:65.361, precision:64.773, recall:65.959, loss:0.469 at 20
04/10 20:42:12:  Best: f1:66.698, precision:65.730, recall:67.695, loss:0.385 at 13
04/10 20:42:12:  ---------
04/10 20:43:29:  #RES: f1:66.763, precision:66.699, recall:66.827, loss:0.457 at 22
04/10 20:43:29:  Best: f1:66.763, precision:66.699, recall:66.827, loss:0.457 at 22
04/10 20:43:29:  ---------
04/10 20:43:47:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/10 20:43:47:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/10 20:43:47:  ---------
04/10 20:44:46:  #RES: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:44:46:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:44:46:  ---------
04/10 20:45:24:  #RES: f1:64.279, precision:65.435, recall:63.163, loss:18.707 at 20
04/10 20:45:24:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:45:24:  ---------
04/10 20:46:03:  #RES: f1:66.412, precision:65.814, recall:67.020, loss:0.460 at 27
04/10 20:46:03:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:46:03:  ---------
04/10 20:47:03:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/10 20:47:03:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/10 20:47:03:  ---------
04/10 20:47:20:  #RES: f1:66.378, precision:66.124, recall:66.635, loss:0.489 at 29
04/10 20:47:20:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:47:20:  ---------
04/10 20:48:37:  #RES: f1:66.825, precision:65.886, recall:67.792, loss:0.491 at 31
04/10 20:48:37:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:48:37:  ---------
04/10 20:49:54:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.501 at 34
04/10 20:49:54:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:49:54:  ---------
04/10 20:50:21:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:50:21:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:50:21:  ---------
04/10 20:51:11:  #RES: f1:66.216, precision:66.376, recall:66.056, loss:0.476 at 36
04/10 20:51:11:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:51:11:  ---------
04/10 20:52:29:  #RES: f1:65.357, precision:64.952, recall:65.767, loss:0.500 at 38
04/10 20:52:29:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:52:29:  ---------
04/10 20:52:43:  #RES: f1:63.962, precision:62.604, recall:65.381, loss:16.208 at 22
04/10 20:52:43:  Best: f1:64.621, precision:62.388, recall:67.020, loss:14.275 at 18
04/10 20:52:43:  ---------
04/10 20:53:37:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/10 20:53:37:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/10 20:53:37:  ---------
04/10 20:53:39:  #RES: f1:64.389, precision:64.265, recall:64.513, loss:115.893 at 77
04/10 20:53:39:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 20:53:39:  ---------
04/10 20:53:46:  #RES: f1:66.088, precision:66.120, recall:66.056, loss:0.507 at 40
04/10 20:53:46:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:53:46:  ---------
04/10 20:55:03:  #RES: f1:65.811, precision:64.916, recall:66.731, loss:0.526 at 43
04/10 20:55:03:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:55:03:  ---------
04/10 20:56:23:  #RES: f1:66.030, precision:65.436, recall:66.635, loss:0.525 at 45
04/10 20:56:23:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:56:23:  ---------
04/10 20:56:58:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 20:56:58:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 20:56:58:  ---------
04/10 20:57:48:  #RES: f1:66.311, precision:66.764, recall:65.863, loss:0.554 at 47
04/10 20:57:48:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:57:48:  ---------
04/10 20:59:09:  #RES: f1:66.828, precision:67.121, recall:66.538, loss:0.539 at 49
04/10 20:59:09:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 20:59:09:  ---------
04/10 21:00:12:  #RES: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:00:12:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:00:12:  ---------
04/10 21:00:21:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/10 21:00:21:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:00:21:  ---------
04/10 21:00:32:  #RES: f1:67.144, precision:66.415, recall:67.888, loss:0.524 at 52
04/10 21:00:32:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:00:32:  ---------
04/10 21:01:49:  #RES: f1:66.408, precision:66.764, recall:66.056, loss:0.542 at 54
04/10 21:01:49:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:01:49:  ---------
04/10 21:03:13:  #RES: f1:66.088, precision:66.120, recall:66.056, loss:0.540 at 56
04/10 21:03:13:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:03:13:  ---------
04/10 21:03:47:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/10 21:03:47:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:03:47:  ---------
04/10 21:04:30:  #RES: f1:67.113, precision:66.635, recall:67.599, loss:0.562 at 59
04/10 21:04:30:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:04:30:  ---------
04/10 21:05:24:  #RES: f1:66.382, precision:65.388, recall:67.406, loss:484.777 at 79
04/10 21:05:24:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:05:24:  ---------
04/10 21:05:47:  #RES: f1:67.015, precision:66.073, recall:67.985, loss:0.546 at 61
04/10 21:05:47:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:05:47:  ---------
04/10 21:07:11:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/10 21:07:11:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:07:11:  ---------
04/10 21:07:12:  #RES: f1:66.093, precision:65.560, recall:66.635, loss:0.567 at 63
04/10 21:07:12:  Best: f1:67.211, precision:66.922, recall:67.502, loss:0.453 at 24
04/10 21:07:12:  ---------
04/10 21:07:43:  #RES: f1:64.353, precision:62.648, recall:66.152, loss:22.313 at 27
04/10 21:07:43:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:07:43:  ---------
04/10 21:08:37:  #RES: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:08:37:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:08:37:  ---------
04/10 21:10:03:  #RES: f1:66.603, precision:66.379, recall:66.827, loss:0.542 at 68
04/10 21:10:03:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:10:03:  ---------
04/10 21:10:40:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/10 21:10:40:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:10:40:  ---------
04/10 21:11:34:  #RES: f1:65.538, precision:63.945, recall:67.213, loss:0.572 at 70
04/10 21:11:34:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:11:34:  ---------
04/10 21:13:06:  #RES: f1:65.897, precision:65.739, recall:66.056, loss:0.589 at 72
04/10 21:13:06:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:13:06:  ---------
04/10 21:14:02:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/10 21:14:02:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/10 21:14:02:  ---------
04/10 21:14:37:  #RES: f1:66.346, precision:66.250, recall:66.442, loss:0.552 at 74
04/10 21:14:37:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:14:37:  ---------
04/10 21:15:20:  #RES: f1:64.389, precision:64.265, recall:64.513, loss:24.936 at 29
04/10 21:15:20:  Best: f1:65.716, precision:64.822, recall:66.635, loss:20.151 at 25
04/10 21:15:20:  ---------
04/10 21:16:08:  #RES: f1:66.060, precision:65.589, recall:66.538, loss:0.563 at 77
04/10 21:16:08:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:16:08:  ---------
04/10 21:17:10:  #RES: f1:66.350, precision:65.327, recall:67.406, loss:362.054 at 81
04/10 21:17:10:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:17:10:  ---------
04/10 21:17:21:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:17:21:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:17:21:  ---------
04/10 21:17:39:  #RES: f1:65.995, precision:65.649, recall:66.345, loss:0.559 at 79
04/10 21:17:39:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:17:39:  ---------
04/10 21:19:09:  #RES: f1:66.924, precision:66.924, recall:66.924, loss:0.585 at 81
04/10 21:19:09:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:19:09:  ---------
04/10 21:20:39:  #RES: f1:66.828, precision:67.022, recall:66.635, loss:0.585 at 84
04/10 21:20:39:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:20:39:  ---------
04/10 21:20:41:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/10 21:20:41:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:20:41:  ---------
04/10 21:22:10:  #RES: f1:66.890, precision:66.476, recall:67.310, loss:0.584 at 86
04/10 21:22:10:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:22:10:  ---------
04/10 21:22:43:  #RES: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:22:43:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:22:43:  ---------
04/10 21:23:41:  #RES: f1:65.968, precision:65.221, recall:66.731, loss:0.576 at 88
04/10 21:23:41:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:23:41:  ---------
04/10 21:24:00:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/10 21:24:00:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:24:00:  ---------
04/10 21:25:11:  #RES: f1:66.380, precision:65.752, recall:67.020, loss:0.566 at 90
04/10 21:25:11:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:25:11:  ---------
04/10 21:26:32:  #RES: f1:67.344, precision:67.573, recall:67.117, loss:0.566 at 93
04/10 21:26:32:  Best: f1:67.379, precision:67.840, recall:66.924, loss:0.558 at 65
04/10 21:26:32:  ---------
04/10 21:27:18:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/10 21:27:18:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:27:18:  ---------
04/10 21:27:49:  #RES: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:27:49:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:27:49:  ---------
04/10 21:28:42:  #RES: f1:65.966, precision:65.403, recall:66.538, loss:820.967 at 84
04/10 21:28:42:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:28:42:  ---------
04/10 21:29:06:  #RES: f1:67.212, precision:67.115, recall:67.310, loss:0.575 at 97
04/10 21:29:06:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:29:06:  ---------
04/10 21:30:02:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:13.487 at 34
04/10 21:30:02:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:30:02:  ---------
04/10 21:30:23:  #RES: f1:67.084, precision:66.955, recall:67.213, loss:0.574 at 99
04/10 21:30:23:  Best: f1:67.472, precision:67.636, recall:67.310, loss:0.572 at 95
04/10 21:30:23:  ---------
04/10 21:30:34:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/10 21:30:34:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/10 21:30:34:  ---------
04/10 21:34:01:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:34:01:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:34:01:  ---------
04/10 21:37:33:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/10 21:37:33:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:37:33:  ---------
04/10 21:37:46:  #RES: f1:65.748, precision:64.794, recall:66.731, loss:274.803 at 36
04/10 21:37:46:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:37:46:  ---------
04/10 21:40:25:  #RES: f1:58.049, precision:57.854, recall:58.245, loss:685.411 at 86
04/10 21:40:25:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:40:25:  ---------
04/10 21:41:00:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/10 21:41:00:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:41:00:  ---------
04/10 21:44:28:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/10 21:44:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:44:28:  ---------
04/10 21:45:26:  #RES: f1:65.717, precision:64.733, recall:66.731, loss:54.442 at 38
04/10 21:45:26:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:45:26:  ---------
04/10 21:47:53:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/10 21:47:53:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:47:53:  ---------
04/10 21:51:21:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/10 21:51:21:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:51:21:  ---------
04/10 21:52:09:  #RES: f1:64.193, precision:64.162, recall:64.224, loss:300.165 at 88
04/10 21:52:09:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 21:52:09:  ---------
04/10 21:53:03:  #RES: f1:65.781, precision:64.766, recall:66.827, loss:776.498 at 40
04/10 21:53:03:  Best: f1:65.789, precision:64.161, recall:67.502, loss:14.830 at 31
04/10 21:53:03:  ---------
04/10 21:54:46:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/10 21:54:46:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:54:46:  ---------
04/10 21:58:15:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/10 21:58:15:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 21:58:15:  ---------
04/10 22:00:45:  #RES: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:00:45:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:00:45:  ---------
04/10 22:01:44:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/10 22:01:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:01:44:  ---------
04/10 22:03:55:  #RES: f1:67.077, precision:65.831, recall:68.370, loss:312.426 at 90
04/10 22:03:55:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:03:55:  ---------
04/10 22:05:13:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/10 22:05:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:05:13:  ---------
04/10 22:08:34:  #RES: f1:65.564, precision:64.259, recall:66.924, loss:98.427 at 45
04/10 22:08:34:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:08:34:  ---------
04/10 22:08:44:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/10 22:08:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/10 22:08:44:  ---------
04/10 22:14:10:  #RES: f1:67.268, precision:66.292, recall:68.274, loss:317.774 at 93
04/10 22:14:10:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:14:10:  ---------
04/10 22:14:32:  #RES: f1:64.672, precision:63.704, recall:65.670, loss:192.335 at 47
04/10 22:14:32:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:14:32:  ---------
04/10 22:19:10:  #RES: f1:64.133, precision:63.202, recall:65.092, loss:171.023 at 50
04/10 22:19:10:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:19:10:  ---------
04/10 22:21:18:  #RES: f1:67.299, precision:66.170, recall:68.467, loss:1150.949 at 95
04/10 22:21:18:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:21:18:  ---------
04/10 22:23:47:  #RES: f1:64.458, precision:63.290, recall:65.670, loss:165.365 at 52
04/10 22:23:47:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:23:47:  ---------
04/10 22:28:24:  #RES: f1:67.047, precision:66.043, recall:68.081, loss:429.561 at 97
04/10 22:28:24:  Best: f1:67.991, precision:66.698, recall:69.335, loss:2005.097 at 45
04/10 22:28:24:  ---------
04/10 22:28:25:  #RES: f1:63.671, precision:62.488, recall:64.899, loss:276.744 at 54
04/10 22:28:25:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:28:25:  ---------
04/10 22:33:03:  #RES: f1:63.758, precision:63.394, recall:64.127, loss:1108.264 at 56
04/10 22:33:03:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:33:03:  ---------
04/10 22:35:52:  #RES: f1:65.318, precision:65.255, recall:65.381, loss:270.836 at 59
04/10 22:35:52:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:35:52:  ---------
04/10 22:37:36:  #RES: f1:58.055, precision:56.593, recall:59.595, loss:320.622 at 61
04/10 22:37:36:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:37:36:  ---------
04/10 22:39:20:  #RES: f1:63.394, precision:62.677, recall:64.127, loss:325.205 at 63
04/10 22:39:20:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:39:20:  ---------
04/10 22:41:04:  #RES: f1:64.995, precision:63.788, recall:66.249, loss:235.938 at 65
04/10 22:41:04:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:41:04:  ---------
04/10 22:42:47:  #RES: f1:64.348, precision:64.472, recall:64.224, loss:354.782 at 68
04/10 22:42:47:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:42:47:  ---------
04/10 22:44:32:  #RES: f1:63.504, precision:61.905, recall:65.188, loss:360.767 at 70
04/10 22:44:32:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:44:32:  ---------
04/10 22:46:17:  #RES: f1:64.596, precision:64.015, recall:65.188, loss:266.620 at 72
04/10 22:46:17:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:46:17:  ---------
04/10 22:48:02:  #RES: f1:63.524, precision:62.747, recall:64.320, loss:1834.763 at 75
04/10 22:48:02:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:48:02:  ---------
04/10 22:49:48:  #RES: f1:64.865, precision:63.806, recall:65.959, loss:398.005 at 77
04/10 22:49:48:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:49:48:  ---------
04/10 22:51:33:  #RES: f1:62.782, precision:61.228, recall:64.417, loss:1043.555 at 79
04/10 22:51:33:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:51:33:  ---------
04/10 22:53:18:  #RES: f1:63.163, precision:62.595, recall:63.742, loss:485.795 at 81
04/10 22:53:18:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:53:18:  ---------
04/10 22:55:03:  #RES: f1:62.846, precision:62.252, recall:63.452, loss:281.170 at 84
04/10 22:55:03:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:55:03:  ---------
04/10 22:56:48:  #RES: f1:64.538, precision:64.757, recall:64.320, loss:234.235 at 86
04/10 22:56:48:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:56:48:  ---------
04/10 22:58:33:  #RES: f1:64.697, precision:64.981, recall:64.417, loss:227.272 at 88
04/10 22:58:33:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 22:58:33:  ---------
04/10 23:00:19:  #RES: f1:64.906, precision:64.627, recall:65.188, loss:230.439 at 91
04/10 23:00:19:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:00:19:  ---------
04/10 23:02:04:  #RES: f1:65.167, precision:64.672, recall:65.670, loss:211.422 at 93
04/10 23:02:04:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:02:04:  ---------
04/10 23:03:49:  #RES: f1:64.748, precision:63.401, recall:66.152, loss:900.709 at 95
04/10 23:03:49:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:03:49:  ---------
04/10 23:05:34:  #RES: f1:64.194, precision:63.321, recall:65.092, loss:200.223 at 97
04/10 23:05:34:  Best: f1:66.098, precision:65.019, recall:67.213, loss:146.382 at 43
04/10 23:05:34:  ---------
04/10 23:46:47:  ======================== New Round =============================
04/10 23:46:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 23:46:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:47:30:  ======================== New Round =============================
04/10 23:47:30:  2015, add_gan:True, add_gan_loss: True, add_gpt: False, text_model deberta
04/10 23:47:30:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=True, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:48:18:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 23:48:18:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/10 23:48:18:  ---------
04/10 23:49:55:  ======================== New Round =============================
04/10 23:49:55:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/10 23:49:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/10 23:50:06:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:50:06:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:50:06:  ---------
04/10 23:50:45:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/10 23:50:45:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/10 23:50:45:  ---------
04/10 23:51:35:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/10 23:51:35:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/10 23:51:35:  ---------
04/10 23:52:24:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:52:24:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:52:24:  ---------
04/10 23:53:05:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/10 23:53:05:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:53:05:  ---------
04/10 23:53:13:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/10 23:53:13:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/10 23:53:13:  ---------
04/10 23:54:03:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/10 23:54:03:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/10 23:54:03:  ---------
04/10 23:54:53:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/10 23:54:53:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/10 23:54:53:  ---------
04/10 23:55:43:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:55:43:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:55:43:  ---------
04/10 23:56:05:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/10 23:56:05:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/10 23:56:05:  ---------
04/10 23:56:32:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/10 23:56:32:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/10 23:56:32:  ---------
04/10 23:57:22:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/10 23:57:22:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/10 23:57:22:  ---------
04/10 23:58:12:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/10 23:58:12:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/10 23:58:12:  ---------
04/10 23:59:01:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:01:  ---------
04/10 23:59:05:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 23:59:05:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/10 23:59:05:  ---------
04/10 23:59:51:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/10 23:59:51:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/10 23:59:51:  ---------
04/11 00:00:40:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/11 00:00:40:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:00:40:  ---------
04/11 00:01:30:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/11 00:01:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:01:30:  ---------
04/11 00:02:04:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/11 00:02:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:02:04:  ---------
04/11 00:02:19:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/11 00:02:19:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:02:19:  ---------
04/11 00:03:09:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/11 00:03:09:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:03:09:  ---------
04/11 00:03:59:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/11 00:03:59:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:03:59:  ---------
04/11 00:04:49:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/11 00:04:49:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:04:49:  ---------
04/11 00:05:04:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/11 00:05:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:05:04:  ---------
04/11 00:05:37:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/11 00:05:37:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:05:37:  ---------
04/11 00:06:26:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/11 00:06:26:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:06:26:  ---------
04/11 00:07:14:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/11 00:07:14:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:07:14:  ---------
04/11 00:07:54:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.211 at 2
04/11 00:07:54:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.211 at 2
04/11 00:07:54:  ---------
04/11 00:07:59:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/11 00:07:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:07:59:  ---------
04/11 00:08:02:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/11 00:08:02:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:08:02:  ---------
04/11 00:08:52:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/11 00:08:52:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:08:52:  ---------
04/11 00:09:42:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/11 00:09:42:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:09:42:  ---------
04/11 00:10:31:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/11 00:10:31:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:10:31:  ---------
04/11 00:10:59:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/11 00:10:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:10:59:  ---------
04/11 00:11:21:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/11 00:11:21:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:11:21:  ---------
04/11 00:12:11:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/11 00:12:11:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:12:11:  ---------
04/11 00:13:00:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/11 00:13:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:13:00:  ---------
04/11 00:13:50:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/11 00:13:50:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:13:50:  ---------
04/11 00:13:59:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/11 00:13:59:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/11 00:13:59:  ---------
04/11 00:14:40:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/11 00:14:40:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:14:40:  ---------
04/11 00:15:30:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/11 00:15:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 00:15:30:  ---------
04/11 00:16:20:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:16:20:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:16:20:  ---------
04/11 00:16:58:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/11 00:16:58:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/11 00:16:58:  ---------
04/11 00:17:09:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/11 00:17:09:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:17:09:  ---------
04/11 00:17:59:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/11 00:17:59:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:17:59:  ---------
04/11 00:18:48:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/11 00:18:48:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:18:48:  ---------
04/11 00:19:38:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/11 00:19:38:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 00:19:38:  ---------
04/11 00:19:58:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:19:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:19:58:  ---------
04/11 00:20:28:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:20:28:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:20:28:  ---------
04/11 00:21:17:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/11 00:21:17:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:21:17:  ---------
04/11 00:22:07:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/11 00:22:07:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:22:07:  ---------
04/11 00:22:56:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/11 00:22:56:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:22:56:  ---------
04/11 00:22:58:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/11 00:22:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:22:58:  ---------
04/11 00:23:46:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/11 00:23:46:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:23:46:  ---------
04/11 00:24:36:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/11 00:24:36:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:24:36:  ---------
04/11 00:25:26:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/11 00:25:26:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:25:26:  ---------
04/11 00:25:57:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/11 00:25:57:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:25:57:  ---------
04/11 00:26:15:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/11 00:26:15:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 00:26:15:  ---------
04/11 00:27:42:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/11 00:27:42:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:27:42:  ---------
04/11 00:28:19:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:28:19:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:28:19:  ---------
04/11 00:29:19:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/11 00:29:19:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:29:19:  ---------
04/11 00:30:59:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/11 00:30:59:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:30:59:  ---------
04/11 00:32:38:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/11 00:32:38:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/11 00:32:38:  ---------
04/11 00:34:17:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/11 00:34:17:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/11 00:34:17:  ---------
04/11 00:35:56:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:35:56:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:35:56:  ---------
04/11 00:37:35:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/11 00:37:35:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/11 00:37:35:  ---------
04/11 00:39:11:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:39:11:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:39:11:  ---------
04/11 00:39:31:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.337 at 6
04/11 00:39:31:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:39:31:  ---------
04/11 00:40:50:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/11 00:40:50:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:40:50:  ---------
04/11 00:42:28:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/11 00:42:28:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:42:28:  ---------
04/11 00:44:08:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/11 00:44:08:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:44:08:  ---------
04/11 00:45:46:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/11 00:45:46:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:45:46:  ---------
04/11 00:47:25:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/11 00:47:25:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/11 00:47:25:  ---------
04/11 00:49:04:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:49:04:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:49:04:  ---------
04/11 00:50:40:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/11 00:50:40:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:50:40:  ---------
04/11 00:50:42:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.408 at 9
04/11 00:50:42:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.280 at 4
04/11 00:50:42:  ---------
04/11 00:52:19:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/11 00:52:19:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:52:19:  ---------
04/11 00:53:58:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/11 00:53:58:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:53:58:  ---------
04/11 00:55:37:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/11 00:55:37:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/11 00:55:37:  ---------
04/11 00:57:17:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:57:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:57:17:  ---------
04/11 00:58:56:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/11 00:58:56:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 00:58:56:  ---------
04/11 01:00:35:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/11 01:00:35:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:00:35:  ---------
04/11 01:01:54:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:01:54:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:01:54:  ---------
04/11 01:02:10:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/11 01:02:10:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:02:10:  ---------
04/11 01:03:49:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/11 01:03:49:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:03:49:  ---------
04/11 01:05:28:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/11 01:05:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:05:28:  ---------
04/11 01:07:07:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/11 01:07:07:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:07:07:  ---------
04/11 01:08:47:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/11 01:08:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:08:47:  ---------
04/11 01:10:26:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/11 01:10:26:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:10:26:  ---------
04/11 01:12:04:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/11 01:12:04:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:12:04:  ---------
04/11 01:13:08:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.445 at 13
04/11 01:13:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:13:08:  ---------
04/11 01:13:39:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/11 01:13:39:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/11 01:13:39:  ---------
04/11 01:17:14:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.508 at 15
04/11 01:17:14:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:17:14:  ---------
04/11 01:19:48:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.491 at 18
04/11 01:19:48:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:19:48:  ---------
04/11 01:22:22:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.533 at 20
04/11 01:22:22:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:22:22:  ---------
04/11 01:24:55:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.512 at 22
04/11 01:24:55:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.421 at 11
04/11 01:24:55:  ---------
04/11 01:27:28:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.521 at 25
04/11 01:27:28:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.521 at 25
04/11 01:27:28:  ---------
04/11 01:30:03:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:30:03:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:30:03:  ---------
04/11 01:32:37:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.488 at 29
04/11 01:32:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:32:37:  ---------
04/11 01:35:10:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.529 at 31
04/11 01:35:10:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:35:10:  ---------
04/11 01:37:43:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.551 at 34
04/11 01:37:43:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:37:43:  ---------
04/11 01:40:17:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.571 at 36
04/11 01:40:17:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:40:17:  ---------
04/11 01:42:51:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.547 at 38
04/11 01:42:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:42:51:  ---------
04/11 01:45:24:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.550 at 40
04/11 01:45:24:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.553 at 27
04/11 01:45:24:  ---------
04/11 01:47:56:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.561 at 43
04/11 01:47:56:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.561 at 43
04/11 01:47:56:  ---------
04/11 01:50:30:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:50:30:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:50:30:  ---------
04/11 01:53:03:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.581 at 47
04/11 01:53:03:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.592 at 45
04/11 01:53:03:  ---------
04/11 01:55:36:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:55:36:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:55:36:  ---------
04/11 01:58:09:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.609 at 52
04/11 01:58:09:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 01:58:09:  ---------
04/11 02:00:43:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.547 at 54
04/11 02:00:43:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:00:43:  ---------
04/11 02:03:17:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.602 at 56
04/11 02:03:17:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:03:17:  ---------
04/11 02:05:50:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.626 at 59
04/11 02:05:50:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:05:50:  ---------
04/11 02:08:24:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.604 at 61
04/11 02:08:24:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.570 at 50
04/11 02:08:24:  ---------
04/11 02:10:57:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:10:57:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:10:57:  ---------
04/11 02:13:31:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.572 at 65
04/11 02:13:31:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:13:31:  ---------
04/11 02:16:05:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.529 at 68
04/11 02:16:05:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.597 at 63
04/11 02:16:05:  ---------
04/11 02:18:39:  #RES: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:18:39:  Best: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:18:39:  ---------
04/11 02:21:12:  #RES: f1:67.519, precision:66.326, recall:68.756, loss:0.592 at 72
04/11 02:21:12:  Best: f1:68.134, precision:67.616, recall:68.660, loss:0.554 at 70
04/11 02:21:12:  ---------
04/11 02:23:46:  #RES: f1:68.251, precision:67.291, recall:69.238, loss:0.588 at 75
04/11 02:23:46:  Best: f1:68.251, precision:67.291, recall:69.238, loss:0.588 at 75
04/11 02:23:46:  ---------
04/11 02:26:19:  #RES: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:26:19:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:26:19:  ---------
04/11 02:28:53:  #RES: f1:67.874, precision:67.107, recall:68.660, loss:0.600 at 79
04/11 02:28:53:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:28:53:  ---------
04/11 02:31:26:  #RES: f1:67.023, precision:67.515, recall:66.538, loss:0.618 at 81
04/11 02:31:26:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:31:26:  ---------
04/11 02:33:59:  #RES: f1:66.924, precision:66.924, recall:66.924, loss:0.621 at 84
04/11 02:33:59:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:33:59:  ---------
04/11 02:36:32:  #RES: f1:67.407, precision:67.505, recall:67.310, loss:0.618 at 86
04/11 02:36:32:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:36:32:  ---------
04/11 02:39:07:  #RES: f1:67.771, precision:66.543, recall:69.045, loss:0.642 at 88
04/11 02:39:07:  Best: f1:68.401, precision:68.335, recall:68.467, loss:0.566 at 77
04/11 02:39:07:  ---------
04/11 02:41:40:  #RES: f1:68.577, precision:67.928, recall:69.238, loss:0.629 at 90
04/11 02:41:40:  Best: f1:68.577, precision:67.928, recall:69.238, loss:0.629 at 90
04/11 02:41:40:  ---------
04/11 02:44:14:  #RES: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:44:14:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:44:14:  ---------
04/11 02:46:47:  #RES: f1:68.073, precision:67.684, recall:68.467, loss:0.635 at 95
04/11 02:46:47:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:46:47:  ---------
04/11 02:49:21:  #RES: f1:68.393, precision:68.034, recall:68.756, loss:0.639 at 97
04/11 02:49:21:  Best: f1:68.640, precision:67.958, recall:69.335, loss:0.635 at 93
04/11 02:49:21:  ---------
04/11 15:34:42:  ======================== New Round =============================
04/11 15:34:42:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:34:42:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:35:16:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:35:16:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:35:16:  ---------
04/11 15:35:57:  ======================== New Round =============================
04/11 15:35:57:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:35:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:36:37:  ======================== New Round =============================
04/11 15:36:37:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:36:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:02:  ======================== New Round =============================
04/11 15:37:02:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:37:02:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:17:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:37:17:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/11 15:37:17:  ---------
04/11 15:37:47:  ======================== New Round =============================
04/11 15:37:47:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 15:37:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=False, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 15:37:57:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/11 15:37:57:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/11 15:37:57:  ---------
04/11 15:38:03:  #RES: f1:48.740, precision:48.978, recall:48.505, loss:7.979 at 2
04/11 15:38:03:  Best: f1:48.740, precision:48.978, recall:48.505, loss:7.979 at 2
04/11 15:38:03:  ---------
04/11 15:38:43:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:38:43:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:38:43:  ---------
04/11 15:39:28:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/11 15:39:28:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/11 15:39:28:  ---------
04/11 15:40:11:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/11 15:40:11:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/11 15:40:11:  ---------
04/11 15:40:57:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/11 15:40:57:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/11 15:40:57:  ---------
04/11 15:41:24:  #RES: f1:20.489, precision:56.034, recall:12.536, loss:7.979 at 2
04/11 15:41:24:  Best: f1:20.489, precision:56.034, recall:12.536, loss:7.979 at 2
04/11 15:41:24:  ---------
04/11 15:41:37:  #RES: f1:50.121, precision:50.291, recall:49.952, loss:6.328 at 4
04/11 15:41:37:  Best: f1:50.121, precision:50.291, recall:49.952, loss:6.328 at 4
04/11 15:41:37:  ---------
04/11 15:41:41:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:41:41:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:41:41:  ---------
04/11 15:42:27:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/11 15:42:27:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/11 15:42:27:  ---------
04/11 15:43:08:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/11 15:43:08:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/11 15:43:08:  ---------
04/11 15:43:45:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/11 15:43:45:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/11 15:43:45:  ---------
04/11 15:44:23:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:23:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:23:  ---------
04/11 15:44:33:  #RES: f1:50.168, precision:49.905, recall:50.434, loss:6.328 at 4
04/11 15:44:33:  Best: f1:50.168, precision:49.905, recall:50.434, loss:6.328 at 4
04/11 15:44:33:  ---------
04/11 15:44:44:  #RES: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:44:44:  Best: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:44:44:  ---------
04/11 15:44:59:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/11 15:44:59:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:44:59:  ---------
04/11 15:45:37:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/11 15:45:37:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:45:37:  ---------
04/11 15:46:15:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/11 15:46:15:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:46:15:  ---------
04/11 15:46:53:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/11 15:46:53:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:46:53:  ---------
04/11 15:47:27:  #RES: f1:50.422, precision:49.043, recall:51.880, loss:6.807 at 6
04/11 15:47:27:  Best: f1:50.422, precision:49.043, recall:51.880, loss:6.807 at 6
04/11 15:47:27:  ---------
04/11 15:47:30:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/11 15:47:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:47:30:  ---------
04/11 15:47:37:  #RES: f1:47.245, precision:46.952, recall:47.541, loss:7.158 at 9
04/11 15:47:37:  Best: f1:51.207, precision:50.279, recall:52.170, loss:6.807 at 6
04/11 15:47:37:  ---------
04/11 15:48:07:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/11 15:48:07:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:48:07:  ---------
04/11 15:48:45:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/11 15:48:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:48:45:  ---------
04/11 15:49:22:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/11 15:49:22:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:49:22:  ---------
04/11 15:50:00:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/11 15:50:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:50:00:  ---------
04/11 15:50:20:  #RES: f1:51.659, precision:50.792, recall:52.555, loss:7.158 at 9
04/11 15:50:20:  Best: f1:51.659, precision:50.792, recall:52.555, loss:7.158 at 9
04/11 15:50:20:  ---------
04/11 15:50:31:  #RES: f1:56.057, precision:55.243, recall:56.895, loss:7.211 at 11
04/11 15:50:31:  Best: f1:56.057, precision:55.243, recall:56.895, loss:7.211 at 11
04/11 15:50:31:  ---------
04/11 15:50:36:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/11 15:50:36:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:50:36:  ---------
04/11 15:51:14:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/11 15:51:14:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:51:14:  ---------
04/11 15:51:52:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/11 15:51:52:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:51:52:  ---------
04/11 15:52:29:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/11 15:52:29:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:52:29:  ---------
04/11 15:53:06:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/11 15:53:06:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:53:06:  ---------
04/11 15:53:14:  #RES: f1:56.159, precision:54.991, recall:57.377, loss:7.211 at 11
04/11 15:53:14:  Best: f1:56.159, precision:54.991, recall:57.377, loss:7.211 at 11
04/11 15:53:14:  ---------
04/11 15:53:26:  #RES: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:53:26:  Best: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:53:26:  ---------
04/11 15:53:43:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/11 15:53:43:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:53:43:  ---------
04/11 15:54:20:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/11 15:54:20:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:54:20:  ---------
04/11 15:54:58:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/11 15:54:58:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:54:58:  ---------
04/11 15:55:35:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/11 15:55:35:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:55:35:  ---------
04/11 15:56:10:  #RES: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:56:10:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:56:10:  ---------
04/11 15:56:12:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/11 15:56:12:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:56:12:  ---------
04/11 15:56:22:  #RES: f1:58.611, precision:57.840, recall:59.402, loss:9.189 at 15
04/11 15:56:22:  Best: f1:59.743, precision:59.040, recall:60.463, loss:7.791 at 13
04/11 15:56:22:  ---------
04/11 15:56:49:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/11 15:56:49:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/11 15:56:49:  ---------
04/11 15:57:26:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:57:26:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:57:26:  ---------
04/11 15:58:04:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/11 15:58:04:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:58:04:  ---------
04/11 15:58:41:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/11 15:58:41:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:58:41:  ---------
04/11 15:59:05:  #RES: f1:58.812, precision:57.959, recall:59.691, loss:9.189 at 15
04/11 15:59:05:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 15:59:05:  ---------
04/11 15:59:17:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/11 15:59:17:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:59:17:  ---------
04/11 15:59:17:  #RES: f1:61.816, precision:60.976, recall:62.681, loss:9.993 at 18
04/11 15:59:17:  Best: f1:61.816, precision:60.976, recall:62.681, loss:9.993 at 18
04/11 15:59:17:  ---------
04/11 15:59:55:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/11 15:59:55:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/11 15:59:55:  ---------
04/11 16:00:32:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:00:32:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:00:32:  ---------
04/11 16:01:10:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/11 16:01:10:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:01:10:  ---------
04/11 16:01:47:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/11 16:01:47:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:01:47:  ---------
04/11 16:01:59:  #RES: f1:60.837, precision:59.981, recall:61.716, loss:9.993 at 18
04/11 16:01:59:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 16:01:59:  ---------
04/11 16:02:12:  #RES: f1:62.732, precision:60.538, recall:65.092, loss:11.214 at 20
04/11 16:02:12:  Best: f1:62.732, precision:60.538, recall:65.092, loss:11.214 at 20
04/11 16:02:12:  ---------
04/11 16:02:24:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/11 16:02:24:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:02:24:  ---------
04/11 16:03:01:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/11 16:03:01:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:03:01:  ---------
04/11 16:03:39:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/11 16:03:39:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:03:39:  ---------
04/11 16:04:16:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/11 16:04:16:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:04:16:  ---------
04/11 16:04:53:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/11 16:04:53:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/11 16:04:53:  ---------
04/11 16:04:54:  #RES: f1:61.424, precision:59.353, recall:63.645, loss:11.214 at 20
04/11 16:04:54:  Best: f1:61.706, precision:60.671, recall:62.777, loss:7.791 at 13
04/11 16:04:54:  ---------
04/11 16:04:58:  #RES: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:04:58:  Best: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:04:58:  ---------
04/11 16:06:01:  #RES: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:06:01:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:06:01:  ---------
04/11 16:06:05:  #RES: f1:64.620, precision:63.333, recall:65.959, loss:13.393 at 24
04/11 16:06:05:  Best: f1:65.455, precision:64.957, recall:65.959, loss:12.544 at 22
04/11 16:06:05:  ---------
04/11 16:07:09:  #RES: f1:64.110, precision:62.977, recall:65.284, loss:13.393 at 24
04/11 16:07:09:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:07:09:  ---------
04/11 16:07:13:  #RES: f1:66.046, precision:63.874, recall:68.370, loss:14.056 at 27
04/11 16:07:13:  Best: f1:66.046, precision:63.874, recall:68.370, loss:14.056 at 27
04/11 16:07:13:  ---------
04/11 16:08:17:  #RES: f1:64.832, precision:62.782, recall:67.020, loss:14.056 at 27
04/11 16:08:17:  Best: f1:64.935, precision:64.779, recall:65.092, loss:12.544 at 22
04/11 16:08:17:  ---------
04/11 16:08:21:  #RES: f1:66.099, precision:64.842, recall:67.406, loss:14.220 at 29
04/11 16:08:21:  Best: f1:66.099, precision:64.842, recall:67.406, loss:14.220 at 29
04/11 16:08:21:  ---------
04/11 16:09:25:  #RES: f1:65.751, precision:64.618, recall:66.924, loss:14.220 at 29
04/11 16:09:25:  Best: f1:65.751, precision:64.618, recall:66.924, loss:14.220 at 29
04/11 16:09:25:  ---------
04/11 16:09:28:  #RES: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:09:28:  Best: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:09:28:  ---------
04/11 16:10:33:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:10:33:  Best: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:10:33:  ---------
04/11 16:10:36:  #RES: f1:65.101, precision:63.462, recall:66.827, loss:16.341 at 34
04/11 16:10:36:  Best: f1:66.603, precision:65.819, recall:67.406, loss:15.777 at 31
04/11 16:10:36:  ---------
04/11 16:11:41:  #RES: f1:63.756, precision:62.123, recall:65.477, loss:16.341 at 34
04/11 16:11:41:  Best: f1:66.857, precision:66.132, recall:67.599, loss:15.777 at 31
04/11 16:11:41:  ---------
04/11 16:11:44:  #RES: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:11:44:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:11:44:  ---------
04/11 16:12:49:  #RES: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:12:49:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:12:49:  ---------
04/11 16:12:51:  #RES: f1:64.309, precision:62.140, recall:66.635, loss:17.053 at 38
04/11 16:12:51:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:12:51:  ---------
04/11 16:13:57:  #RES: f1:63.492, precision:61.538, recall:65.574, loss:17.053 at 38
04/11 16:13:57:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:13:57:  ---------
04/11 16:13:59:  #RES: f1:65.419, precision:63.545, recall:67.406, loss:16.265 at 40
04/11 16:13:59:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:13:59:  ---------
04/11 16:15:05:  #RES: f1:64.981, precision:63.148, recall:66.924, loss:16.265 at 40
04/11 16:15:05:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:15:05:  ---------
04/11 16:15:07:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:16.379 at 43
04/11 16:15:07:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:15:07:  ---------
04/11 16:16:13:  #RES: f1:66.950, precision:65.587, recall:68.370, loss:16.379 at 43
04/11 16:16:13:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:16:13:  ---------
04/11 16:16:14:  #RES: f1:66.319, precision:65.177, recall:67.502, loss:17.415 at 45
04/11 16:16:14:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:16:14:  ---------
04/11 16:17:21:  #RES: f1:65.751, precision:64.618, recall:66.924, loss:17.415 at 45
04/11 16:17:21:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:17:21:  ---------
04/11 16:17:22:  #RES: f1:66.889, precision:66.102, recall:67.695, loss:16.983 at 47
04/11 16:17:22:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:17:22:  ---------
04/11 16:18:29:  #RES: f1:66.476, precision:65.663, recall:67.310, loss:16.983 at 47
04/11 16:18:29:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:18:29:  ---------
04/11 16:18:30:  #RES: f1:65.621, precision:64.728, recall:66.538, loss:16.996 at 49
04/11 16:18:30:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:18:30:  ---------
04/11 16:19:37:  #RES: f1:65.527, precision:64.546, recall:66.538, loss:16.996 at 49
04/11 16:19:37:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:19:37:  ---------
04/11 16:19:37:  #RES: f1:66.541, precision:64.982, recall:68.177, loss:17.661 at 52
04/11 16:19:37:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:19:37:  ---------
04/11 16:20:45:  #RES: f1:65.976, precision:64.430, recall:67.599, loss:17.661 at 52
04/11 16:20:45:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:20:45:  ---------
04/11 16:20:45:  #RES: f1:66.477, precision:65.482, recall:67.502, loss:17.699 at 54
04/11 16:20:45:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:20:45:  ---------
04/11 16:21:52:  #RES: f1:65.842, precision:64.977, recall:66.731, loss:17.699 at 54
04/11 16:21:52:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:21:52:  ---------
04/11 16:21:53:  #RES: f1:66.920, precision:65.979, recall:67.888, loss:17.879 at 56
04/11 16:21:53:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:21:53:  ---------
04/11 16:23:00:  #RES: f1:66.382, precision:65.388, recall:67.406, loss:17.879 at 56
04/11 16:23:00:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:23:00:  ---------
04/11 16:23:01:  #RES: f1:65.240, precision:63.040, recall:67.599, loss:18.649 at 59
04/11 16:23:01:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:23:01:  ---------
04/11 16:24:08:  #RES: f1:65.300, precision:63.153, recall:67.599, loss:18.649 at 59
04/11 16:24:08:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:24:08:  ---------
04/11 16:24:09:  #RES: f1:66.792, precision:65.374, recall:68.274, loss:18.159 at 61
04/11 16:24:09:  Best: f1:66.978, precision:64.946, recall:69.142, loss:16.057 at 36
04/11 16:24:09:  ---------
04/11 16:25:16:  #RES: f1:66.792, precision:65.285, recall:68.370, loss:18.159 at 61
04/11 16:25:16:  Best: f1:67.135, precision:65.328, recall:69.045, loss:16.057 at 36
04/11 16:25:16:  ---------
04/11 16:25:17:  #RES: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:25:17:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:25:17:  ---------
04/11 16:26:24:  #RES: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:26:24:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:26:24:  ---------
04/11 16:26:24:  #RES: f1:67.144, precision:66.509, recall:67.792, loss:18.260 at 65
04/11 16:26:24:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:26:24:  ---------
04/11 16:27:32:  #RES: f1:66.826, precision:66.350, recall:67.310, loss:18.260 at 65
04/11 16:27:32:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:27:32:  ---------
04/11 16:27:32:  #RES: f1:65.221, precision:63.779, recall:66.731, loss:18.695 at 68
04/11 16:27:32:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:27:32:  ---------
04/11 16:28:40:  #RES: f1:65.158, precision:63.745, recall:66.635, loss:18.695 at 68
04/11 16:28:40:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:28:40:  ---------
04/11 16:28:40:  #RES: f1:65.982, precision:63.924, recall:68.177, loss:19.493 at 70
04/11 16:28:40:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:28:40:  ---------
04/11 16:29:48:  #RES: f1:65.796, precision:63.743, recall:67.985, loss:19.493 at 70
04/11 16:29:48:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:29:48:  ---------
04/11 16:29:48:  #RES: f1:64.516, precision:62.613, recall:66.538, loss:19.424 at 72
04/11 16:29:48:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:29:48:  ---------
04/11 16:30:55:  #RES: f1:64.761, precision:62.990, recall:66.635, loss:19.424 at 72
04/11 16:30:55:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:30:55:  ---------
04/11 16:30:56:  #RES: f1:65.855, precision:64.026, recall:67.792, loss:19.553 at 74
04/11 16:30:56:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:30:56:  ---------
04/11 16:32:03:  #RES: f1:65.946, precision:64.286, recall:67.695, loss:19.553 at 74
04/11 16:32:03:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:32:03:  ---------
04/11 16:32:03:  #RES: f1:66.293, precision:64.253, recall:68.467, loss:19.903 at 77
04/11 16:32:03:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:32:03:  ---------
04/11 16:33:11:  #RES: f1:66.698, precision:64.674, recall:68.852, loss:19.925 at 79
04/11 16:33:11:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:33:11:  ---------
04/11 16:33:11:  #RES: f1:66.417, precision:64.487, recall:68.467, loss:19.903 at 77
04/11 16:33:11:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:33:11:  ---------
04/11 16:34:20:  #RES: f1:64.972, precision:63.477, recall:66.538, loss:20.388 at 81
04/11 16:34:20:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:34:20:  ---------
04/11 16:34:20:  #RES: f1:66.262, precision:64.279, recall:68.370, loss:19.925 at 79
04/11 16:34:20:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:34:20:  ---------
04/11 16:35:27:  #RES: f1:66.132, precision:64.728, recall:67.599, loss:20.226 at 84
04/11 16:35:27:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:35:27:  ---------
04/11 16:35:27:  #RES: f1:65.068, precision:63.486, recall:66.731, loss:20.388 at 81
04/11 16:35:27:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:35:27:  ---------
04/11 16:36:35:  #RES: f1:66.792, precision:65.023, recall:68.660, loss:19.997 at 86
04/11 16:36:35:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:36:35:  ---------
04/11 16:36:36:  #RES: f1:65.600, precision:64.062, recall:67.213, loss:20.226 at 84
04/11 16:36:36:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:36:36:  ---------
04/11 16:37:43:  #RES: f1:66.444, precision:65.877, recall:67.020, loss:19.767 at 88
04/11 16:37:43:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:37:43:  ---------
04/11 16:37:43:  #RES: f1:66.635, precision:64.813, recall:68.563, loss:19.997 at 86
04/11 16:37:43:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:37:43:  ---------
04/11 16:38:51:  #RES: f1:65.912, precision:64.483, recall:67.406, loss:20.123 at 90
04/11 16:38:51:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:38:51:  ---------
04/11 16:38:51:  #RES: f1:66.158, precision:65.501, recall:66.827, loss:19.767 at 88
04/11 16:38:51:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:38:51:  ---------
04/11 16:39:58:  #RES: f1:65.546, precision:63.529, recall:67.695, loss:20.496 at 93
04/11 16:39:58:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:39:58:  ---------
04/11 16:39:59:  #RES: f1:66.102, precision:64.581, recall:67.695, loss:20.123 at 90
04/11 16:39:59:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:39:59:  ---------
04/11 16:41:06:  #RES: f1:66.134, precision:64.555, recall:67.792, loss:20.380 at 95
04/11 16:41:06:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:41:06:  ---------
04/11 16:41:07:  #RES: f1:65.329, precision:63.291, recall:67.502, loss:20.496 at 93
04/11 16:41:07:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:41:07:  ---------
04/11 16:42:14:  #RES: f1:66.103, precision:64.410, recall:67.888, loss:20.513 at 97
04/11 16:42:14:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:42:14:  ---------
04/11 16:42:15:  #RES: f1:66.291, precision:64.679, recall:67.985, loss:20.380 at 95
04/11 16:42:15:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:42:15:  ---------
04/11 16:43:21:  #RES: f1:66.479, precision:64.776, recall:68.274, loss:20.407 at 99
04/11 16:43:21:  Best: f1:67.202, precision:65.894, recall:68.563, loss:17.957 at 63
04/11 16:43:21:  ---------
04/11 16:43:22:  #RES: f1:65.885, precision:64.168, recall:67.695, loss:20.513 at 97
04/11 16:43:22:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:43:22:  ---------
04/11 16:43:52:  #RES: f1:66.354, precision:64.625, recall:68.177, loss:20.407 at 99
04/11 16:43:52:  Best: f1:67.392, precision:66.171, recall:68.660, loss:17.957 at 63
04/11 16:43:52:  ---------
04/11 17:25:54:  ======================== New Round =============================
04/11 17:25:54:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:25:54:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:26:20:  ======================== New Round =============================
04/11 17:26:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:26:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:26:43:  ======================== New Round =============================
04/11 17:26:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/11 17:26:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='vit', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/11 17:28:52:  #RES: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:28:52:  Best: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:28:52:  ---------
04/11 17:31:03:  #RES: f1:58.889, precision:57.052, recall:60.849, loss:0.321 at 4
04/11 17:31:03:  Best: f1:60.283, precision:59.003, recall:61.620, loss:0.197 at 2
04/11 17:31:03:  ---------
04/11 17:33:14:  #RES: f1:62.518, precision:61.610, recall:63.452, loss:0.315 at 6
04/11 17:33:14:  Best: f1:62.518, precision:61.610, recall:63.452, loss:0.315 at 6
04/11 17:33:14:  ---------
04/11 17:35:26:  #RES: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:35:26:  Best: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:35:26:  ---------
04/11 17:37:38:  #RES: f1:61.635, precision:61.457, recall:61.813, loss:0.448 at 11
04/11 17:37:38:  Best: f1:64.477, precision:62.797, recall:66.249, loss:0.382 at 9
04/11 17:37:38:  ---------
04/11 17:39:49:  #RES: f1:65.855, precision:64.026, recall:67.792, loss:0.470 at 13
04/11 17:39:49:  Best: f1:65.855, precision:64.026, recall:67.792, loss:0.470 at 13
04/11 17:39:49:  ---------
04/11 17:42:01:  #RES: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:42:01:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:42:01:  ---------
04/11 17:44:12:  #RES: f1:64.818, precision:65.422, recall:64.224, loss:0.482 at 18
04/11 17:44:12:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:44:12:  ---------
04/11 17:46:24:  #RES: f1:65.481, precision:65.292, recall:65.670, loss:0.492 at 20
04/11 17:46:24:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:46:24:  ---------
04/11 17:48:35:  #RES: f1:65.161, precision:64.943, recall:65.381, loss:0.494 at 22
04/11 17:48:35:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:48:35:  ---------
04/11 17:50:47:  #RES: f1:66.320, precision:65.000, recall:67.695, loss:0.533 at 25
04/11 17:50:47:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:50:47:  ---------
04/11 17:52:59:  #RES: f1:66.379, precision:66.031, recall:66.731, loss:0.546 at 27
04/11 17:52:59:  Best: f1:66.470, precision:67.907, recall:65.092, loss:0.446 at 15
04/11 17:52:59:  ---------
04/11 17:55:10:  #RES: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:55:10:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:55:10:  ---------
04/11 17:57:22:  #RES: f1:64.602, precision:63.842, recall:65.381, loss:0.516 at 31
04/11 17:57:22:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:57:22:  ---------
04/11 17:59:33:  #RES: f1:66.508, precision:65.634, recall:67.406, loss:0.532 at 34
04/11 17:59:33:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 17:59:33:  ---------
04/11 18:01:45:  #RES: f1:65.676, precision:65.300, recall:66.056, loss:0.530 at 36
04/11 18:01:45:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:01:45:  ---------
04/11 18:03:56:  #RES: f1:64.672, precision:64.734, recall:64.609, loss:0.581 at 38
04/11 18:03:56:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:03:56:  ---------
04/11 18:06:08:  #RES: f1:65.906, precision:65.009, recall:66.827, loss:0.560 at 40
04/11 18:06:08:  Best: f1:66.634, precision:67.024, recall:66.249, loss:0.529 at 29
04/11 18:06:08:  ---------
04/11 18:08:19:  #RES: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:08:19:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:08:19:  ---------
04/11 18:10:31:  #RES: f1:66.798, precision:68.068, recall:65.574, loss:0.527 at 45
04/11 18:10:31:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:10:31:  ---------
04/11 18:12:43:  #RES: f1:64.703, precision:63.764, recall:65.670, loss:0.568 at 47
04/11 18:12:43:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:12:43:  ---------
04/11 18:14:54:  #RES: f1:65.601, precision:65.920, recall:65.284, loss:0.566 at 50
04/11 18:14:54:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:14:54:  ---------
04/11 18:17:06:  #RES: f1:65.901, precision:65.370, recall:66.442, loss:0.570 at 52
04/11 18:17:06:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:17:06:  ---------
04/11 18:19:18:  #RES: f1:66.188, precision:65.655, recall:66.731, loss:0.535 at 54
04/11 18:19:18:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:19:18:  ---------
04/11 18:21:30:  #RES: f1:66.731, precision:67.023, recall:66.442, loss:0.566 at 56
04/11 18:21:30:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:21:30:  ---------
04/11 18:23:42:  #RES: f1:65.316, precision:65.347, recall:65.284, loss:0.572 at 59
04/11 18:23:42:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:23:42:  ---------
04/11 18:25:53:  #RES: f1:65.081, precision:64.225, recall:65.959, loss:0.585 at 61
04/11 18:25:53:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:25:53:  ---------
04/11 18:28:05:  #RES: f1:65.460, precision:64.689, recall:66.249, loss:0.595 at 63
04/11 18:28:05:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:28:05:  ---------
04/11 18:30:17:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.564 at 65
04/11 18:30:17:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:30:17:  ---------
04/11 18:32:28:  #RES: f1:66.921, precision:66.257, recall:67.599, loss:0.551 at 68
04/11 18:32:28:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:32:28:  ---------
04/11 18:34:40:  #RES: f1:65.406, precision:64.133, recall:66.731, loss:0.585 at 70
04/11 18:34:40:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:34:40:  ---------
04/11 18:36:51:  #RES: f1:66.667, precision:66.991, recall:66.345, loss:0.590 at 72
04/11 18:36:51:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:36:51:  ---------
04/11 18:39:03:  #RES: f1:66.635, precision:65.789, recall:67.502, loss:0.570 at 75
04/11 18:39:03:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:39:03:  ---------
04/11 18:41:14:  #RES: f1:65.527, precision:64.546, recall:66.538, loss:0.582 at 77
04/11 18:41:14:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:41:14:  ---------
04/11 18:43:26:  #RES: f1:65.803, precision:65.458, recall:66.152, loss:0.574 at 79
04/11 18:43:26:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:43:26:  ---------
04/11 18:45:37:  #RES: f1:66.667, precision:67.189, recall:66.152, loss:0.567 at 81
04/11 18:45:37:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:45:37:  ---------
04/11 18:47:49:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.569 at 84
04/11 18:47:49:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:47:49:  ---------
04/11 18:50:01:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.577 at 86
04/11 18:50:01:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:50:01:  ---------
04/11 18:52:12:  #RES: f1:66.827, precision:66.539, recall:67.117, loss:0.585 at 88
04/11 18:52:12:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:52:12:  ---------
04/11 18:54:24:  #RES: f1:65.672, precision:65.577, recall:65.767, loss:0.593 at 90
04/11 18:54:24:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:54:24:  ---------
04/11 18:56:35:  #RES: f1:65.420, precision:65.076, recall:65.767, loss:0.597 at 93
04/11 18:56:35:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:56:35:  ---------
04/11 18:58:47:  #RES: f1:65.862, precision:65.957, recall:65.767, loss:0.600 at 95
04/11 18:58:47:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 18:58:47:  ---------
04/11 19:00:58:  #RES: f1:66.089, precision:65.931, recall:66.249, loss:0.599 at 97
04/11 19:00:58:  Best: f1:67.334, precision:66.604, recall:68.081, loss:0.562 at 43
04/11 19:00:58:  ---------
04/12 10:26:31:  ======================== New Round =============================
04/12 10:26:31:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:26:31:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:41:08:  ======================== New Round =============================
04/12 10:41:08:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:41:08:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:42:06:  ======================== New Round =============================
04/12 10:42:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:42:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:46:17:  ======================== New Round =============================
04/12 10:46:17:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:46:17:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:47:38:  ======================== New Round =============================
04/12 10:47:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:47:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:56:23:  ======================== New Round =============================
04/12 10:56:23:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:56:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:56:44:  #RES: f1:46.065, precision:46.132, recall:45.998, loss:0.287 at 0
04/12 10:56:44:  Best: f1:46.065, precision:46.132, recall:45.998, loss:0.287 at 0
04/12 10:56:44:  ---------
04/12 10:57:01:  ======================== New Round =============================
04/12 10:57:01:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:57:01:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:57:06:  #RES: f1:49.351, precision:49.232, recall:49.470, loss:0.256 at 0
04/12 10:57:06:  Best: f1:49.351, precision:49.232, recall:49.470, loss:0.256 at 0
04/12 10:57:06:  ---------
04/12 10:57:28:  #RES: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:28:  Best: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:28:  ---------
04/12 10:57:50:  #RES: f1:54.537, precision:55.183, recall:53.905, loss:0.268 at 0
04/12 10:57:50:  Best: f1:56.447, precision:55.913, recall:56.991, loss:0.241 at 0
04/12 10:57:50:  ---------
04/12 10:58:06:  ======================== New Round =============================
04/12 10:58:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 10:58:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=1, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 10:58:13:  #RES: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:13:  Best: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:13:  ---------
04/12 10:58:34:  #RES: f1:55.996, precision:54.857, recall:57.184, loss:0.243 at 0
04/12 10:58:34:  Best: f1:58.920, precision:58.920, recall:58.920, loss:0.225 at 0
04/12 10:58:34:  ---------
04/12 10:58:56:  #RES: f1:59.166, precision:59.512, recall:58.824, loss:0.228 at 0
04/12 10:58:56:  Best: f1:59.166, precision:59.512, recall:58.824, loss:0.228 at 0
04/12 10:58:56:  ---------
04/12 10:59:18:  #RES: f1:63.245, precision:63.521, recall:62.970, loss:0.231 at 1
04/12 10:59:18:  Best: f1:63.245, precision:63.521, recall:62.970, loss:0.231 at 1
04/12 10:59:18:  ---------
04/12 10:59:40:  #RES: f1:63.610, precision:63.671, recall:63.549, loss:0.241 at 1
04/12 10:59:40:  Best: f1:63.610, precision:63.671, recall:63.549, loss:0.241 at 1
04/12 10:59:40:  ---------
04/12 11:00:03:  #RES: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:03:  Best: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:03:  ---------
04/12 11:00:24:  #RES: f1:65.373, precision:65.756, recall:64.995, loss:0.250 at 1
04/12 11:00:24:  Best: f1:66.475, precision:66.126, recall:66.827, loss:0.237 at 1
04/12 11:00:24:  ---------
04/12 11:01:05:  ======================== New Round =============================
04/12 11:01:05:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:01:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:01:27:  #RES: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/12 11:01:27:  Best: f1:58.857, precision:58.137, recall:59.595, loss:0.188 at 2
04/12 11:01:27:  ---------
04/12 11:01:31:  ======================== New Round =============================
04/12 11:01:31:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:01:31:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:02:07:  #RES: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/12 11:02:07:  Best: f1:64.165, precision:64.012, recall:64.320, loss:0.227 at 4
04/12 11:02:07:  ---------
04/12 11:02:50:  #RES: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:02:50:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:02:50:  ---------
04/12 11:03:34:  #RES: f1:63.412, precision:63.566, recall:63.259, loss:0.331 at 9
04/12 11:03:34:  Best: f1:66.215, precision:66.472, recall:65.959, loss:0.274 at 6
04/12 11:03:34:  ---------
04/12 11:04:17:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 11:04:17:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 11:04:17:  ---------
04/12 11:04:18:  #RES: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/12 11:04:18:  Best: f1:66.413, precision:65.631, recall:67.213, loss:0.350 at 11
04/12 11:04:18:  ---------
04/12 11:05:02:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/12 11:05:02:  Best: f1:66.980, precision:65.381, recall:68.660, loss:0.372 at 13
04/12 11:05:02:  ---------
04/12 11:05:47:  #RES: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:05:47:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:05:47:  ---------
04/12 11:06:27:  ======================== New Round =============================
04/12 11:06:27:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:06:27:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:06:32:  #RES: f1:66.667, precision:65.851, recall:67.502, loss:0.424 at 18
04/12 11:06:32:  Best: f1:67.776, precision:66.823, recall:68.756, loss:0.410 at 15
04/12 11:06:32:  ---------
04/12 11:07:02:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:07:02:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:07:02:  ---------
04/12 11:07:16:  #RES: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/12 11:07:16:  Best: f1:67.960, precision:66.729, recall:69.238, loss:0.440 at 20
04/12 11:07:16:  ---------
04/12 11:07:27:  ======================== New Round =============================
04/12 11:07:27:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:07:27:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:08:02:  #RES: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/12 11:08:02:  Best: f1:67.996, precision:68.597, recall:67.406, loss:0.422 at 22
04/12 11:08:02:  ---------
04/12 11:08:10:  ======================== New Round =============================
04/12 11:08:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:08:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:08:47:  #RES: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:08:47:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:08:47:  ---------
04/12 11:09:24:  ======================== New Round =============================
04/12 11:09:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:09:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:09:33:  #RES: f1:67.265, precision:65.926, recall:68.660, loss:0.471 at 27
04/12 11:09:33:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:09:33:  ---------
04/12 11:09:48:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/12 11:09:48:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:09:48:  ---------
04/12 11:10:17:  #RES: f1:66.794, precision:66.008, recall:67.599, loss:0.477 at 29
04/12 11:10:17:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:10:17:  ---------
04/12 11:10:38:  ======================== New Round =============================
04/12 11:10:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:10:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:11:03:  #RES: f1:63.740, precision:63.078, recall:64.417, loss:0.469 at 31
04/12 11:11:03:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:11:03:  ---------
04/12 11:11:47:  ======================== New Round =============================
04/12 11:11:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:11:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:11:48:  #RES: f1:66.793, precision:65.824, recall:67.792, loss:0.479 at 34
04/12 11:11:48:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:11:48:  ---------
04/12 11:12:33:  #RES: f1:67.992, precision:68.390, recall:67.599, loss:0.476 at 36
04/12 11:12:33:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:12:33:  ---------
04/12 11:12:35:  ======================== New Round =============================
04/12 11:12:35:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:12:35:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:12:36:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/12 11:12:36:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 11:12:36:  ---------
04/12 11:13:12:  ======================== New Round =============================
04/12 11:13:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:13:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:13:19:  #RES: f1:66.146, precision:67.030, recall:65.284, loss:0.495 at 38
04/12 11:13:19:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:13:19:  ---------
04/12 11:14:04:  #RES: f1:67.079, precision:66.197, recall:67.985, loss:0.497 at 40
04/12 11:14:04:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:14:04:  ---------
04/12 11:14:48:  #RES: f1:68.034, precision:67.328, recall:68.756, loss:0.503 at 43
04/12 11:14:48:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:14:48:  ---------
04/12 11:15:21:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:15:21:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:15:21:  ---------
04/12 11:15:32:  #RES: f1:66.857, precision:66.132, recall:67.599, loss:0.530 at 45
04/12 11:15:32:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:15:32:  ---------
04/12 11:16:16:  #RES: f1:68.188, precision:68.689, recall:67.695, loss:0.532 at 47
04/12 11:16:16:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:16:16:  ---------
04/12 11:17:01:  #RES: f1:67.654, precision:67.045, recall:68.274, loss:0.524 at 49
04/12 11:17:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:17:01:  ---------
04/12 11:17:46:  #RES: f1:67.619, precision:66.792, recall:68.467, loss:0.536 at 52
04/12 11:17:46:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:17:46:  ---------
04/12 11:18:05:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/12 11:18:05:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:18:05:  ---------
04/12 11:18:30:  #RES: f1:66.731, precision:66.635, recall:66.827, loss:0.525 at 54
04/12 11:18:30:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:18:30:  ---------
04/12 11:19:06:  ======================== New Round =============================
04/12 11:19:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:19:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:19:15:  #RES: f1:66.289, precision:64.940, recall:67.695, loss:0.552 at 56
04/12 11:19:15:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:19:15:  ---------
04/12 11:19:50:  ======================== New Round =============================
04/12 11:19:50:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:19:50:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:20:01:  #RES: f1:66.540, precision:65.695, recall:67.406, loss:0.542 at 59
04/12 11:20:01:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:20:01:  ---------
04/12 11:20:45:  #RES: f1:67.825, precision:66.117, recall:69.624, loss:0.555 at 61
04/12 11:20:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:20:45:  ---------
04/12 11:20:51:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/12 11:20:51:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:20:51:  ---------
04/12 11:21:10:  ======================== New Round =============================
04/12 11:21:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:21:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:21:31:  #RES: f1:67.754, precision:67.431, recall:68.081, loss:0.549 at 63
04/12 11:21:31:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:21:31:  ---------
04/12 11:21:32:  ======================== New Round =============================
04/12 11:21:32:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:21:32:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:22:16:  #RES: f1:67.793, precision:67.892, recall:67.695, loss:0.536 at 65
04/12 11:22:16:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:22:16:  ---------
04/12 11:23:00:  #RES: f1:67.147, precision:66.890, recall:67.406, loss:0.536 at 68
04/12 11:23:00:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:23:00:  ---------
04/12 11:23:20:  ======================== New Round =============================
04/12 11:23:20:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:23:20:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:23:37:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/12 11:23:37:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:23:37:  ---------
04/12 11:23:45:  #RES: f1:67.653, precision:66.950, recall:68.370, loss:0.538 at 70
04/12 11:23:45:  Best: f1:68.293, precision:67.742, recall:68.852, loss:0.454 at 24
04/12 11:23:45:  ---------
04/12 11:24:30:  #RES: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:24:30:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:24:30:  ---------
04/12 11:24:33:  ======================== New Round =============================
04/12 11:24:33:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:24:33:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:25:15:  #RES: f1:67.711, precision:66.698, recall:68.756, loss:0.538 at 74
04/12 11:25:15:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:25:15:  ---------
04/12 11:25:47:  ======================== New Round =============================
04/12 11:25:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:25:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:26:00:  #RES: f1:67.811, precision:67.075, recall:68.563, loss:0.553 at 77
04/12 11:26:00:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:26:00:  ---------
04/12 11:26:22:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/12 11:26:22:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:26:22:  ---------
04/12 11:26:44:  #RES: f1:67.182, precision:67.345, recall:67.020, loss:0.536 at 79
04/12 11:26:44:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:26:44:  ---------
04/12 11:26:57:  ======================== New Round =============================
04/12 11:26:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:26:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:27:29:  #RES: f1:67.716, precision:66.981, recall:68.467, loss:0.550 at 81
04/12 11:27:29:  Best: f1:68.426, precision:68.099, recall:68.756, loss:0.542 at 72
04/12 11:27:29:  ---------
04/12 11:28:14:  #RES: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:14:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:14:  ---------
04/12 11:28:50:  ======================== New Round =============================
04/12 11:28:50:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:28:50:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:28:59:  #RES: f1:68.439, precision:68.605, recall:68.274, loss:0.563 at 86
04/12 11:28:59:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:28:59:  ---------
04/12 11:29:08:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/12 11:29:08:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 11:29:08:  ---------
04/12 11:29:32:  ======================== New Round =============================
04/12 11:29:32:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:29:32:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:29:46:  #RES: f1:68.110, precision:67.946, recall:68.274, loss:0.562 at 88
04/12 11:29:46:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:29:46:  ---------
04/12 11:30:10:  ======================== New Round =============================
04/12 11:30:10:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 11:30:10:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 11:30:39:  #RES: f1:67.990, precision:68.288, recall:67.695, loss:0.564 at 90
04/12 11:30:39:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:30:39:  ---------
04/12 11:31:13:  #RES: f1:45.164, precision:45.296, recall:45.034, loss:0.256 at 2
04/12 11:31:13:  Best: f1:45.164, precision:45.296, recall:45.034, loss:0.256 at 2
04/12 11:31:13:  ---------
04/12 11:31:45:  #RES: f1:68.142, precision:68.012, recall:68.274, loss:0.569 at 93
04/12 11:31:45:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:31:45:  ---------
04/12 11:32:13:  #RES: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:32:13:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:32:13:  ---------
04/12 11:32:43:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 11:32:43:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 11:32:43:  ---------
04/12 11:32:49:  #RES: f1:68.279, precision:68.477, recall:68.081, loss:0.565 at 95
04/12 11:32:49:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:32:49:  ---------
04/12 11:33:11:  #RES: f1:48.702, precision:47.689, recall:49.759, loss:0.319 at 6
04/12 11:33:11:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:33:11:  ---------
04/12 11:33:55:  #RES: f1:68.239, precision:68.108, recall:68.370, loss:0.564 at 97
04/12 11:33:55:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:33:55:  ---------
04/12 11:34:11:  #RES: f1:47.145, precision:47.727, recall:46.577, loss:0.346 at 9
04/12 11:34:11:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:34:11:  ---------
04/12 11:35:00:  #RES: f1:68.304, precision:68.239, recall:68.370, loss:0.563 at 99
04/12 11:35:00:  Best: f1:68.723, precision:68.690, recall:68.756, loss:0.557 at 84
04/12 11:35:00:  ---------
04/12 11:35:06:  #RES: f1:49.544, precision:49.331, recall:49.759, loss:0.388 at 11
04/12 11:35:06:  Best: f1:49.760, precision:49.569, recall:49.952, loss:0.263 at 4
04/12 11:35:06:  ---------
04/12 11:35:45:  #RES: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:35:45:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:35:45:  ---------
04/12 11:36:07:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:36:07:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:36:07:  ---------
04/12 11:36:23:  #RES: f1:51.755, precision:50.273, recall:53.327, loss:0.452 at 15
04/12 11:36:23:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:36:23:  ---------
04/12 11:37:02:  #RES: f1:47.428, precision:48.207, recall:46.673, loss:0.485 at 18
04/12 11:37:02:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:37:02:  ---------
04/12 11:37:40:  #RES: f1:47.105, precision:47.952, recall:46.287, loss:0.514 at 20
04/12 11:37:40:  Best: f1:52.093, precision:50.314, recall:54.002, loss:0.426 at 13
04/12 11:37:40:  ---------
04/12 11:38:19:  #RES: f1:53.282, precision:53.333, recall:53.230, loss:0.512 at 22
04/12 11:38:19:  Best: f1:53.282, precision:53.333, recall:53.230, loss:0.512 at 22
04/12 11:38:19:  ---------
04/12 11:38:45:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/12 11:38:45:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:38:45:  ---------
04/12 11:38:57:  #RES: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:38:57:  Best: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:38:57:  ---------
04/12 11:39:35:  #RES: f1:51.785, precision:51.128, recall:52.459, loss:0.559 at 27
04/12 11:39:35:  Best: f1:53.552, precision:55.225, recall:51.977, loss:0.545 at 24
04/12 11:39:35:  ---------
04/12 11:40:14:  #RES: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:14:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:14:  ---------
04/12 11:40:53:  #RES: f1:53.180, precision:52.751, recall:53.616, loss:0.582 at 31
04/12 11:40:53:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:40:53:  ---------
04/12 11:41:22:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/12 11:41:22:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:41:22:  ---------
04/12 11:41:31:  #RES: f1:52.261, precision:51.598, recall:52.941, loss:0.603 at 34
04/12 11:41:31:  Best: f1:53.718, precision:54.518, recall:52.941, loss:0.571 at 29
04/12 11:41:31:  ---------
04/12 11:42:10:  #RES: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:10:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:10:  ---------
04/12 11:42:49:  #RES: f1:53.748, precision:54.995, recall:52.555, loss:0.595 at 38
04/12 11:42:49:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:42:49:  ---------
04/12 11:43:27:  #RES: f1:53.462, precision:53.696, recall:53.230, loss:0.596 at 40
04/12 11:43:27:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:43:27:  ---------
04/12 11:44:00:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/12 11:44:00:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:44:00:  ---------
04/12 11:44:05:  #RES: f1:53.861, precision:54.609, recall:53.134, loss:0.625 at 43
04/12 11:44:05:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:44:05:  ---------
04/12 11:44:44:  #RES: f1:52.915, precision:53.785, recall:52.073, loss:0.618 at 45
04/12 11:44:44:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:44:44:  ---------
04/12 11:45:23:  #RES: f1:53.173, precision:53.020, recall:53.327, loss:0.616 at 47
04/12 11:45:23:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:45:23:  ---------
04/12 11:46:01:  #RES: f1:53.531, precision:54.858, recall:52.266, loss:0.629 at 49
04/12 11:46:01:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:46:01:  ---------
04/12 11:46:37:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/12 11:46:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:46:37:  ---------
04/12 11:46:39:  #RES: f1:53.944, precision:54.474, recall:53.423, loss:0.633 at 52
04/12 11:46:39:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:46:39:  ---------
04/12 11:47:18:  #RES: f1:52.967, precision:53.892, recall:52.073, loss:0.642 at 54
04/12 11:47:18:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:47:18:  ---------
04/12 11:47:57:  #RES: f1:52.715, precision:54.536, recall:51.013, loss:0.634 at 56
04/12 11:47:57:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:47:57:  ---------
04/12 11:48:36:  #RES: f1:53.556, precision:54.491, recall:52.652, loss:0.658 at 59
04/12 11:48:36:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:48:36:  ---------
04/12 11:49:13:  #RES: f1:52.243, precision:54.076, recall:50.530, loss:0.656 at 61
04/12 11:49:13:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:49:13:  ---------
04/12 11:49:14:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/12 11:49:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:49:14:  ---------
04/12 11:49:52:  #RES: f1:53.786, precision:54.154, recall:53.423, loss:0.657 at 63
04/12 11:49:52:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:49:52:  ---------
04/12 11:50:31:  #RES: f1:53.346, precision:54.059, recall:52.652, loss:0.646 at 65
04/12 11:50:31:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:50:31:  ---------
04/12 11:51:10:  #RES: f1:54.339, precision:55.600, recall:53.134, loss:0.678 at 68
04/12 11:51:10:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:51:10:  ---------
04/12 11:51:48:  #RES: f1:52.746, precision:54.167, recall:51.398, loss:0.659 at 70
04/12 11:51:48:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:51:48:  ---------
04/12 11:51:51:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/12 11:51:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 11:51:51:  ---------
04/12 11:52:27:  #RES: f1:53.552, precision:55.225, recall:51.977, loss:0.677 at 72
04/12 11:52:27:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:52:27:  ---------
04/12 11:53:05:  #RES: f1:53.193, precision:52.964, recall:53.423, loss:0.643 at 74
04/12 11:53:05:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:53:05:  ---------
04/12 11:53:44:  #RES: f1:53.281, precision:53.725, recall:52.845, loss:0.652 at 77
04/12 11:53:44:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:53:44:  ---------
04/12 11:54:22:  #RES: f1:54.267, precision:53.956, recall:54.581, loss:0.667 at 79
04/12 11:54:22:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:54:22:  ---------
04/12 11:54:27:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 11:54:27:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 11:54:27:  ---------
04/12 11:55:01:  #RES: f1:53.673, precision:55.705, recall:51.784, loss:0.668 at 81
04/12 11:55:01:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:55:01:  ---------
04/12 11:55:39:  #RES: f1:54.537, precision:55.489, recall:53.616, loss:0.664 at 84
04/12 11:55:39:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:55:39:  ---------
04/12 11:56:18:  #RES: f1:54.492, precision:55.193, recall:53.809, loss:0.675 at 86
04/12 11:56:18:  Best: f1:54.581, precision:55.791, recall:53.423, loss:0.603 at 36
04/12 11:56:18:  ---------
04/12 11:56:57:  #RES: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:56:57:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:56:57:  ---------
04/12 11:57:05:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:57:05:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:57:05:  ---------
04/12 11:57:35:  #RES: f1:54.448, precision:54.902, recall:54.002, loss:0.682 at 90
04/12 11:57:35:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:57:35:  ---------
04/12 11:58:14:  #RES: f1:54.652, precision:55.217, recall:54.098, loss:0.678 at 93
04/12 11:58:14:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:58:14:  ---------
04/12 11:58:52:  #RES: f1:54.599, precision:55.412, recall:53.809, loss:0.677 at 95
04/12 11:58:52:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:58:52:  ---------
04/12 11:59:31:  #RES: f1:54.608, precision:55.533, recall:53.713, loss:0.679 at 97
04/12 11:59:31:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 11:59:31:  ---------
04/12 11:59:42:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/12 11:59:42:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 11:59:42:  ---------
04/12 12:00:09:  #RES: f1:54.465, precision:55.445, recall:53.520, loss:0.679 at 99
04/12 12:00:09:  Best: f1:55.061, precision:55.853, recall:54.291, loss:0.677 at 88
04/12 12:00:09:  ---------
04/12 12:01:12:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:01:12:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:01:12:  ---------
04/12 12:02:27:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/12 12:02:27:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:02:27:  ---------
04/12 12:03:43:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/12 12:03:43:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:03:43:  ---------
04/12 12:04:59:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/12 12:04:59:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:04:59:  ---------
04/12 12:06:15:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/12 12:06:15:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:06:15:  ---------
04/12 12:07:31:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/12 12:07:31:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 12:07:31:  ---------
04/12 12:08:47:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:08:47:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:08:47:  ---------
04/12 12:10:03:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/12 12:10:03:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:10:03:  ---------
04/12 12:11:18:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/12 12:11:18:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:11:18:  ---------
04/12 12:12:34:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/12 12:12:34:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:12:34:  ---------
04/12 12:13:50:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/12 12:13:50:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 12:13:50:  ---------
04/12 12:15:06:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:15:06:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:15:06:  ---------
04/12 12:16:22:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/12 12:16:22:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:16:22:  ---------
04/12 12:17:38:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/12 12:17:38:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:17:38:  ---------
04/12 12:18:54:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/12 12:18:54:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:18:54:  ---------
04/12 12:20:10:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/12 12:20:10:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:20:10:  ---------
04/12 12:21:26:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/12 12:21:26:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:21:26:  ---------
04/12 12:22:41:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/12 12:22:41:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:22:41:  ---------
04/12 12:23:57:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/12 12:23:57:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:23:57:  ---------
04/12 12:25:13:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/12 12:25:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:25:13:  ---------
04/12 12:26:29:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/12 12:26:29:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:26:29:  ---------
04/12 12:27:44:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/12 12:27:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 12:27:44:  ---------
04/12 15:58:19:  ======================== New Round =============================
04/12 15:58:19:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 15:58:19:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 15:59:36:  ======================== New Round =============================
04/12 15:59:36:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 15:59:36:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:04:45:  ======================== New Round =============================
04/12 16:04:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:04:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:06:03:  ======================== New Round =============================
04/12 16:06:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:06:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:06:36:  #RES: f1:61.531, precision:59.302, recall:63.934, loss:0.181 at 2
04/12 16:06:36:  Best: f1:61.531, precision:59.302, recall:63.934, loss:0.181 at 2
04/12 16:06:36:  ---------
04/12 16:07:09:  #RES: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:09:  Best: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:09:  ---------
04/12 16:07:43:  #RES: f1:64.369, precision:64.809, recall:63.934, loss:0.293 at 6
04/12 16:07:43:  Best: f1:65.316, precision:65.347, recall:65.284, loss:0.232 at 4
04/12 16:07:43:  ---------
04/12 16:08:16:  #RES: f1:66.444, precision:65.693, recall:67.213, loss:0.327 at 9
04/12 16:08:16:  Best: f1:66.444, precision:65.693, recall:67.213, loss:0.327 at 9
04/12 16:08:16:  ---------
04/12 16:08:49:  #RES: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:08:49:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:08:49:  ---------
04/12 16:09:23:  #RES: f1:64.793, precision:66.297, recall:63.356, loss:0.409 at 13
04/12 16:09:23:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:09:23:  ---------
04/12 16:09:56:  #RES: f1:66.446, precision:65.242, recall:67.695, loss:0.425 at 15
04/12 16:09:56:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:09:56:  ---------
04/12 16:10:30:  #RES: f1:66.029, precision:65.527, recall:66.538, loss:0.432 at 18
04/12 16:10:30:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:10:30:  ---------
04/12 16:11:03:  #RES: f1:66.539, precision:66.252, recall:66.827, loss:0.464 at 20
04/12 16:11:03:  Best: f1:67.114, precision:66.730, recall:67.502, loss:0.376 at 11
04/12 16:11:03:  ---------
04/12 16:11:37:  #RES: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:11:37:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:11:37:  ---------
04/12 16:12:10:  #RES: f1:66.376, precision:66.699, recall:66.056, loss:0.492 at 25
04/12 16:12:10:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:12:10:  ---------
04/12 16:12:44:  #RES: f1:66.951, precision:65.858, recall:68.081, loss:0.506 at 27
04/12 16:12:44:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:12:44:  ---------
04/12 16:13:17:  #RES: f1:66.218, precision:65.996, recall:66.442, loss:0.514 at 29
04/12 16:13:17:  Best: f1:67.367, precision:66.761, recall:67.985, loss:0.462 at 22
04/12 16:13:17:  ---------
04/12 16:13:51:  #RES: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:13:51:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:13:51:  ---------
04/12 16:14:24:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 34
04/12 16:14:24:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:14:24:  ---------
04/12 16:14:58:  #RES: f1:64.786, precision:63.211, recall:66.442, loss:0.530 at 36
04/12 16:14:58:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:14:58:  ---------
04/12 16:15:31:  #RES: f1:66.603, precision:65.728, recall:67.502, loss:0.514 at 38
04/12 16:15:31:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:15:31:  ---------
04/12 16:16:05:  #RES: f1:67.148, precision:66.987, recall:67.310, loss:0.529 at 40
04/12 16:16:05:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:16:05:  ---------
04/12 16:16:38:  #RES: f1:66.033, precision:65.075, recall:67.020, loss:0.543 at 43
04/12 16:16:38:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:16:38:  ---------
04/12 16:17:12:  #RES: f1:66.542, precision:64.636, recall:68.563, loss:0.560 at 45
04/12 16:17:12:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:17:12:  ---------
04/12 16:17:45:  #RES: f1:66.572, precision:65.485, recall:67.695, loss:0.581 at 47
04/12 16:17:45:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:17:45:  ---------
04/12 16:18:19:  #RES: f1:66.730, precision:65.610, recall:67.888, loss:0.573 at 50
04/12 16:18:19:  Best: f1:67.458, precision:66.479, recall:68.467, loss:0.492 at 31
04/12 16:18:19:  ---------
04/12 16:18:52:  #RES: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:18:52:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:18:52:  ---------
04/12 16:19:25:  #RES: f1:66.407, precision:67.060, recall:65.767, loss:0.586 at 54
04/12 16:19:25:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:19:25:  ---------
04/12 16:19:59:  #RES: f1:65.875, precision:64.948, recall:66.827, loss:0.584 at 56
04/12 16:19:59:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:19:59:  ---------
04/12 16:20:33:  #RES: f1:66.212, precision:66.963, recall:65.477, loss:0.559 at 59
04/12 16:20:33:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:20:33:  ---------
04/12 16:21:06:  #RES: f1:65.938, precision:64.981, recall:66.924, loss:0.573 at 61
04/12 16:21:06:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:21:06:  ---------
04/12 16:21:40:  #RES: f1:65.880, precision:64.510, recall:67.310, loss:0.592 at 63
04/12 16:21:40:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:21:40:  ---------
04/12 16:22:13:  #RES: f1:66.252, precision:65.687, recall:66.827, loss:0.592 at 65
04/12 16:22:13:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:22:13:  ---------
04/12 16:22:47:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.592 at 68
04/12 16:22:47:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:22:47:  ---------
04/12 16:23:20:  #RES: f1:65.967, precision:65.312, recall:66.635, loss:0.598 at 70
04/12 16:23:20:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:23:20:  ---------
04/12 16:23:54:  #RES: f1:66.223, precision:65.262, recall:67.213, loss:0.610 at 72
04/12 16:23:54:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:23:54:  ---------
04/12 16:24:27:  #RES: f1:66.603, precision:66.004, recall:67.213, loss:0.591 at 75
04/12 16:24:27:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:24:27:  ---------
04/12 16:25:01:  #RES: f1:66.130, precision:64.991, recall:67.310, loss:0.611 at 77
04/12 16:25:01:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:25:01:  ---------
04/12 16:25:34:  #RES: f1:66.318, precision:65.446, recall:67.213, loss:0.597 at 79
04/12 16:25:34:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:25:34:  ---------
04/12 16:26:08:  #RES: f1:66.982, precision:65.829, recall:68.177, loss:0.600 at 81
04/12 16:26:08:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:26:08:  ---------
04/12 16:26:41:  #RES: f1:67.430, precision:66.698, recall:68.177, loss:0.598 at 84
04/12 16:26:41:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:26:41:  ---------
04/12 16:27:15:  #RES: f1:65.934, precision:65.341, recall:66.538, loss:0.602 at 86
04/12 16:27:15:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:27:15:  ---------
04/12 16:27:48:  #RES: f1:65.952, precision:63.784, recall:68.274, loss:0.649 at 88
04/12 16:27:48:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:27:48:  ---------
04/12 16:28:22:  #RES: f1:66.761, precision:65.138, recall:68.467, loss:0.624 at 90
04/12 16:28:22:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:28:22:  ---------
04/12 16:28:55:  #RES: f1:66.540, precision:65.335, recall:67.792, loss:0.619 at 93
04/12 16:28:55:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:28:55:  ---------
04/12 16:29:28:  #RES: f1:66.161, precision:65.051, recall:67.310, loss:0.612 at 95
04/12 16:29:28:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:29:28:  ---------
04/12 16:30:02:  #RES: f1:66.635, precision:65.881, recall:67.406, loss:0.608 at 97
04/12 16:30:02:  Best: f1:67.647, precision:66.573, recall:68.756, loss:0.578 at 52
04/12 16:30:02:  ---------
04/12 16:32:05:  ======================== New Round =============================
04/12 16:32:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 16:32:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 16:32:28:  #RES: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 16:32:28:  Best: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 16:32:28:  ---------
04/12 16:32:52:  #RES: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 16:32:52:  Best: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 16:32:52:  ---------
04/12 16:33:15:  #RES: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:15:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:15:  ---------
04/12 16:33:38:  #RES: f1:63.234, precision:63.112, recall:63.356, loss:0.331 at 9
04/12 16:33:38:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 16:33:38:  ---------
04/12 16:34:02:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:02:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:02:  ---------
04/12 16:34:25:  #RES: f1:65.163, precision:64.852, recall:65.477, loss:0.395 at 13
04/12 16:34:25:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 16:34:25:  ---------
04/12 16:34:49:  #RES: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:34:49:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:34:49:  ---------
04/12 16:35:12:  #RES: f1:64.790, precision:64.117, recall:65.477, loss:0.419 at 18
04/12 16:35:12:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:12:  ---------
04/12 16:35:36:  #RES: f1:65.079, precision:64.313, recall:65.863, loss:0.438 at 20
04/12 16:35:36:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:36:  ---------
04/12 16:35:59:  #RES: f1:66.031, precision:65.345, recall:66.731, loss:0.446 at 22
04/12 16:35:59:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 16:35:59:  ---------
04/12 16:36:22:  #RES: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:22:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:22:  ---------
04/12 16:36:46:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.449 at 27
04/12 16:36:46:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:36:46:  ---------
04/12 16:37:09:  #RES: f1:66.635, precision:66.443, recall:66.827, loss:0.468 at 29
04/12 16:37:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:09:  ---------
04/12 16:37:33:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.462 at 31
04/12 16:37:33:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:33:  ---------
04/12 16:37:56:  #RES: f1:66.602, precision:66.959, recall:66.249, loss:0.506 at 34
04/12 16:37:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:37:56:  ---------
04/12 16:38:20:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 36
04/12 16:38:20:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:38:20:  ---------
04/12 16:38:43:  #RES: f1:66.988, precision:66.956, recall:67.020, loss:0.482 at 38
04/12 16:38:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:38:43:  ---------
04/12 16:39:07:  #RES: f1:67.281, precision:67.641, recall:66.924, loss:0.519 at 40
04/12 16:39:07:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:07:  ---------
04/12 16:39:30:  #RES: f1:66.256, precision:65.144, recall:67.406, loss:0.534 at 43
04/12 16:39:30:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:30:  ---------
04/12 16:39:54:  #RES: f1:66.731, precision:67.221, recall:66.249, loss:0.490 at 45
04/12 16:39:54:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:39:54:  ---------
04/12 16:40:17:  #RES: f1:67.021, precision:67.216, recall:66.827, loss:0.514 at 47
04/12 16:40:17:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:40:17:  ---------
04/12 16:40:41:  #RES: f1:66.860, precision:66.892, recall:66.827, loss:0.528 at 49
04/12 16:40:41:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:40:41:  ---------
04/12 16:41:04:  #RES: f1:66.794, precision:66.381, recall:67.213, loss:0.545 at 52
04/12 16:41:04:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:41:04:  ---------
04/12 16:41:28:  #RES: f1:65.755, precision:64.358, recall:67.213, loss:0.570 at 54
04/12 16:41:28:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:41:28:  ---------
04/12 16:43:56:  #RES: f1:65.928, precision:65.896, recall:65.959, loss:0.532 at 56
04/12 16:43:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:43:56:  ---------
04/12 16:44:19:  #RES: f1:65.961, precision:65.865, recall:66.056, loss:0.552 at 59
04/12 16:44:19:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:44:19:  ---------
04/12 16:44:43:  #RES: f1:67.429, precision:66.604, recall:68.274, loss:0.543 at 61
04/12 16:44:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:44:43:  ---------
04/12 16:45:06:  #RES: f1:67.153, precision:67.679, recall:66.635, loss:0.553 at 63
04/12 16:45:06:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:06:  ---------
04/12 16:45:29:  #RES: f1:66.921, precision:66.164, recall:67.695, loss:0.528 at 65
04/12 16:45:29:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:29:  ---------
04/12 16:45:53:  #RES: f1:67.016, precision:66.165, recall:67.888, loss:0.562 at 68
04/12 16:45:53:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:45:53:  ---------
04/12 16:46:16:  #RES: f1:65.325, precision:63.455, recall:67.310, loss:0.580 at 70
04/12 16:46:16:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:46:16:  ---------
04/12 16:46:40:  #RES: f1:66.889, precision:66.195, recall:67.599, loss:0.558 at 72
04/12 16:46:40:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:46:40:  ---------
04/12 16:47:03:  #RES: f1:66.381, precision:65.478, recall:67.310, loss:0.557 at 74
04/12 16:47:03:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:03:  ---------
04/12 16:47:27:  #RES: f1:65.783, precision:64.591, recall:67.020, loss:0.562 at 77
04/12 16:47:27:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:27:  ---------
04/12 16:47:50:  #RES: f1:67.234, precision:65.955, recall:68.563, loss:0.570 at 79
04/12 16:47:50:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:47:50:  ---------
04/12 16:48:14:  #RES: f1:67.323, precision:65.511, recall:69.238, loss:0.571 at 81
04/12 16:48:14:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:48:14:  ---------
04/12 16:48:37:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.566 at 84
04/12 16:48:37:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:48:37:  ---------
04/12 16:49:01:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:0.569 at 86
04/12 16:49:01:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 16:49:01:  ---------
04/12 16:49:24:  #RES: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 16:49:24:  Best: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 16:49:24:  ---------
04/12 16:49:48:  #RES: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 16:49:48:  Best: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 16:49:48:  ---------
04/12 16:50:12:  #RES: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:12:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:12:  ---------
04/12 16:50:35:  #RES: f1:67.173, precision:66.106, recall:68.274, loss:0.566 at 95
04/12 16:50:35:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:35:  ---------
04/12 16:50:59:  #RES: f1:66.952, precision:65.949, recall:67.985, loss:0.571 at 97
04/12 16:50:59:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:50:59:  ---------
04/12 16:51:22:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.571 at 99
04/12 16:51:22:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 16:51:22:  ---------
04/12 17:11:29:  ======================== New Round =============================
04/12 17:11:29:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 17:11:29:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 17:11:53:  #RES: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 17:11:53:  Best: f1:59.139, precision:58.689, recall:59.595, loss:0.184 at 2
04/12 17:11:53:  ---------
04/12 17:12:16:  #RES: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 17:12:16:  Best: f1:63.976, precision:65.327, recall:62.681, loss:0.218 at 4
04/12 17:12:16:  ---------
04/12 17:12:39:  #RES: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:12:39:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:12:39:  ---------
04/12 17:13:02:  #RES: f1:63.234, precision:63.112, recall:63.356, loss:0.331 at 9
04/12 17:13:02:  Best: f1:65.026, precision:63.848, recall:66.249, loss:0.267 at 6
04/12 17:13:02:  ---------
04/12 17:13:25:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:25:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:25:  ---------
04/12 17:13:49:  #RES: f1:65.163, precision:64.852, recall:65.477, loss:0.395 at 13
04/12 17:13:49:  Best: f1:66.316, precision:65.812, recall:66.827, loss:0.355 at 11
04/12 17:13:49:  ---------
04/12 17:14:12:  #RES: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:12:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:12:  ---------
04/12 17:14:36:  #RES: f1:64.790, precision:64.117, recall:65.477, loss:0.419 at 18
04/12 17:14:36:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:36:  ---------
04/12 17:14:59:  #RES: f1:65.079, precision:64.313, recall:65.863, loss:0.438 at 20
04/12 17:14:59:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:14:59:  ---------
04/12 17:15:23:  #RES: f1:66.031, precision:65.345, recall:66.731, loss:0.446 at 22
04/12 17:15:23:  Best: f1:66.506, precision:66.284, recall:66.731, loss:0.400 at 15
04/12 17:15:23:  ---------
04/12 17:15:46:  #RES: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:15:46:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:15:46:  ---------
04/12 17:16:09:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.449 at 27
04/12 17:16:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:09:  ---------
04/12 17:16:33:  #RES: f1:66.635, precision:66.443, recall:66.827, loss:0.468 at 29
04/12 17:16:33:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:33:  ---------
04/12 17:16:56:  #RES: f1:66.730, precision:65.701, recall:67.792, loss:0.462 at 31
04/12 17:16:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:16:56:  ---------
04/12 17:17:20:  #RES: f1:66.602, precision:66.959, recall:66.249, loss:0.506 at 34
04/12 17:17:20:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:17:20:  ---------
04/12 17:17:43:  #RES: f1:66.316, precision:65.812, recall:66.827, loss:0.492 at 36
04/12 17:17:43:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:17:43:  ---------
04/12 17:18:06:  #RES: f1:66.988, precision:66.956, recall:67.020, loss:0.482 at 38
04/12 17:18:06:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:06:  ---------
04/12 17:18:30:  #RES: f1:67.281, precision:67.641, recall:66.924, loss:0.519 at 40
04/12 17:18:30:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:30:  ---------
04/12 17:18:54:  #RES: f1:66.256, precision:65.144, recall:67.406, loss:0.534 at 43
04/12 17:18:54:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:18:54:  ---------
04/12 17:19:17:  #RES: f1:66.731, precision:67.221, recall:66.249, loss:0.490 at 45
04/12 17:19:17:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:19:17:  ---------
04/12 17:19:40:  #RES: f1:67.021, precision:67.216, recall:66.827, loss:0.514 at 47
04/12 17:19:40:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:19:40:  ---------
04/12 17:20:04:  #RES: f1:66.860, precision:66.892, recall:66.827, loss:0.528 at 49
04/12 17:20:04:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:04:  ---------
04/12 17:20:28:  #RES: f1:66.794, precision:66.381, recall:67.213, loss:0.545 at 52
04/12 17:20:28:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:28:  ---------
04/12 17:20:51:  #RES: f1:65.755, precision:64.358, recall:67.213, loss:0.570 at 54
04/12 17:20:51:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:20:51:  ---------
04/12 17:21:14:  #RES: f1:65.928, precision:65.896, recall:65.959, loss:0.532 at 56
04/12 17:21:14:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:21:14:  ---------
04/12 17:21:38:  #RES: f1:65.961, precision:65.865, recall:66.056, loss:0.552 at 59
04/12 17:21:38:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:21:38:  ---------
04/12 17:22:01:  #RES: f1:67.429, precision:66.604, recall:68.274, loss:0.543 at 61
04/12 17:22:01:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:01:  ---------
04/12 17:22:25:  #RES: f1:67.153, precision:67.679, recall:66.635, loss:0.553 at 63
04/12 17:22:25:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:25:  ---------
04/12 17:22:48:  #RES: f1:66.921, precision:66.164, recall:67.695, loss:0.528 at 65
04/12 17:22:48:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:22:48:  ---------
04/12 17:23:11:  #RES: f1:67.016, precision:66.165, recall:67.888, loss:0.562 at 68
04/12 17:23:11:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:11:  ---------
04/12 17:23:35:  #RES: f1:65.325, precision:63.455, recall:67.310, loss:0.580 at 70
04/12 17:23:35:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:35:  ---------
04/12 17:23:58:  #RES: f1:66.889, precision:66.195, recall:67.599, loss:0.558 at 72
04/12 17:23:58:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:23:58:  ---------
04/12 17:24:22:  #RES: f1:66.381, precision:65.478, recall:67.310, loss:0.557 at 74
04/12 17:24:22:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:24:22:  ---------
04/12 17:24:45:  #RES: f1:65.783, precision:64.591, recall:67.020, loss:0.562 at 77
04/12 17:24:45:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:24:45:  ---------
04/12 17:25:09:  #RES: f1:67.234, precision:65.955, recall:68.563, loss:0.570 at 79
04/12 17:25:09:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:09:  ---------
04/12 17:25:32:  #RES: f1:67.323, precision:65.511, recall:69.238, loss:0.571 at 81
04/12 17:25:32:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:32:  ---------
04/12 17:25:56:  #RES: f1:66.980, precision:65.381, recall:68.660, loss:0.566 at 84
04/12 17:25:56:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:25:56:  ---------
04/12 17:26:19:  #RES: f1:66.824, precision:65.524, recall:68.177, loss:0.569 at 86
04/12 17:26:19:  Best: f1:67.658, precision:67.335, recall:67.985, loss:0.455 at 24
04/12 17:26:19:  ---------
04/12 17:26:42:  #RES: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 17:26:42:  Best: f1:68.021, precision:66.667, recall:69.431, loss:0.564 at 88
04/12 17:26:42:  ---------
04/12 17:27:06:  #RES: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 17:27:06:  Best: f1:68.128, precision:67.326, recall:68.949, loss:0.559 at 90
04/12 17:27:06:  ---------
04/12 17:27:29:  #RES: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:29:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:29:  ---------
04/12 17:27:52:  #RES: f1:67.173, precision:66.106, recall:68.274, loss:0.566 at 95
04/12 17:27:52:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:27:52:  ---------
04/12 17:28:16:  #RES: f1:66.952, precision:65.949, recall:67.985, loss:0.571 at 97
04/12 17:28:16:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:28:16:  ---------
04/12 17:28:39:  #RES: f1:66.825, precision:65.704, recall:67.985, loss:0.571 at 99
04/12 17:28:39:  Best: f1:68.711, precision:68.286, recall:69.142, loss:0.557 at 93
04/12 17:28:39:  ---------
04/12 18:18:15:  ======================== New Round =============================
04/12 18:18:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 18:18:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 18:19:57:  ======================== New Round =============================
04/12 18:19:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/12 18:19:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/12 18:21:12:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 18:21:12:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/12 18:21:12:  ---------
04/12 18:22:26:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:22:26:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:22:26:  ---------
04/12 18:23:41:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/12 18:23:41:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:23:41:  ---------
04/12 18:24:56:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/12 18:24:56:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/12 18:24:56:  ---------
04/12 18:26:11:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:26:11:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:26:11:  ---------
04/12 18:27:27:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/12 18:27:27:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:27:27:  ---------
04/12 18:28:42:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/12 18:28:42:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:28:42:  ---------
04/12 18:29:57:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/12 18:29:57:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:29:57:  ---------
04/12 18:31:12:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/12 18:31:12:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:31:12:  ---------
04/12 18:32:27:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/12 18:32:27:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/12 18:32:27:  ---------
04/12 18:33:42:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 18:33:42:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/12 18:33:42:  ---------
04/12 18:34:58:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:34:58:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:34:58:  ---------
04/12 18:36:13:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/12 18:36:13:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:36:13:  ---------
04/12 18:37:28:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/12 18:37:28:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:37:28:  ---------
04/12 18:38:43:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/12 18:38:43:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:38:43:  ---------
04/12 18:39:59:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/12 18:39:59:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:39:59:  ---------
04/12 18:41:14:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/12 18:41:14:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:41:14:  ---------
04/12 18:42:29:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/12 18:42:29:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/12 18:42:29:  ---------
04/12 18:43:44:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 18:43:44:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/12 18:43:44:  ---------
04/12 18:44:59:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:44:59:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:44:59:  ---------
04/12 18:46:14:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/12 18:46:14:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/12 18:46:14:  ---------
04/12 18:47:29:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:47:29:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:47:29:  ---------
04/12 18:48:45:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/12 18:48:45:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:48:45:  ---------
04/12 18:50:00:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/12 18:50:00:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:50:00:  ---------
04/12 18:51:15:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/12 18:51:15:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:51:15:  ---------
04/12 18:52:30:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/12 18:52:30:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:52:30:  ---------
04/12 18:53:46:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/12 18:53:46:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/12 18:53:46:  ---------
04/12 18:55:01:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:55:01:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:55:01:  ---------
04/12 18:56:16:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/12 18:56:16:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:56:16:  ---------
04/12 18:57:31:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/12 18:57:31:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:57:31:  ---------
04/12 18:58:46:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/12 18:58:46:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 18:58:46:  ---------
04/12 19:00:01:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/12 19:00:01:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/12 19:00:01:  ---------
04/12 19:01:17:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:01:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:01:17:  ---------
04/12 19:02:32:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/12 19:02:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:02:32:  ---------
04/12 19:03:47:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/12 19:03:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:03:47:  ---------
04/12 19:05:02:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/12 19:05:02:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:05:02:  ---------
04/12 19:06:17:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/12 19:06:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:06:17:  ---------
04/12 19:07:32:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/12 19:07:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:07:32:  ---------
04/12 19:08:47:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/12 19:08:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:08:47:  ---------
04/12 19:10:02:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/12 19:10:02:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:10:02:  ---------
04/12 19:11:17:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/12 19:11:17:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:11:17:  ---------
04/12 19:12:32:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/12 19:12:32:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:12:32:  ---------
04/12 19:13:47:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/12 19:13:47:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/12 19:13:47:  ---------
04/14 15:06:24:  ======================== New Round =============================
04/14 15:06:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:06:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:07:44:  ======================== New Round =============================
04/14 15:07:44:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:07:44:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:08:17:  ======================== New Round =============================
04/14 15:08:17:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:08:17:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:19:18:  ======================== New Round =============================
04/14 15:19:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:19:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:22:43:  ======================== New Round =============================
04/14 15:22:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:22:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:27:49:  ======================== New Round =============================
04/14 15:27:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:27:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:28:46:  ======================== New Round =============================
04/14 15:28:46:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:28:46:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:30:16:  ======================== New Round =============================
04/14 15:30:16:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:30:16:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:30:57:  ======================== New Round =============================
04/14 15:30:57:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:30:57:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:31:51:  ======================== New Round =============================
04/14 15:31:51:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:31:51:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:37:21:  ======================== New Round =============================
04/14 15:37:21:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:37:21:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='laion', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:37:56:  ======================== New Round =============================
04/14 15:37:56:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:37:56:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:38:19:  ======================== New Round =============================
04/14 15:38:19:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/14 15:38:19:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/14 15:39:33:  #RES: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/14 15:39:33:  Best: f1:62.817, precision:61.208, recall:64.513, loss:0.189 at 2
04/14 15:39:33:  ---------
04/14 15:40:47:  #RES: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:40:47:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:40:47:  ---------
04/14 15:42:02:  #RES: f1:64.599, precision:63.928, recall:65.284, loss:0.315 at 6
04/14 15:42:02:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:42:02:  ---------
04/14 15:43:18:  #RES: f1:61.626, precision:60.426, recall:62.874, loss:0.386 at 9
04/14 15:43:18:  Best: f1:65.642, precision:65.421, recall:65.863, loss:0.258 at 4
04/14 15:43:18:  ---------
04/14 15:44:33:  #RES: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:44:33:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:44:33:  ---------
04/14 15:45:48:  #RES: f1:65.372, precision:65.851, recall:64.899, loss:0.423 at 13
04/14 15:45:48:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:45:48:  ---------
04/14 15:47:04:  #RES: f1:62.530, precision:62.865, recall:62.199, loss:0.486 at 15
04/14 15:47:04:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:47:04:  ---------
04/14 15:48:19:  #RES: f1:65.650, precision:64.878, recall:66.442, loss:0.469 at 18
04/14 15:48:19:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:48:19:  ---------
04/14 15:49:34:  #RES: f1:64.550, precision:63.467, recall:65.670, loss:0.511 at 20
04/14 15:49:34:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:49:34:  ---------
04/14 15:50:50:  #RES: f1:65.597, precision:64.233, recall:67.020, loss:0.489 at 22
04/14 15:50:50:  Best: f1:65.855, precision:66.535, recall:65.188, loss:0.399 at 11
04/14 15:50:50:  ---------
04/14 15:52:05:  #RES: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/14 15:52:05:  Best: f1:65.993, precision:65.835, recall:66.152, loss:0.499 at 25
04/14 15:52:05:  ---------
04/14 15:53:20:  #RES: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:53:20:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:53:20:  ---------
04/14 15:54:35:  #RES: f1:65.479, precision:65.385, recall:65.574, loss:0.466 at 29
04/14 15:54:35:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:54:35:  ---------
04/14 15:55:51:  #RES: f1:64.728, precision:63.014, recall:66.538, loss:0.507 at 31
04/14 15:55:51:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:55:51:  ---------
04/14 15:57:06:  #RES: f1:66.573, precision:64.695, recall:68.563, loss:0.529 at 34
04/14 15:57:06:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:57:06:  ---------
04/14 15:58:21:  #RES: f1:65.201, precision:64.645, recall:65.767, loss:0.549 at 36
04/14 15:58:21:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:58:21:  ---------
04/14 15:59:37:  #RES: f1:65.612, precision:65.267, recall:65.959, loss:0.525 at 38
04/14 15:59:37:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 15:59:37:  ---------
04/14 16:00:52:  #RES: f1:65.051, precision:65.400, recall:64.706, loss:0.527 at 40
04/14 16:00:52:  Best: f1:66.635, precision:65.250, recall:68.081, loss:0.531 at 27
04/14 16:00:52:  ---------
04/14 16:02:07:  #RES: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/14 16:02:07:  Best: f1:66.762, precision:65.945, recall:67.599, loss:0.539 at 43
04/14 16:02:07:  ---------
04/14 16:03:22:  #RES: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:03:22:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:03:22:  ---------
04/14 16:04:38:  #RES: f1:65.724, precision:66.469, recall:64.995, loss:0.559 at 47
04/14 16:04:38:  Best: f1:66.956, precision:66.988, recall:66.924, loss:0.570 at 45
04/14 16:04:38:  ---------
04/14 16:05:53:  #RES: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:05:53:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:05:53:  ---------
04/14 16:07:09:  #RES: f1:66.603, precision:66.475, recall:66.731, loss:0.587 at 52
04/14 16:07:09:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:07:09:  ---------
04/14 16:08:24:  #RES: f1:67.668, precision:68.031, recall:67.310, loss:0.525 at 54
04/14 16:08:24:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:08:24:  ---------
04/14 16:09:39:  #RES: f1:65.963, precision:65.679, recall:66.249, loss:0.580 at 56
04/14 16:09:39:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:09:39:  ---------
04/14 16:10:54:  #RES: f1:66.445, precision:65.331, recall:67.599, loss:0.604 at 59
04/14 16:10:54:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:10:54:  ---------
04/14 16:12:10:  #RES: f1:66.921, precision:66.071, recall:67.792, loss:0.581 at 61
04/14 16:12:10:  Best: f1:67.890, precision:67.988, recall:67.792, loss:0.547 at 50
04/14 16:12:10:  ---------
04/14 16:13:25:  #RES: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:13:25:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:13:25:  ---------
04/14 16:14:40:  #RES: f1:67.277, precision:67.245, recall:67.310, loss:0.550 at 65
04/14 16:14:40:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:14:40:  ---------
04/14 16:15:56:  #RES: f1:67.706, precision:68.406, recall:67.020, loss:0.506 at 68
04/14 16:15:56:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:15:56:  ---------
04/14 16:17:11:  #RES: f1:67.872, precision:67.011, recall:68.756, loss:0.551 at 70
04/14 16:17:11:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:17:11:  ---------
04/14 16:18:26:  #RES: f1:67.614, precision:66.419, recall:68.852, loss:0.573 at 72
04/14 16:18:26:  Best: f1:68.040, precision:67.619, recall:68.467, loss:0.575 at 63
04/14 16:18:26:  ---------
04/14 16:19:42:  #RES: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:19:42:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:19:42:  ---------
04/14 16:20:57:  #RES: f1:67.788, precision:67.593, recall:67.985, loss:0.565 at 77
04/14 16:20:57:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:20:57:  ---------
04/14 16:22:13:  #RES: f1:67.950, precision:67.819, recall:68.081, loss:0.576 at 79
04/14 16:22:13:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:22:13:  ---------
04/14 16:23:28:  #RES: f1:67.631, precision:67.664, recall:67.599, loss:0.594 at 81
04/14 16:23:28:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:23:28:  ---------
04/14 16:24:43:  #RES: f1:67.557, precision:66.856, recall:68.274, loss:0.609 at 84
04/14 16:24:43:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:24:43:  ---------
04/14 16:25:58:  #RES: f1:67.410, precision:67.805, recall:67.020, loss:0.589 at 86
04/14 16:25:58:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:25:58:  ---------
04/14 16:27:14:  #RES: f1:68.321, precision:67.611, recall:69.045, loss:0.592 at 88
04/14 16:27:14:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:27:14:  ---------
04/14 16:28:29:  #RES: f1:67.767, precision:68.231, recall:67.310, loss:0.598 at 90
04/14 16:28:29:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:28:29:  ---------
04/14 16:29:44:  #RES: f1:67.756, precision:67.529, recall:67.985, loss:0.603 at 93
04/14 16:29:44:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:29:44:  ---------
04/14 16:30:59:  #RES: f1:67.535, precision:67.568, recall:67.502, loss:0.601 at 95
04/14 16:30:59:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:30:59:  ---------
04/14 16:32:15:  #RES: f1:68.197, precision:67.647, recall:68.756, loss:0.606 at 97
04/14 16:32:15:  Best: f1:68.536, precision:67.573, recall:69.527, loss:0.545 at 75
04/14 16:32:15:  ---------
04/15 21:52:36:  ======================== New Round =============================
04/15 21:52:36:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 21:52:36:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 21:53:30:  ======================== New Round =============================
04/15 21:53:30:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 21:53:30:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 21:53:49:  ======================== New Round =============================
04/15 21:53:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 21:53:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 21:54:01:  ======================== New Round =============================
04/15 21:54:01:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 21:54:01:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 21:59:53:  ======================== New Round =============================
04/15 21:59:53:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 21:59:53:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:00:56:  ======================== New Round =============================
04/15 22:00:56:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:00:56:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:01:42:  ======================== New Round =============================
04/15 22:01:42:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:01:42:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:11:05:  ======================== New Round =============================
04/15 22:11:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:11:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:13:00:  ======================== New Round =============================
04/15 22:13:00:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:13:00:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:13:29:  ======================== New Round =============================
04/15 22:13:29:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:13:29:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:13:38:  ======================== New Round =============================
04/15 22:13:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:13:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:14:23:  ======================== New Round =============================
04/15 22:14:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:14:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:21:23:  ======================== New Round =============================
04/15 22:21:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:21:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:25:25:  ======================== New Round =============================
04/15 22:25:25:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:25:25:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:26:46:  ======================== New Round =============================
04/15 22:26:46:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:26:46:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:34:06:  ======================== New Round =============================
04/15 22:34:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:34:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:34:55:  ======================== New Round =============================
04/15 22:34:55:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:34:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:36:51:  ======================== New Round =============================
04/15 22:36:51:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:36:51:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:39:17:  ======================== New Round =============================
04/15 22:39:17:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:39:17:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:42:25:  ======================== New Round =============================
04/15 22:42:25:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:42:25:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:46:19:  ======================== New Round =============================
04/15 22:46:19:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:46:19:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:47:23:  ======================== New Round =============================
04/15 22:47:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:47:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:50:00:  ======================== New Round =============================
04/15 22:50:00:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:50:00:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:51:24:  ======================== New Round =============================
04/15 22:51:24:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:51:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:52:41:  ======================== New Round =============================
04/15 22:52:41:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:52:41:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:53:03:  ======================== New Round =============================
04/15 22:53:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:53:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:54:00:  ======================== New Round =============================
04/15 22:54:00:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:54:00:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 22:54:25:  ======================== New Round =============================
04/15 22:54:25:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 22:54:25:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:12:46:  ======================== New Round =============================
04/15 23:12:46:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:12:46:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:14:23:  ======================== New Round =============================
04/15 23:14:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:14:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:15:18:  ======================== New Round =============================
04/15 23:15:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:15:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:16:38:  ======================== New Round =============================
04/15 23:16:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:16:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:19:04:  ======================== New Round =============================
04/15 23:19:04:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:19:04:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:19:59:  ======================== New Round =============================
04/15 23:19:59:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:19:59:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:20:55:  ======================== New Round =============================
04/15 23:20:55:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:20:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=16, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=1, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:24:47:  ======================== New Round =============================
04/15 23:24:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:24:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:27:03:  ======================== New Round =============================
04/15 23:27:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/15 23:27:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/15 23:28:59:  #RES: f1:0.612, precision:0.707, recall:0.577, loss:0.806 at 2
04/15 23:28:59:  Best: f1:0.612, precision:0.707, recall:0.577, loss:0.806 at 2
04/15 23:28:59:  ---------
04/15 23:30:55:  #RES: f1:0.665, precision:0.712, recall:0.642, loss:0.955 at 6
04/15 23:30:55:  Best: f1:0.665, precision:0.712, recall:0.642, loss:0.955 at 6
04/15 23:30:55:  ---------
04/15 23:32:52:  #RES: f1:0.620, precision:0.731, recall:0.590, loss:1.325 at 9
04/15 23:32:52:  Best: f1:0.665, precision:0.712, recall:0.642, loss:0.955 at 6
04/15 23:32:52:  ---------
04/15 23:34:48:  #RES: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:34:48:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:34:48:  ---------
04/15 23:36:45:  #RES: f1:0.642, precision:0.720, recall:0.608, loss:1.316 at 15
04/15 23:36:45:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:36:45:  ---------
04/15 23:38:41:  #RES: f1:0.654, precision:0.696, recall:0.629, loss:1.366 at 18
04/15 23:38:41:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:38:41:  ---------
04/15 23:40:37:  #RES: f1:0.667, precision:0.676, recall:0.661, loss:1.201 at 21
04/15 23:40:37:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:40:37:  ---------
04/15 23:42:34:  #RES: f1:0.673, precision:0.659, recall:0.693, loss:1.194 at 24
04/15 23:42:34:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:42:34:  ---------
04/15 23:44:30:  #RES: f1:0.666, precision:0.696, recall:0.646, loss:1.282 at 27
04/15 23:44:30:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:44:30:  ---------
04/15 23:46:27:  #RES: f1:0.660, precision:0.702, recall:0.635, loss:1.474 at 30
04/15 23:46:27:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:46:27:  ---------
04/15 23:48:23:  #RES: f1:0.659, precision:0.665, recall:0.658, loss:1.429 at 33
04/15 23:48:23:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:48:23:  ---------
04/15 23:50:20:  #RES: f1:0.658, precision:0.661, recall:0.671, loss:1.529 at 36
04/15 23:50:20:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:50:20:  ---------
04/15 23:52:16:  #RES: f1:0.673, precision:0.702, recall:0.657, loss:1.447 at 39
04/15 23:52:16:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:52:16:  ---------
04/15 23:54:12:  #RES: f1:0.670, precision:0.681, recall:0.661, loss:1.432 at 42
04/15 23:54:12:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:54:12:  ---------
04/15 23:56:08:  #RES: f1:0.646, precision:0.650, recall:0.650, loss:1.798 at 45
04/15 23:56:08:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:56:08:  ---------
04/15 23:58:05:  #RES: f1:0.650, precision:0.692, recall:0.624, loss:1.668 at 48
04/15 23:58:05:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/15 23:58:05:  ---------
04/16 00:00:01:  #RES: f1:0.669, precision:0.702, recall:0.651, loss:1.556 at 51
04/16 00:00:01:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:00:01:  ---------
04/16 00:01:57:  #RES: f1:0.658, precision:0.708, recall:0.629, loss:1.623 at 54
04/16 00:01:57:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:01:57:  ---------
04/16 00:03:54:  #RES: f1:0.659, precision:0.699, recall:0.633, loss:1.574 at 57
04/16 00:03:54:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:03:54:  ---------
04/16 00:05:50:  #RES: f1:0.673, precision:0.689, recall:0.659, loss:1.686 at 60
04/16 00:05:50:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:05:50:  ---------
04/16 00:07:47:  #RES: f1:0.669, precision:0.700, recall:0.647, loss:1.546 at 63
04/16 00:07:47:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:07:47:  ---------
04/16 00:09:43:  #RES: f1:0.669, precision:0.681, recall:0.660, loss:1.841 at 66
04/16 00:09:43:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:09:43:  ---------
04/16 00:11:40:  #RES: f1:0.652, precision:0.668, recall:0.639, loss:1.693 at 69
04/16 00:11:40:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:11:40:  ---------
04/16 00:13:36:  #RES: f1:0.663, precision:0.698, recall:0.639, loss:1.596 at 72
04/16 00:13:36:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:13:36:  ---------
04/16 00:15:32:  #RES: f1:0.667, precision:0.678, recall:0.657, loss:1.656 at 75
04/16 00:15:32:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:15:32:  ---------
04/16 00:17:28:  #RES: f1:0.657, precision:0.665, recall:0.652, loss:1.742 at 78
04/16 00:17:28:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:17:28:  ---------
04/16 00:19:25:  #RES: f1:0.655, precision:0.672, recall:0.645, loss:1.723 at 81
04/16 00:19:25:  Best: f1:0.683, precision:0.669, recall:0.702, loss:1.132 at 12
04/16 00:19:25:  ---------
04/16 10:50:47:  ======================== New Round =============================
04/16 10:50:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 10:50:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 10:52:41:  #RES: f1:61.188, precision:70.730, recall:57.721, loss:0.806 at 2
04/16 10:52:41:  Best: f1:61.188, precision:70.730, recall:57.721, loss:0.806 at 2
04/16 10:52:41:  ---------
04/16 10:54:37:  #RES: f1:66.451, precision:71.160, recall:64.214, loss:0.955 at 6
04/16 10:54:37:  Best: f1:66.451, precision:71.160, recall:64.214, loss:0.955 at 6
04/16 10:54:37:  ---------
04/16 10:56:33:  #RES: f1:61.977, precision:73.103, recall:59.028, loss:1.325 at 9
04/16 10:56:33:  Best: f1:66.451, precision:71.160, recall:64.214, loss:0.955 at 6
04/16 10:56:33:  ---------
04/16 10:58:30:  #RES: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 10:58:30:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 10:58:30:  ---------
04/16 10:59:12:  ======================== New Round =============================
04/16 10:59:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 10:59:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 10:59:55:  ======================== New Round =============================
04/16 10:59:55:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 10:59:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 11:00:27:  #RES: f1:64.157, precision:71.980, recall:60.757, loss:1.316 at 15
04/16 11:00:27:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:00:27:  ---------
04/16 11:02:24:  #RES: f1:65.421, precision:69.597, recall:62.856, loss:1.366 at 18
04/16 11:02:24:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:02:24:  ---------
04/16 11:04:20:  #RES: f1:66.733, precision:67.578, recall:66.123, loss:1.201 at 21
04/16 11:04:20:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:04:20:  ---------
04/16 11:05:43:  ======================== New Round =============================
04/16 11:05:43:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 11:05:43:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 11:06:55:  #RES: f1:67.285, precision:65.879, recall:69.328, loss:1.194 at 24
04/16 11:06:55:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:06:55:  ---------
04/16 11:10:56:  #RES: f1:61.947, precision:64.019, recall:63.929, loss:0.823 at 2
04/16 11:10:56:  Best: f1:61.947, precision:64.019, recall:63.929, loss:0.823 at 2
04/16 11:10:56:  ---------
04/16 11:11:08:  #RES: f1:66.576, precision:69.557, recall:64.569, loss:1.282 at 27
04/16 11:11:08:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:11:08:  ---------
04/16 11:15:21:  #RES: f1:66.005, precision:70.227, recall:63.536, loss:1.474 at 30
04/16 11:15:21:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:15:21:  ---------
04/16 11:16:09:  #RES: f1:64.730, precision:62.435, recall:70.459, loss:1.023 at 6
04/16 11:16:09:  Best: f1:64.730, precision:62.435, recall:70.459, loss:1.023 at 6
04/16 11:16:09:  ---------
04/16 11:19:33:  #RES: f1:65.898, precision:66.539, recall:65.755, loss:1.429 at 33
04/16 11:19:33:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:19:33:  ---------
04/16 11:21:22:  #RES: f1:65.160, precision:66.363, recall:64.550, loss:0.958 at 9
04/16 11:21:22:  Best: f1:65.160, precision:66.363, recall:64.550, loss:0.958 at 9
04/16 11:21:22:  ---------
04/16 11:23:46:  #RES: f1:65.774, precision:66.095, recall:67.054, loss:1.529 at 36
04/16 11:23:46:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:23:46:  ---------
04/16 11:26:34:  #RES: f1:58.358, precision:66.933, recall:55.970, loss:1.344 at 12
04/16 11:26:34:  Best: f1:65.160, precision:66.363, recall:64.550, loss:0.958 at 9
04/16 11:26:34:  ---------
04/16 11:27:58:  #RES: f1:67.311, precision:70.157, recall:65.731, loss:1.447 at 39
04/16 11:27:58:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:27:58:  ---------
04/16 11:31:47:  #RES: f1:63.418, precision:69.079, recall:60.981, loss:1.183 at 15
04/16 11:31:47:  Best: f1:65.160, precision:66.363, recall:64.550, loss:0.958 at 9
04/16 11:31:47:  ---------
04/16 11:32:10:  #RES: f1:67.029, precision:68.142, recall:66.083, loss:1.432 at 42
04/16 11:32:10:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:32:10:  ---------
04/16 11:36:23:  #RES: f1:64.575, precision:64.987, recall:65.039, loss:1.798 at 45
04/16 11:36:23:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:36:23:  ---------
04/16 11:37:00:  #RES: f1:63.515, precision:68.224, recall:60.955, loss:1.231 at 18
04/16 11:37:00:  Best: f1:65.160, precision:66.363, recall:64.550, loss:0.958 at 9
04/16 11:37:00:  ---------
04/16 11:40:35:  #RES: f1:65.005, precision:69.156, recall:62.410, loss:1.668 at 48
04/16 11:40:35:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:40:35:  ---------
04/16 11:42:13:  #RES: f1:65.998, precision:65.243, recall:67.016, loss:1.488 at 21
04/16 11:42:13:  Best: f1:65.998, precision:65.243, recall:67.016, loss:1.488 at 21
04/16 11:42:13:  ---------
04/16 11:44:47:  #RES: f1:66.901, precision:70.165, recall:65.074, loss:1.556 at 51
04/16 11:44:47:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:44:47:  ---------
04/16 11:47:26:  #RES: f1:63.831, precision:62.457, recall:69.404, loss:1.377 at 24
04/16 11:47:26:  Best: f1:65.998, precision:65.243, recall:67.016, loss:1.488 at 21
04/16 11:47:26:  ---------
04/16 11:48:59:  #RES: f1:65.752, precision:70.759, recall:62.897, loss:1.623 at 54
04/16 11:48:59:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:48:59:  ---------
04/16 11:52:39:  #RES: f1:63.710, precision:68.955, recall:61.669, loss:1.469 at 27
04/16 11:52:39:  Best: f1:65.998, precision:65.243, recall:67.016, loss:1.488 at 21
04/16 11:52:39:  ---------
04/16 11:53:11:  #RES: f1:65.864, precision:69.862, recall:63.340, loss:1.574 at 57
04/16 11:53:11:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:53:11:  ---------
04/16 11:57:24:  #RES: f1:67.253, precision:68.875, recall:65.944, loss:1.686 at 60
04/16 11:57:24:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 11:57:24:  ---------
04/16 11:57:50:  #RES: f1:63.801, precision:66.776, recall:62.947, loss:1.523 at 30
04/16 11:57:50:  Best: f1:65.998, precision:65.243, recall:67.016, loss:1.488 at 21
04/16 11:57:50:  ---------
04/16 12:01:37:  #RES: f1:66.882, precision:70.001, recall:64.731, loss:1.546 at 63
04/16 12:01:37:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:01:37:  ---------
04/16 12:03:03:  #RES: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:03:03:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:03:03:  ---------
04/16 12:05:50:  #RES: f1:66.924, precision:68.061, recall:65.996, loss:1.841 at 66
04/16 12:05:50:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:05:50:  ---------
04/16 12:08:16:  #RES: f1:65.828, precision:65.147, recall:66.627, loss:1.387 at 36
04/16 12:08:16:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:08:16:  ---------
04/16 12:10:02:  #RES: f1:65.214, precision:66.835, recall:63.923, loss:1.693 at 69
04/16 12:10:02:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:10:02:  ---------
04/16 12:13:29:  #RES: f1:64.683, precision:65.428, recall:64.025, loss:1.306 at 39
04/16 12:13:29:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:13:29:  ---------
04/16 12:14:14:  #RES: f1:66.253, precision:69.759, recall:63.919, loss:1.596 at 72
04/16 12:14:14:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:14:14:  ---------
04/16 12:18:27:  #RES: f1:66.700, precision:67.832, recall:65.743, loss:1.656 at 75
04/16 12:18:27:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:18:27:  ---------
04/16 12:18:42:  #RES: f1:65.896, precision:65.912, recall:66.729, loss:1.337 at 42
04/16 12:18:42:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:18:42:  ---------
04/16 12:22:39:  #RES: f1:65.683, precision:66.490, recall:65.211, loss:1.742 at 78
04/16 12:22:39:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:22:39:  ---------
04/16 12:23:54:  #RES: f1:64.620, precision:63.069, recall:67.791, loss:1.296 at 45
04/16 12:23:54:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:23:54:  ---------
04/16 12:26:51:  #RES: f1:65.472, precision:67.226, recall:64.548, loss:1.723 at 81
04/16 12:26:51:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:26:51:  ---------
04/16 12:29:07:  #RES: f1:64.490, precision:64.198, recall:65.620, loss:1.343 at 48
04/16 12:29:07:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:29:07:  ---------
04/16 12:31:03:  #RES: f1:65.889, precision:68.034, recall:64.857, loss:1.823 at 84
04/16 12:31:03:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:31:03:  ---------
04/16 12:34:19:  #RES: f1:65.568, precision:63.964, recall:67.888, loss:1.413 at 51
04/16 12:34:19:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:34:19:  ---------
04/16 12:35:15:  #RES: f1:65.766, precision:67.107, recall:64.930, loss:1.846 at 87
04/16 12:35:15:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:35:15:  ---------
04/16 12:39:27:  #RES: f1:65.829, precision:67.986, recall:64.667, loss:1.786 at 90
04/16 12:39:27:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:39:27:  ---------
04/16 12:39:32:  #RES: f1:63.262, precision:64.832, recall:63.749, loss:1.812 at 54
04/16 12:39:32:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:39:32:  ---------
04/16 12:43:40:  #RES: f1:65.555, precision:68.345, recall:63.778, loss:1.823 at 93
04/16 12:43:40:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:43:40:  ---------
04/16 12:44:45:  #RES: f1:65.120, precision:65.448, recall:65.121, loss:1.588 at 57
04/16 12:44:45:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:44:45:  ---------
04/16 12:47:53:  #RES: f1:65.917, precision:68.705, recall:64.093, loss:1.863 at 96
04/16 12:47:53:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:47:53:  ---------
04/16 12:49:57:  #RES: f1:65.632, precision:66.938, recall:64.641, loss:1.551 at 60
04/16 12:49:57:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:49:57:  ---------
04/16 12:52:06:  #RES: f1:65.574, precision:67.710, recall:64.075, loss:1.852 at 99
04/16 12:52:06:  Best: f1:68.252, precision:66.919, recall:70.196, loss:1.132 at 12
04/16 12:52:06:  ---------
04/16 12:54:05:  #RES: f1:62.456, precision:62.516, recall:63.928, loss:1.568 at 63
04/16 12:54:05:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:54:05:  ---------
04/16 12:56:32:  #RES: f1:63.283, precision:65.808, recall:62.553, loss:1.670 at 66
04/16 12:56:32:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:56:32:  ---------
04/16 12:58:58:  #RES: f1:64.534, precision:63.538, recall:66.449, loss:1.531 at 69
04/16 12:58:58:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 12:58:58:  ---------
04/16 13:01:24:  #RES: f1:65.635, precision:64.642, recall:67.315, loss:1.647 at 72
04/16 13:01:24:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:01:24:  ---------
04/16 13:03:49:  #RES: f1:64.308, precision:63.762, recall:65.608, loss:1.645 at 75
04/16 13:03:49:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:03:49:  ---------
04/16 13:06:15:  #RES: f1:67.168, precision:66.240, recall:68.275, loss:1.567 at 78
04/16 13:06:15:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:06:15:  ---------
04/16 13:08:41:  #RES: f1:64.775, precision:64.749, recall:65.137, loss:1.688 at 81
04/16 13:08:41:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:08:41:  ---------
04/16 13:11:07:  #RES: f1:66.730, precision:66.075, recall:67.610, loss:1.648 at 84
04/16 13:11:07:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:11:07:  ---------
04/16 13:13:33:  #RES: f1:65.738, precision:65.839, recall:65.882, loss:1.565 at 87
04/16 13:13:33:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:13:33:  ---------
04/16 13:15:59:  #RES: f1:65.867, precision:65.426, recall:66.550, loss:1.591 at 90
04/16 13:15:59:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:15:59:  ---------
04/16 13:18:24:  #RES: f1:66.295, precision:65.511, recall:67.336, loss:1.645 at 93
04/16 13:18:24:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:18:24:  ---------
04/16 13:20:51:  #RES: f1:67.163, precision:67.029, recall:67.414, loss:1.666 at 96
04/16 13:20:51:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:20:51:  ---------
04/16 13:23:17:  #RES: f1:67.035, precision:66.523, recall:67.720, loss:1.678 at 99
04/16 13:23:17:  Best: f1:67.396, precision:69.054, recall:66.733, loss:1.273 at 33
04/16 13:23:17:  ---------
04/16 14:55:55:  ======================== New Round =============================
04/16 14:55:55:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 14:55:55:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=32, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 14:58:17:  #RES: f1:64.197, precision:72.653, recall:60.624, loss:0.713 at 2
04/16 14:58:17:  Best: f1:64.197, precision:72.653, recall:60.624, loss:0.713 at 2
04/16 14:58:17:  ---------
04/16 15:00:41:  #RES: f1:61.669, precision:64.058, recall:64.328, loss:0.943 at 6
04/16 15:00:41:  Best: f1:64.197, precision:72.653, recall:60.624, loss:0.713 at 2
04/16 15:00:41:  ---------
04/16 15:03:05:  #RES: f1:63.618, precision:69.950, recall:60.612, loss:1.032 at 9
04/16 15:03:05:  Best: f1:64.197, precision:72.653, recall:60.624, loss:0.713 at 2
04/16 15:03:05:  ---------
04/16 15:05:29:  #RES: f1:63.431, precision:64.998, recall:64.709, loss:1.119 at 12
04/16 15:05:29:  Best: f1:64.197, precision:72.653, recall:60.624, loss:0.713 at 2
04/16 15:05:29:  ---------
04/16 15:07:53:  #RES: f1:64.905, precision:70.667, recall:62.172, loss:1.427 at 15
04/16 15:07:53:  Best: f1:64.905, precision:70.667, recall:62.172, loss:1.427 at 15
04/16 15:07:53:  ---------
04/16 15:10:17:  #RES: f1:67.905, precision:69.943, recall:66.356, loss:1.488 at 18
04/16 15:10:17:  Best: f1:67.905, precision:69.943, recall:66.356, loss:1.488 at 18
04/16 15:10:17:  ---------
04/16 15:12:41:  #RES: f1:66.733, precision:65.627, recall:68.155, loss:1.787 at 21
04/16 15:12:41:  Best: f1:67.905, precision:69.943, recall:66.356, loss:1.488 at 18
04/16 15:12:41:  ---------
04/16 15:15:05:  #RES: f1:68.208, precision:68.499, recall:67.935, loss:2.011 at 24
04/16 15:15:05:  Best: f1:68.208, precision:68.499, recall:67.935, loss:2.011 at 24
04/16 15:15:05:  ---------
04/16 15:17:29:  #RES: f1:68.980, precision:68.771, recall:69.197, loss:2.232 at 27
04/16 15:17:29:  Best: f1:68.980, precision:68.771, recall:69.197, loss:2.232 at 27
04/16 15:17:29:  ---------
04/16 15:19:53:  #RES: f1:66.783, precision:70.230, recall:64.506, loss:2.567 at 30
04/16 15:19:53:  Best: f1:68.980, precision:68.771, recall:69.197, loss:2.232 at 27
04/16 15:19:53:  ---------
04/16 15:22:17:  #RES: f1:69.020, precision:68.222, recall:70.066, loss:2.674 at 33
04/16 15:22:17:  Best: f1:69.020, precision:68.222, recall:70.066, loss:2.674 at 33
04/16 15:22:17:  ---------
04/16 15:24:41:  #RES: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:24:41:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:24:41:  ---------
04/16 15:27:05:  #RES: f1:68.964, precision:69.778, recall:68.501, loss:2.767 at 39
04/16 15:27:05:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:27:05:  ---------
04/16 15:29:28:  #RES: f1:69.103, precision:71.609, recall:67.243, loss:2.755 at 42
04/16 15:29:28:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:29:28:  ---------
04/16 15:31:52:  #RES: f1:66.954, precision:68.668, recall:65.858, loss:2.852 at 45
04/16 15:31:52:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:31:52:  ---------
04/16 15:34:16:  #RES: f1:66.823, precision:67.308, recall:66.811, loss:2.821 at 48
04/16 15:34:16:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:34:16:  ---------
04/16 15:36:40:  #RES: f1:68.683, precision:68.003, recall:69.448, loss:2.907 at 51
04/16 15:36:40:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:36:40:  ---------
04/16 15:39:04:  #RES: f1:67.779, precision:69.316, recall:66.548, loss:2.952 at 54
04/16 15:39:04:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:39:04:  ---------
04/16 15:41:27:  #RES: f1:68.295, precision:69.127, recall:67.648, loss:2.807 at 57
04/16 15:41:27:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:41:27:  ---------
04/16 15:43:51:  #RES: f1:68.421, precision:68.673, recall:68.218, loss:2.903 at 60
04/16 15:43:51:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:43:51:  ---------
04/16 15:46:15:  #RES: f1:68.938, precision:68.227, recall:69.915, loss:2.897 at 63
04/16 15:46:15:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:46:15:  ---------
04/16 15:48:39:  #RES: f1:68.313, precision:67.405, recall:69.445, loss:2.914 at 66
04/16 15:48:39:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:48:39:  ---------
04/16 15:51:03:  #RES: f1:68.212, precision:69.094, recall:67.513, loss:2.985 at 69
04/16 15:51:03:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:51:03:  ---------
04/16 15:53:27:  #RES: f1:67.256, precision:67.101, recall:67.748, loss:2.875 at 72
04/16 15:53:27:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:53:27:  ---------
04/16 15:55:50:  #RES: f1:67.384, precision:67.539, recall:67.237, loss:2.930 at 75
04/16 15:55:50:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:55:50:  ---------
04/16 15:58:14:  #RES: f1:68.982, precision:68.851, recall:69.294, loss:2.993 at 78
04/16 15:58:14:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 15:58:14:  ---------
04/16 16:00:38:  #RES: f1:68.781, precision:69.169, recall:68.523, loss:2.973 at 81
04/16 16:00:38:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:00:38:  ---------
04/16 16:03:02:  #RES: f1:68.641, precision:69.502, recall:67.894, loss:2.958 at 84
04/16 16:03:02:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:03:02:  ---------
04/16 16:05:26:  #RES: f1:68.629, precision:69.718, recall:67.852, loss:3.057 at 87
04/16 16:05:26:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:05:26:  ---------
04/16 16:07:49:  #RES: f1:69.521, precision:70.812, recall:68.434, loss:2.945 at 90
04/16 16:07:49:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:07:49:  ---------
04/16 16:10:14:  #RES: f1:69.591, precision:70.982, recall:68.430, loss:2.939 at 93
04/16 16:10:14:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:10:14:  ---------
04/16 16:12:38:  #RES: f1:69.093, precision:70.244, recall:68.109, loss:2.958 at 96
04/16 16:12:38:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:12:38:  ---------
04/16 16:15:01:  #RES: f1:68.852, precision:69.969, recall:67.899, loss:2.941 at 99
04/16 16:15:01:  Best: f1:69.779, precision:69.646, recall:70.104, loss:2.479 at 36
04/16 16:15:01:  ---------
04/16 16:27:37:  ======================== New Round =============================
04/16 16:27:37:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 16:27:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=128, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=300, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 16:27:47:  ======================== New Round =============================
04/16 16:27:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 16:27:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=128, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 16:30:28:  #RES: f1:61.447, precision:68.231, recall:61.564, loss:0.685 at 3
04/16 16:30:28:  Best: f1:61.447, precision:68.231, recall:61.564, loss:0.685 at 3
04/16 16:30:28:  ---------
04/16 16:57:07:  ======================== New Round =============================
04/16 16:57:07:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 16:57:07:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=64, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 16:58:35:  #RES: f1:61.850, precision:69.501, recall:58.736, loss:0.623 at 1
04/16 16:58:35:  Best: f1:61.850, precision:69.501, recall:58.736, loss:0.623 at 1
04/16 16:58:35:  ---------
04/16 17:00:04:  #RES: f1:68.091, precision:70.255, recall:66.659, loss:0.658 at 3
04/16 17:00:04:  Best: f1:68.091, precision:70.255, recall:66.659, loss:0.658 at 3
04/16 17:00:04:  ---------
04/16 17:01:34:  #RES: f1:67.636, precision:69.880, recall:66.751, loss:0.739 at 5
04/16 17:01:34:  Best: f1:68.091, precision:70.255, recall:66.659, loss:0.658 at 3
04/16 17:01:34:  ---------
04/16 17:03:04:  #RES: f1:62.176, precision:70.579, recall:61.628, loss:0.911 at 7
04/16 17:03:04:  Best: f1:68.091, precision:70.255, recall:66.659, loss:0.658 at 3
04/16 17:03:04:  ---------
04/16 17:04:34:  #RES: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:04:34:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:04:34:  ---------
04/16 17:06:04:  #RES: f1:65.460, precision:63.638, recall:69.962, loss:0.914 at 12
04/16 17:06:04:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:06:04:  ---------
04/16 17:07:34:  #RES: f1:64.154, precision:68.703, recall:62.925, loss:1.218 at 14
04/16 17:07:34:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:07:34:  ---------
04/16 17:09:04:  #RES: f1:66.137, precision:74.767, recall:62.394, loss:1.211 at 16
04/16 17:09:04:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:09:04:  ---------
04/16 17:10:35:  #RES: f1:67.518, precision:72.574, recall:64.591, loss:1.011 at 18
04/16 17:10:35:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:10:35:  ---------
04/16 17:12:04:  #RES: f1:67.962, precision:68.202, recall:68.147, loss:1.290 at 20
04/16 17:12:04:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:12:04:  ---------
04/16 17:13:34:  #RES: f1:68.391, precision:66.705, recall:71.385, loss:1.642 at 22
04/16 17:13:34:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:13:34:  ---------
04/16 17:15:04:  #RES: f1:68.491, precision:66.341, recall:72.337, loss:1.574 at 24
04/16 17:15:04:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:15:04:  ---------
04/16 17:16:34:  #RES: f1:69.198, precision:70.036, recall:68.875, loss:1.613 at 26
04/16 17:16:34:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:16:34:  ---------
04/16 17:18:05:  #RES: f1:68.657, precision:68.006, recall:70.307, loss:1.816 at 28
04/16 17:18:05:  Best: f1:69.792, precision:71.474, recall:68.432, loss:0.712 at 10
04/16 17:18:05:  ---------
04/16 17:19:34:  #RES: f1:69.873, precision:68.967, recall:70.937, loss:1.882 at 30
04/16 17:19:34:  Best: f1:69.873, precision:68.967, recall:70.937, loss:1.882 at 30
04/16 17:19:34:  ---------
04/16 17:21:05:  #RES: f1:68.802, precision:68.320, recall:69.763, loss:2.110 at 32
04/16 17:21:05:  Best: f1:69.873, precision:68.967, recall:70.937, loss:1.882 at 30
04/16 17:21:05:  ---------
04/16 17:22:35:  #RES: f1:69.197, precision:68.207, recall:70.388, loss:2.134 at 34
04/16 17:22:35:  Best: f1:69.873, precision:68.967, recall:70.937, loss:1.882 at 30
04/16 17:22:35:  ---------
04/16 17:24:04:  #RES: f1:70.271, precision:69.360, recall:71.491, loss:2.115 at 36
04/16 17:24:04:  Best: f1:70.271, precision:69.360, recall:71.491, loss:2.115 at 36
04/16 17:24:04:  ---------
04/16 17:25:35:  #RES: f1:69.955, precision:69.499, recall:70.744, loss:2.250 at 38
04/16 17:25:35:  Best: f1:70.271, precision:69.360, recall:71.491, loss:2.115 at 36
04/16 17:25:35:  ---------
04/16 17:27:05:  #RES: f1:70.451, precision:71.393, recall:69.841, loss:2.310 at 40
04/16 17:27:05:  Best: f1:70.451, precision:71.393, recall:69.841, loss:2.310 at 40
04/16 17:27:05:  ---------
04/16 17:28:35:  #RES: f1:69.096, precision:69.692, recall:69.130, loss:2.524 at 42
04/16 17:28:35:  Best: f1:70.451, precision:71.393, recall:69.841, loss:2.310 at 40
04/16 17:28:35:  ---------
04/16 17:30:04:  #RES: f1:71.055, precision:71.138, recall:71.069, loss:2.331 at 44
04/16 17:30:04:  Best: f1:71.055, precision:71.138, recall:71.069, loss:2.331 at 44
04/16 17:30:04:  ---------
04/16 17:31:35:  #RES: f1:69.947, precision:70.309, recall:69.647, loss:2.291 at 46
04/16 17:31:35:  Best: f1:71.055, precision:71.138, recall:71.069, loss:2.331 at 44
04/16 17:31:35:  ---------
04/16 17:33:04:  #RES: f1:70.218, precision:70.796, recall:69.823, loss:2.309 at 48
04/16 17:33:04:  Best: f1:71.055, precision:71.138, recall:71.069, loss:2.331 at 44
04/16 17:33:04:  ---------
04/16 17:34:34:  #RES: f1:71.407, precision:70.689, recall:72.222, loss:2.458 at 50
04/16 17:34:34:  Best: f1:71.407, precision:70.689, recall:72.222, loss:2.458 at 50
04/16 17:34:34:  ---------
04/16 17:36:04:  #RES: f1:70.857, precision:70.141, recall:71.731, loss:2.261 at 52
04/16 17:36:04:  Best: f1:71.407, precision:70.689, recall:72.222, loss:2.458 at 50
04/16 17:36:04:  ---------
04/16 17:37:34:  #RES: f1:70.596, precision:70.541, recall:70.652, loss:2.257 at 54
04/16 17:37:34:  Best: f1:71.407, precision:70.689, recall:72.222, loss:2.458 at 50
04/16 17:37:34:  ---------
04/16 17:39:04:  #RES: f1:71.877, precision:71.826, recall:72.015, loss:2.466 at 56
04/16 17:39:04:  Best: f1:71.877, precision:71.826, recall:72.015, loss:2.466 at 56
04/16 17:39:04:  ---------
04/16 17:40:34:  #RES: f1:71.242, precision:71.274, recall:71.295, loss:2.398 at 58
04/16 17:40:34:  Best: f1:71.877, precision:71.826, recall:72.015, loss:2.466 at 56
04/16 17:40:34:  ---------
04/16 17:42:04:  #RES: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:42:04:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:42:04:  ---------
04/16 17:43:34:  #RES: f1:71.042, precision:72.102, recall:70.202, loss:2.697 at 62
04/16 17:43:34:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:43:34:  ---------
04/16 17:45:03:  #RES: f1:70.287, precision:70.714, recall:69.886, loss:2.252 at 64
04/16 17:45:03:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:45:03:  ---------
04/16 17:46:33:  #RES: f1:70.379, precision:70.749, recall:70.163, loss:2.629 at 66
04/16 17:46:33:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:46:33:  ---------
04/16 17:48:03:  #RES: f1:70.027, precision:72.345, recall:68.514, loss:2.714 at 68
04/16 17:48:03:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:48:03:  ---------
04/16 17:49:33:  #RES: f1:70.796, precision:71.998, recall:69.779, loss:2.412 at 70
04/16 17:49:33:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:49:33:  ---------
04/16 17:51:03:  #RES: f1:70.659, precision:71.159, recall:70.229, loss:2.495 at 72
04/16 17:51:03:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:51:03:  ---------
04/16 17:52:33:  #RES: f1:71.165, precision:70.701, recall:71.697, loss:2.446 at 74
04/16 17:52:33:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:52:33:  ---------
04/16 17:54:03:  #RES: f1:71.496, precision:71.688, recall:71.340, loss:2.438 at 76
04/16 17:54:03:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:54:03:  ---------
04/16 17:55:34:  #RES: f1:70.713, precision:71.384, recall:70.115, loss:2.476 at 78
04/16 17:55:34:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:55:34:  ---------
04/16 17:57:04:  #RES: f1:70.821, precision:70.465, recall:71.221, loss:2.536 at 80
04/16 17:57:04:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:57:04:  ---------
04/16 17:58:34:  #RES: f1:71.237, precision:71.293, recall:71.205, loss:2.598 at 82
04/16 17:58:34:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 17:58:34:  ---------
04/16 18:00:04:  #RES: f1:71.601, precision:70.957, recall:72.318, loss:2.604 at 84
04/16 18:00:04:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 18:00:04:  ---------
04/16 18:01:34:  #RES: f1:71.905, precision:71.667, recall:72.156, loss:2.582 at 86
04/16 18:01:34:  Best: f1:72.116, precision:73.121, recall:71.254, loss:2.482 at 60
04/16 18:01:34:  ---------
04/16 18:03:04:  #RES: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:03:04:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:03:04:  ---------
04/16 18:04:34:  #RES: f1:71.396, precision:70.789, recall:72.062, loss:2.493 at 90
04/16 18:04:34:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:04:34:  ---------
04/16 18:06:04:  #RES: f1:71.666, precision:71.184, recall:72.188, loss:2.474 at 92
04/16 18:06:04:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:06:04:  ---------
04/16 18:07:34:  #RES: f1:71.546, precision:71.349, recall:71.752, loss:2.471 at 94
04/16 18:07:34:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:07:34:  ---------
04/16 18:09:03:  #RES: f1:71.590, precision:71.489, recall:71.706, loss:2.501 at 96
04/16 18:09:03:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:09:03:  ---------
04/16 18:10:33:  #RES: f1:71.846, precision:71.884, recall:71.816, loss:2.514 at 98
04/16 18:10:33:  Best: f1:72.159, precision:71.841, recall:72.508, loss:2.572 at 88
04/16 18:10:33:  ---------
04/16 18:29:52:  ======================== New Round =============================
04/16 18:29:52:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 18:29:52:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=64, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 18:30:27:  #RES: f1:59.606, precision:58.704, recall:63.697, loss:0.746 at 1
04/16 18:30:27:  Best: f1:59.606, precision:58.704, recall:63.697, loss:0.746 at 1
04/16 18:30:27:  ---------
04/16 18:31:03:  #RES: f1:66.763, precision:68.689, recall:66.012, loss:0.624 at 3
04/16 18:31:03:  Best: f1:66.763, precision:68.689, recall:66.012, loss:0.624 at 3
04/16 18:31:03:  ---------
04/16 18:31:38:  #RES: f1:66.265, precision:65.534, recall:68.707, loss:0.729 at 5
04/16 18:31:38:  Best: f1:66.763, precision:68.689, recall:66.012, loss:0.624 at 3
04/16 18:31:38:  ---------
04/16 18:32:13:  #RES: f1:63.096, precision:71.032, recall:59.642, loss:0.825 at 7
04/16 18:32:13:  Best: f1:66.763, precision:68.689, recall:66.012, loss:0.624 at 3
04/16 18:32:13:  ---------
04/16 18:32:49:  #RES: f1:68.570, precision:67.932, recall:69.365, loss:0.841 at 9
04/16 18:32:49:  Best: f1:68.570, precision:67.932, recall:69.365, loss:0.841 at 9
04/16 18:32:49:  ---------
04/16 18:33:24:  #RES: f1:67.417, precision:67.649, recall:67.258, loss:0.923 at 11
04/16 18:33:24:  Best: f1:68.570, precision:67.932, recall:69.365, loss:0.841 at 9
04/16 18:33:24:  ---------
04/16 18:34:00:  #RES: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:34:00:  Best: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:34:00:  ---------
04/16 18:34:36:  #RES: f1:67.457, precision:70.851, recall:65.161, loss:1.212 at 15
04/16 18:34:36:  Best: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:34:36:  ---------
04/16 18:35:12:  #RES: f1:67.605, precision:66.269, recall:70.337, loss:1.301 at 17
04/16 18:35:12:  Best: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:35:12:  ---------
04/16 18:35:48:  #RES: f1:67.298, precision:65.942, recall:69.835, loss:1.473 at 19
04/16 18:35:48:  Best: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:35:48:  ---------
04/16 18:36:23:  #RES: f1:68.949, precision:68.079, recall:70.126, loss:1.515 at 21
04/16 18:36:23:  Best: f1:69.585, precision:71.201, recall:68.293, loss:0.960 at 13
04/16 18:36:23:  ---------
04/16 18:36:59:  #RES: f1:69.996, precision:69.237, recall:70.857, loss:1.595 at 23
04/16 18:36:59:  Best: f1:69.996, precision:69.237, recall:70.857, loss:1.595 at 23
04/16 18:36:59:  ---------
04/16 18:37:35:  #RES: f1:69.338, precision:70.456, recall:68.428, loss:1.712 at 25
04/16 18:37:35:  Best: f1:69.996, precision:69.237, recall:70.857, loss:1.595 at 23
04/16 18:37:35:  ---------
04/16 18:38:11:  #RES: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:38:11:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:38:11:  ---------
04/16 18:38:47:  #RES: f1:67.321, precision:65.940, recall:70.051, loss:1.993 at 29
04/16 18:38:47:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:38:47:  ---------
04/16 18:39:22:  #RES: f1:69.136, precision:68.378, recall:69.992, loss:2.012 at 31
04/16 18:39:22:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:39:22:  ---------
04/16 18:39:58:  #RES: f1:69.996, precision:69.222, recall:71.041, loss:2.027 at 33
04/16 18:39:58:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:39:58:  ---------
04/16 18:40:34:  #RES: f1:69.937, precision:69.312, recall:70.622, loss:2.070 at 35
04/16 18:40:34:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:40:34:  ---------
04/16 18:41:10:  #RES: f1:68.762, precision:67.334, recall:70.675, loss:2.015 at 37
04/16 18:41:10:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:41:10:  ---------
04/16 18:41:45:  #RES: f1:69.635, precision:68.725, recall:70.692, loss:1.985 at 39
04/16 18:41:45:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:41:45:  ---------
04/16 18:42:21:  #RES: f1:69.555, precision:69.777, recall:69.477, loss:2.093 at 41
04/16 18:42:21:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:42:21:  ---------
04/16 18:42:57:  #RES: f1:69.065, precision:69.821, recall:68.501, loss:2.114 at 43
04/16 18:42:57:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:42:57:  ---------
04/16 18:43:33:  #RES: f1:68.877, precision:69.682, recall:68.252, loss:2.120 at 45
04/16 18:43:33:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:43:33:  ---------
04/16 18:44:08:  #RES: f1:69.016, precision:71.734, recall:67.036, loss:2.209 at 47
04/16 18:44:08:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:44:08:  ---------
04/16 18:44:44:  #RES: f1:68.873, precision:70.113, recall:67.838, loss:2.183 at 49
04/16 18:44:44:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:44:44:  ---------
04/16 18:45:20:  #RES: f1:68.143, precision:67.830, recall:68.477, loss:2.225 at 51
04/16 18:45:20:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:45:20:  ---------
04/16 18:45:55:  #RES: f1:68.628, precision:69.470, recall:67.901, loss:2.256 at 53
04/16 18:45:55:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:45:55:  ---------
04/16 18:46:31:  #RES: f1:69.554, precision:70.549, recall:68.684, loss:2.207 at 55
04/16 18:46:31:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:46:31:  ---------
04/16 18:47:07:  #RES: f1:69.442, precision:70.816, recall:68.293, loss:2.214 at 57
04/16 18:47:07:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:47:07:  ---------
04/16 18:47:43:  #RES: f1:68.252, precision:69.288, recall:67.461, loss:2.167 at 59
04/16 18:47:43:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:47:43:  ---------
04/16 18:48:18:  #RES: f1:68.506, precision:69.913, recall:67.342, loss:2.258 at 61
04/16 18:48:18:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:48:18:  ---------
04/16 18:48:54:  #RES: f1:69.150, precision:68.767, recall:69.556, loss:2.234 at 63
04/16 18:48:54:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:48:54:  ---------
04/16 18:49:30:  #RES: f1:68.576, precision:67.992, recall:69.314, loss:2.197 at 65
04/16 18:49:30:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:49:30:  ---------
04/16 18:50:06:  #RES: f1:68.697, precision:69.661, recall:67.996, loss:2.253 at 67
04/16 18:50:06:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:50:06:  ---------
04/16 18:50:41:  #RES: f1:68.287, precision:69.766, recall:67.111, loss:2.198 at 69
04/16 18:50:41:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:50:41:  ---------
04/16 18:51:17:  #RES: f1:68.938, precision:69.573, recall:68.471, loss:2.166 at 71
04/16 18:51:17:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:51:17:  ---------
04/16 18:56:22:  #RES: f1:69.104, precision:71.074, recall:67.660, loss:2.236 at 73
04/16 18:56:22:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:56:22:  ---------
04/16 18:56:58:  #RES: f1:69.529, precision:70.456, recall:68.911, loss:2.217 at 75
04/16 18:56:58:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:56:58:  ---------
04/16 18:57:33:  #RES: f1:69.773, precision:70.551, recall:69.684, loss:2.352 at 77
04/16 18:57:33:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:57:33:  ---------
04/16 18:58:08:  #RES: f1:69.832, precision:71.306, recall:68.657, loss:2.306 at 79
04/16 18:58:08:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:58:08:  ---------
04/16 18:58:44:  #RES: f1:69.628, precision:71.220, recall:68.396, loss:2.346 at 81
04/16 18:58:44:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:58:44:  ---------
04/16 18:59:19:  #RES: f1:69.487, precision:71.009, recall:68.247, loss:2.298 at 83
04/16 18:59:19:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:59:19:  ---------
04/16 18:59:55:  #RES: f1:68.842, precision:69.651, recall:68.123, loss:2.272 at 85
04/16 18:59:55:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 18:59:55:  ---------
04/16 19:00:31:  #RES: f1:69.109, precision:70.399, recall:68.023, loss:2.284 at 87
04/16 19:00:31:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:00:31:  ---------
04/16 19:01:07:  #RES: f1:69.024, precision:69.639, recall:68.459, loss:2.284 at 89
04/16 19:01:07:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:01:07:  ---------
04/16 19:01:42:  #RES: f1:69.377, precision:70.093, recall:68.734, loss:2.306 at 91
04/16 19:01:42:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:01:42:  ---------
04/16 19:02:18:  #RES: f1:69.519, precision:70.296, recall:68.853, loss:2.329 at 93
04/16 19:02:18:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:02:18:  ---------
04/16 19:02:54:  #RES: f1:69.279, precision:70.781, recall:68.096, loss:2.364 at 95
04/16 19:02:54:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:02:54:  ---------
04/16 19:03:30:  #RES: f1:69.154, precision:70.543, recall:67.988, loss:2.327 at 97
04/16 19:03:30:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:03:30:  ---------
04/16 19:04:06:  #RES: f1:69.154, precision:70.543, recall:67.988, loss:2.327 at 99
04/16 19:04:06:  Best: f1:70.553, precision:70.659, recall:70.493, loss:1.783 at 27
04/16 19:04:06:  ---------
04/16 23:45:12:  ======================== New Round =============================
04/16 23:45:12:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/16 23:45:12:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=64, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/16 23:46:29:  #RES: f1:65.343, precision:68.916, recall:63.197, loss:0.623 at 1
04/16 23:46:29:  Best: f1:65.343, precision:68.916, recall:63.197, loss:0.623 at 1
04/16 23:46:29:  ---------
04/16 23:47:48:  #RES: f1:57.661, precision:72.641, recall:53.978, loss:0.705 at 3
04/16 23:47:48:  Best: f1:65.343, precision:68.916, recall:63.197, loss:0.623 at 1
04/16 23:47:48:  ---------
04/16 23:49:07:  #RES: f1:61.963, precision:64.631, recall:66.923, loss:0.889 at 5
04/16 23:49:07:  Best: f1:65.343, precision:68.916, recall:63.197, loss:0.623 at 1
04/16 23:49:07:  ---------
04/16 23:50:26:  #RES: f1:62.509, precision:69.792, recall:60.996, loss:0.883 at 7
04/16 23:50:26:  Best: f1:65.343, precision:68.916, recall:63.197, loss:0.623 at 1
04/16 23:50:26:  ---------
04/16 23:51:46:  #RES: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:51:46:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:51:46:  ---------
04/16 23:53:06:  #RES: f1:64.513, precision:68.231, recall:64.056, loss:0.985 at 12
04/16 23:53:06:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:53:06:  ---------
04/16 23:54:25:  #RES: f1:64.086, precision:71.301, recall:61.045, loss:0.916 at 14
04/16 23:54:25:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:54:25:  ---------
04/16 23:55:44:  #RES: f1:63.358, precision:70.477, recall:60.043, loss:1.086 at 16
04/16 23:55:44:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:55:44:  ---------
04/16 23:57:04:  #RES: f1:64.053, precision:63.318, recall:68.402, loss:1.270 at 18
04/16 23:57:04:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:57:04:  ---------
04/16 23:58:24:  #RES: f1:66.685, precision:65.798, recall:68.507, loss:1.340 at 20
04/16 23:58:24:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:58:24:  ---------
04/16 23:59:43:  #RES: f1:67.013, precision:65.525, recall:70.188, loss:1.399 at 22
04/16 23:59:43:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/16 23:59:43:  ---------
04/17 00:01:02:  #RES: f1:66.325, precision:65.075, recall:70.356, loss:1.507 at 24
04/17 00:01:02:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/17 00:01:02:  ---------
04/17 00:02:21:  #RES: f1:64.417, precision:63.250, recall:69.303, loss:1.227 at 26
04/17 00:02:21:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/17 00:02:21:  ---------
04/17 00:03:41:  #RES: f1:65.634, precision:63.940, recall:69.143, loss:1.205 at 28
04/17 00:03:41:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/17 00:03:41:  ---------
04/17 00:05:00:  #RES: f1:67.034, precision:66.756, recall:67.485, loss:1.385 at 30
04/17 00:05:00:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/17 00:05:00:  ---------
04/17 00:06:20:  #RES: f1:65.857, precision:67.047, recall:64.916, loss:1.384 at 32
04/17 00:06:20:  Best: f1:67.399, precision:65.463, recall:72.120, loss:0.899 at 10
04/17 00:06:20:  ---------
04/17 00:07:39:  #RES: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:07:39:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:07:39:  ---------
04/17 00:08:59:  #RES: f1:67.698, precision:67.941, recall:67.466, loss:1.275 at 36
04/17 00:08:59:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:08:59:  ---------
04/17 00:10:18:  #RES: f1:66.467, precision:66.143, recall:66.812, loss:1.398 at 38
04/17 00:10:18:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:10:18:  ---------
04/17 00:11:38:  #RES: f1:66.805, precision:67.552, recall:66.189, loss:1.480 at 40
04/17 00:11:38:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:11:38:  ---------
04/17 00:12:57:  #RES: f1:67.550, precision:68.650, recall:66.811, loss:1.673 at 42
04/17 00:12:57:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:12:57:  ---------
04/17 00:14:17:  #RES: f1:66.244, precision:67.936, recall:65.319, loss:1.860 at 44
04/17 00:14:17:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:14:17:  ---------
04/17 00:15:36:  #RES: f1:66.738, precision:67.673, recall:66.022, loss:1.793 at 46
04/17 00:15:36:  Best: f1:67.849, precision:66.176, recall:70.247, loss:1.527 at 34
04/17 00:15:36:  ---------
04/17 00:16:56:  #RES: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:16:56:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:16:56:  ---------
04/17 00:18:15:  #RES: f1:67.669, precision:67.168, recall:68.386, loss:1.572 at 50
04/17 00:18:15:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:18:15:  ---------
04/17 00:19:34:  #RES: f1:66.931, precision:65.855, recall:68.493, loss:1.600 at 52
04/17 00:19:34:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:19:34:  ---------
04/17 00:20:54:  #RES: f1:67.862, precision:66.107, recall:70.435, loss:1.592 at 54
04/17 00:20:54:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:20:54:  ---------
04/17 00:22:13:  #RES: f1:67.665, precision:67.489, recall:68.039, loss:1.667 at 56
04/17 00:22:13:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:22:13:  ---------
04/17 00:23:33:  #RES: f1:67.437, precision:66.747, recall:68.301, loss:1.596 at 58
04/17 00:23:33:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:23:33:  ---------
04/17 00:24:52:  #RES: f1:67.228, precision:66.328, recall:68.520, loss:1.700 at 60
04/17 00:24:52:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:24:52:  ---------
04/17 00:26:11:  #RES: f1:67.527, precision:66.787, recall:68.516, loss:1.698 at 62
04/17 00:26:11:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:26:11:  ---------
04/17 00:27:30:  #RES: f1:67.264, precision:66.735, recall:67.843, loss:1.708 at 64
04/17 00:27:30:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:27:30:  ---------
04/17 00:28:50:  #RES: f1:66.675, precision:66.734, recall:66.962, loss:1.594 at 66
04/17 00:28:50:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:28:50:  ---------
04/17 00:30:09:  #RES: f1:67.040, precision:66.968, recall:67.116, loss:1.737 at 68
04/17 00:30:09:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:30:09:  ---------
04/17 00:31:29:  #RES: f1:66.929, precision:67.034, recall:66.894, loss:1.530 at 70
04/17 00:31:29:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:31:29:  ---------
04/17 00:32:48:  #RES: f1:66.181, precision:66.275, recall:66.092, loss:1.678 at 72
04/17 00:32:48:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:32:48:  ---------
04/17 00:34:07:  #RES: f1:66.473, precision:67.343, recall:65.734, loss:1.682 at 74
04/17 00:34:07:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:34:07:  ---------
04/17 00:35:27:  #RES: f1:66.136, precision:67.839, recall:64.861, loss:1.630 at 76
04/17 00:35:27:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:35:27:  ---------
04/17 00:36:47:  #RES: f1:66.811, precision:67.911, recall:65.898, loss:1.705 at 78
04/17 00:36:47:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:36:47:  ---------
04/17 00:38:07:  #RES: f1:67.501, precision:69.650, recall:65.901, loss:1.631 at 80
04/17 00:38:07:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:38:07:  ---------
04/17 00:39:26:  #RES: f1:66.054, precision:67.341, recall:65.027, loss:1.637 at 82
04/17 00:39:26:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:39:26:  ---------
04/17 00:40:46:  #RES: f1:66.483, precision:67.930, recall:65.308, loss:1.661 at 84
04/17 00:40:46:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:40:46:  ---------
04/17 00:42:05:  #RES: f1:66.066, precision:67.630, recall:64.842, loss:1.642 at 86
04/17 00:42:05:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:42:05:  ---------
04/17 00:43:24:  #RES: f1:65.588, precision:67.736, recall:63.996, loss:1.596 at 88
04/17 00:43:24:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:43:24:  ---------
04/17 00:44:44:  #RES: f1:66.446, precision:68.991, recall:64.621, loss:1.655 at 90
04/17 00:44:44:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:44:44:  ---------
04/17 00:46:03:  #RES: f1:65.779, precision:67.347, recall:64.543, loss:1.597 at 92
04/17 00:46:03:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:46:03:  ---------
04/17 00:47:23:  #RES: f1:65.643, precision:67.164, recall:64.433, loss:1.666 at 94
04/17 00:47:23:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:47:23:  ---------
04/17 00:48:42:  #RES: f1:66.170, precision:67.991, recall:64.758, loss:1.651 at 96
04/17 00:48:42:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:48:42:  ---------
04/17 00:50:01:  #RES: f1:65.806, precision:67.331, recall:64.593, loss:1.664 at 98
04/17 00:50:01:  Best: f1:68.039, precision:67.852, recall:68.350, loss:1.395 at 48
04/17 00:50:01:  ---------
04/17 16:09:24:  ======================== New Round =============================
04/17 16:09:24:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/17 16:09:24:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, batch_size=64, beta=0.6, dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/17 16:09:49:  #RES: f1:63.374, precision:69.647, recall:60.413, loss:0.634 at 1
04/17 16:09:49:  Best: f1:63.374, precision:69.647, recall:60.413, loss:0.634 at 1
04/17 16:09:49:  ---------
04/17 16:10:13:  #RES: f1:67.041, precision:66.390, recall:68.807, loss:0.634 at 3
04/17 16:10:13:  Best: f1:67.041, precision:66.390, recall:68.807, loss:0.634 at 3
04/17 16:10:13:  ---------
04/17 16:10:38:  #RES: f1:64.221, precision:72.086, recall:60.748, loss:0.706 at 5
04/17 16:10:38:  Best: f1:67.041, precision:66.390, recall:68.807, loss:0.634 at 3
04/17 16:10:38:  ---------
04/17 16:11:03:  #RES: f1:67.479, precision:66.360, recall:69.126, loss:0.709 at 7
04/17 16:11:03:  Best: f1:67.479, precision:66.360, recall:69.126, loss:0.709 at 7
04/17 16:11:03:  ---------
04/17 16:11:28:  #RES: f1:63.855, precision:70.964, recall:60.477, loss:0.780 at 9
04/17 16:11:28:  Best: f1:67.479, precision:66.360, recall:69.126, loss:0.709 at 7
04/17 16:11:28:  ---------
04/17 16:11:53:  #RES: f1:65.608, precision:67.314, recall:64.312, loss:0.783 at 11
04/17 16:11:53:  Best: f1:67.479, precision:66.360, recall:69.126, loss:0.709 at 7
04/17 16:11:53:  ---------
04/17 16:12:18:  #RES: f1:63.386, precision:67.533, recall:65.049, loss:1.024 at 13
04/17 16:12:18:  Best: f1:67.479, precision:66.360, recall:69.126, loss:0.709 at 7
04/17 16:12:18:  ---------
04/17 16:12:43:  #RES: f1:67.608, precision:65.833, recall:70.530, loss:0.958 at 15
04/17 16:12:43:  Best: f1:67.608, precision:65.833, recall:70.530, loss:0.958 at 15
04/17 16:12:43:  ---------
04/17 16:13:08:  #RES: f1:67.691, precision:65.719, recall:71.723, loss:0.949 at 17
04/17 16:13:08:  Best: f1:67.691, precision:65.719, recall:71.723, loss:0.949 at 17
04/17 16:13:08:  ---------
04/17 16:13:33:  #RES: f1:66.277, precision:66.123, recall:68.835, loss:1.001 at 19
04/17 16:13:33:  Best: f1:67.691, precision:65.719, recall:71.723, loss:0.949 at 17
04/17 16:13:33:  ---------
04/17 16:13:59:  #RES: f1:66.869, precision:64.489, recall:72.996, loss:1.125 at 21
04/17 16:13:59:  Best: f1:67.691, precision:65.719, recall:71.723, loss:0.949 at 17
04/17 16:13:59:  ---------
04/17 16:14:24:  #RES: f1:66.142, precision:66.722, recall:65.896, loss:1.009 at 23
04/17 16:14:24:  Best: f1:67.691, precision:65.719, recall:71.723, loss:0.949 at 17
04/17 16:14:24:  ---------
04/17 16:14:49:  #RES: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:14:49:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:14:49:  ---------
04/17 16:15:14:  #RES: f1:67.536, precision:67.188, recall:68.059, loss:1.095 at 27
04/17 16:15:14:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:15:14:  ---------
04/17 16:15:40:  #RES: f1:66.556, precision:68.987, recall:64.781, loss:1.188 at 29
04/17 16:15:40:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:15:40:  ---------
04/17 16:16:05:  #RES: f1:66.234, precision:66.855, recall:65.671, loss:1.298 at 31
04/17 16:16:05:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:16:05:  ---------
04/17 16:16:30:  #RES: f1:66.724, precision:68.140, recall:65.592, loss:1.218 at 33
04/17 16:16:30:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:16:30:  ---------
04/17 16:16:55:  #RES: f1:66.588, precision:67.793, recall:65.578, loss:1.174 at 35
04/17 16:16:55:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:16:55:  ---------
04/17 16:17:21:  #RES: f1:66.185, precision:69.502, recall:64.227, loss:1.155 at 37
04/17 16:17:21:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:17:21:  ---------
04/17 16:17:46:  #RES: f1:65.793, precision:68.288, recall:64.885, loss:1.249 at 39
04/17 16:17:46:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:17:46:  ---------
04/17 16:18:11:  #RES: f1:65.178, precision:66.884, recall:64.537, loss:1.284 at 41
04/17 16:18:11:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:18:11:  ---------
04/17 16:18:36:  #RES: f1:65.992, precision:67.231, recall:65.513, loss:1.385 at 43
04/17 16:18:36:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:18:36:  ---------
04/17 16:19:02:  #RES: f1:66.454, precision:67.164, recall:66.993, loss:1.398 at 45
04/17 16:19:02:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:19:02:  ---------
04/17 16:19:27:  #RES: f1:66.046, precision:67.066, recall:66.378, loss:1.293 at 47
04/17 16:19:27:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:19:27:  ---------
04/17 16:19:52:  #RES: f1:64.870, precision:68.393, recall:63.848, loss:1.402 at 49
04/17 16:19:52:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:19:52:  ---------
04/17 16:20:17:  #RES: f1:65.941, precision:70.288, recall:63.575, loss:1.350 at 51
04/17 16:20:17:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:20:17:  ---------
04/17 16:20:43:  #RES: f1:64.560, precision:68.561, recall:62.455, loss:1.310 at 53
04/17 16:20:43:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:20:43:  ---------
04/17 16:21:08:  #RES: f1:65.787, precision:67.223, recall:65.273, loss:1.309 at 55
04/17 16:21:08:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:21:08:  ---------
04/17 16:21:33:  #RES: f1:65.140, precision:65.829, recall:65.234, loss:1.368 at 57
04/17 16:21:33:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:21:33:  ---------
04/17 16:21:58:  #RES: f1:66.404, precision:68.191, recall:65.289, loss:1.417 at 59
04/17 16:21:58:  Best: f1:67.716, precision:67.367, recall:68.174, loss:1.063 at 25
04/17 16:21:58:  ---------
04/17 16:22:24:  #RES: f1:67.720, precision:68.040, recall:67.916, loss:1.355 at 61
04/17 16:22:24:  Best: f1:67.720, precision:68.040, recall:67.916, loss:1.355 at 61
04/17 16:22:24:  ---------
04/17 16:22:49:  #RES: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:22:49:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:22:49:  ---------
04/17 16:23:14:  #RES: f1:66.327, precision:68.185, recall:65.000, loss:1.391 at 65
04/17 16:23:14:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:23:14:  ---------
04/17 16:23:39:  #RES: f1:67.557, precision:67.411, recall:67.819, loss:1.272 at 67
04/17 16:23:39:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:23:39:  ---------
04/17 16:24:05:  #RES: f1:66.802, precision:66.500, recall:67.220, loss:1.386 at 69
04/17 16:24:05:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:24:05:  ---------
04/17 16:24:30:  #RES: f1:67.173, precision:67.371, recall:67.034, loss:1.292 at 71
04/17 16:24:30:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:24:30:  ---------
04/17 16:24:55:  #RES: f1:66.920, precision:67.900, recall:66.070, loss:1.313 at 73
04/17 16:24:55:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:24:55:  ---------
04/17 16:25:20:  #RES: f1:67.611, precision:67.835, recall:67.435, loss:1.280 at 75
04/17 16:25:20:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:25:20:  ---------
04/17 16:25:45:  #RES: f1:65.495, precision:67.322, recall:64.468, loss:1.429 at 77
04/17 16:25:45:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:25:45:  ---------
04/17 16:26:11:  #RES: f1:66.301, precision:68.077, recall:65.149, loss:1.284 at 79
04/17 16:26:11:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:26:11:  ---------
04/17 16:26:36:  #RES: f1:67.346, precision:67.892, recall:67.037, loss:1.293 at 81
04/17 16:26:36:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:26:36:  ---------
04/17 16:27:01:  #RES: f1:66.818, precision:68.394, recall:65.870, loss:1.362 at 83
04/17 16:27:01:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:27:01:  ---------
04/17 16:27:26:  #RES: f1:67.680, precision:68.183, recall:67.497, loss:1.375 at 85
04/17 16:27:26:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:27:26:  ---------
04/17 16:27:52:  #RES: f1:67.788, precision:68.925, recall:67.001, loss:1.336 at 87
04/17 16:27:52:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:27:52:  ---------
04/17 16:28:17:  #RES: f1:67.738, precision:68.469, recall:67.342, loss:1.343 at 89
04/17 16:28:17:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:28:17:  ---------
04/17 16:28:42:  #RES: f1:67.907, precision:68.532, recall:67.492, loss:1.311 at 91
04/17 16:28:42:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:28:42:  ---------
04/17 16:29:07:  #RES: f1:68.194, precision:68.894, recall:67.707, loss:1.336 at 93
04/17 16:29:07:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:29:07:  ---------
04/17 16:29:33:  #RES: f1:67.923, precision:68.858, recall:67.262, loss:1.353 at 95
04/17 16:29:33:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:29:33:  ---------
04/17 16:29:58:  #RES: f1:68.317, precision:69.612, recall:67.491, loss:1.358 at 97
04/17 16:29:58:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:29:58:  ---------
04/17 16:30:23:  #RES: f1:68.125, precision:69.549, recall:67.141, loss:1.358 at 99
04/17 16:30:23:  Best: f1:68.345, precision:68.362, recall:68.434, loss:1.369 at 63
04/17 16:30:23:  ---------
04/17 21:42:42:  ======================== New Round =============================
04/17 21:42:42:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/17 21:42:42:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/17 21:43:07:  #RES: f1:44.469, precision:46.145, recall:45.940, loss:0.795 at 1
04/17 21:43:07:  Best: f1:44.469, precision:46.145, recall:45.940, loss:0.795 at 1
04/17 21:43:07:  ---------
04/17 21:49:05:  ======================== New Round =============================
04/17 21:49:05:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/17 21:49:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/18 15:58:22:  ======================== New Round =============================
04/18 15:58:22:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/18 15:58:22:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/18 16:01:41:  ======================== New Round =============================
04/18 16:01:41:  2015, add_gan:False, add_gan_loss: False, add_gpt: False, text_model deberta
04/18 16:01:41:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=False, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=True, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/18 16:02:06:  #RES: f1:68.274, precision:68.274, recall:68.274, loss:0.670 at 1
04/18 16:02:06:  Best: f1:68.274, precision:68.274, recall:68.274, loss:0.670 at 1
04/18 16:02:06:  ---------
04/18 16:02:31:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:02:31:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:02:31:  ---------
04/18 16:02:55:  #RES: f1:70.203, precision:70.203, recall:70.203, loss:0.705 at 5
04/18 16:02:55:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:02:55:  ---------
04/18 16:03:20:  #RES: f1:69.335, precision:69.335, recall:69.335, loss:0.906 at 7
04/18 16:03:20:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:03:20:  ---------
04/18 16:03:45:  #RES: f1:70.010, precision:70.010, recall:70.010, loss:0.824 at 9
04/18 16:03:45:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:03:45:  ---------
04/18 16:04:11:  #RES: f1:70.395, precision:70.395, recall:70.395, loss:0.939 at 11
04/18 16:04:11:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:04:11:  ---------
04/18 16:04:36:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:0.936 at 13
04/18 16:04:36:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:04:36:  ---------
04/18 16:05:01:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:0.947 at 15
04/18 16:05:01:  Best: f1:71.938, precision:71.938, recall:71.938, loss:0.623 at 3
04/18 16:05:01:  ---------
04/18 16:05:26:  #RES: f1:72.324, precision:72.324, recall:72.324, loss:1.000 at 17
04/18 16:05:26:  Best: f1:72.324, precision:72.324, recall:72.324, loss:1.000 at 17
04/18 16:05:26:  ---------
04/18 16:05:51:  #RES: f1:72.613, precision:72.613, recall:72.613, loss:0.949 at 19
04/18 16:05:51:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.949 at 19
04/18 16:05:51:  ---------
04/18 16:06:17:  #RES: f1:69.527, precision:69.527, recall:69.527, loss:1.079 at 21
04/18 16:06:17:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.949 at 19
04/18 16:06:17:  ---------
04/18 16:06:42:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.144 at 23
04/18 16:06:42:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.949 at 19
04/18 16:06:42:  ---------
04/18 16:07:07:  #RES: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:07:07:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:07:07:  ---------
04/18 16:07:32:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.098 at 27
04/18 16:07:32:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:07:32:  ---------
04/18 16:07:58:  #RES: f1:72.324, precision:72.324, recall:72.324, loss:1.235 at 29
04/18 16:07:58:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:07:58:  ---------
04/18 16:08:23:  #RES: f1:71.263, precision:71.263, recall:71.263, loss:1.202 at 31
04/18 16:08:23:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:08:23:  ---------
04/18 16:08:48:  #RES: f1:72.613, precision:72.613, recall:72.613, loss:1.214 at 33
04/18 16:08:48:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:08:48:  ---------
04/18 16:09:13:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:1.354 at 35
04/18 16:09:14:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:09:14:  ---------
04/18 16:09:39:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.194 at 37
04/18 16:09:39:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:09:39:  ---------
04/18 16:10:04:  #RES: f1:72.613, precision:72.613, recall:72.613, loss:1.395 at 39
04/18 16:10:04:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:10:04:  ---------
04/18 16:10:29:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.171 at 41
04/18 16:10:29:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:10:29:  ---------
04/18 16:10:55:  #RES: f1:72.228, precision:72.228, recall:72.228, loss:1.394 at 43
04/18 16:10:55:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:10:55:  ---------
04/18 16:11:20:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.384 at 45
04/18 16:11:20:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:11:20:  ---------
04/18 16:11:45:  #RES: f1:70.781, precision:70.781, recall:70.781, loss:1.377 at 47
04/18 16:11:45:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:11:45:  ---------
04/18 16:12:11:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.363 at 49
04/18 16:12:11:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:12:11:  ---------
04/18 16:12:36:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:1.354 at 51
04/18 16:12:36:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:12:36:  ---------
04/18 16:13:01:  #RES: f1:71.263, precision:71.263, recall:71.263, loss:1.450 at 53
04/18 16:13:01:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:13:01:  ---------
04/18 16:13:27:  #RES: f1:72.035, precision:72.035, recall:72.035, loss:1.408 at 55
04/18 16:13:27:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:13:27:  ---------
04/18 16:13:52:  #RES: f1:72.131, precision:72.131, recall:72.131, loss:1.336 at 57
04/18 16:13:52:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:13:52:  ---------
04/18 16:14:17:  #RES: f1:72.131, precision:72.131, recall:72.131, loss:1.418 at 59
04/18 16:14:17:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:14:17:  ---------
04/18 16:14:43:  #RES: f1:73.385, precision:73.385, recall:73.385, loss:1.403 at 61
04/18 16:14:43:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:14:43:  ---------
04/18 16:15:08:  #RES: f1:72.613, precision:72.613, recall:72.613, loss:1.408 at 63
04/18 16:15:08:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:15:08:  ---------
04/18 16:15:33:  #RES: f1:71.553, precision:71.553, recall:71.553, loss:1.405 at 65
04/18 16:15:33:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:15:33:  ---------
04/18 16:15:59:  #RES: f1:72.228, precision:72.228, recall:72.228, loss:1.344 at 67
04/18 16:15:59:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:15:59:  ---------
04/18 16:16:24:  #RES: f1:72.035, precision:72.035, recall:72.035, loss:1.521 at 69
04/18 16:16:24:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:16:24:  ---------
04/18 16:16:49:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:1.385 at 71
04/18 16:16:49:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:16:49:  ---------
04/18 16:17:15:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.293 at 73
04/18 16:17:15:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:17:15:  ---------
04/18 16:17:40:  #RES: f1:71.842, precision:71.842, recall:71.842, loss:1.397 at 75
04/18 16:17:40:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:17:40:  ---------
04/18 16:18:06:  #RES: f1:72.903, precision:72.903, recall:72.903, loss:1.410 at 77
04/18 16:18:06:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:18:06:  ---------
04/18 16:18:31:  #RES: f1:72.517, precision:72.517, recall:72.517, loss:1.387 at 79
04/18 16:18:31:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:18:31:  ---------
04/18 16:18:56:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:1.360 at 81
04/18 16:18:56:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:18:56:  ---------
04/18 16:19:22:  #RES: f1:72.517, precision:72.517, recall:72.517, loss:1.383 at 83
04/18 16:19:22:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:19:22:  ---------
04/18 16:19:47:  #RES: f1:71.842, precision:71.842, recall:71.842, loss:1.451 at 85
04/18 16:19:47:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:19:47:  ---------
04/18 16:20:13:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.512 at 87
04/18 16:20:13:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:20:13:  ---------
04/18 16:20:38:  #RES: f1:72.228, precision:72.228, recall:72.228, loss:1.462 at 89
04/18 16:20:38:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:20:38:  ---------
04/18 16:21:03:  #RES: f1:71.553, precision:71.553, recall:71.553, loss:1.477 at 91
04/18 16:21:03:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:21:03:  ---------
04/18 16:21:29:  #RES: f1:72.324, precision:72.324, recall:72.324, loss:1.458 at 93
04/18 16:21:29:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:21:29:  ---------
04/18 16:21:54:  #RES: f1:72.420, precision:72.420, recall:72.420, loss:1.430 at 95
04/18 16:21:54:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:21:54:  ---------
04/18 16:22:20:  #RES: f1:72.420, precision:72.420, recall:72.420, loss:1.441 at 97
04/18 16:22:20:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:22:20:  ---------
04/18 16:22:45:  #RES: f1:72.420, precision:72.420, recall:72.420, loss:1.444 at 99
04/18 16:22:45:  Best: f1:73.674, precision:73.674, recall:73.674, loss:1.177 at 25
04/18 16:22:45:  ---------
04/18 16:25:09:  ======================== New Round =============================
04/18 16:25:09:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/18 16:25:09:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=True, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/18 16:26:27:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:26:27:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:26:27:  ---------
04/18 16:27:46:  #RES: f1:68.563, precision:68.563, recall:68.563, loss:0.808 at 3
04/18 16:27:46:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:27:46:  ---------
04/18 16:29:05:  #RES: f1:68.949, precision:68.949, recall:68.949, loss:0.825 at 5
04/18 16:29:05:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:29:05:  ---------
04/18 16:30:24:  #RES: f1:70.010, precision:70.010, recall:70.010, loss:0.791 at 7
04/18 16:30:24:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:30:24:  ---------
04/18 16:31:44:  #RES: f1:68.756, precision:68.756, recall:68.756, loss:0.937 at 10
04/18 16:31:44:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:31:44:  ---------
04/18 16:33:04:  #RES: f1:68.852, precision:68.852, recall:68.852, loss:0.903 at 12
04/18 16:33:04:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:33:04:  ---------
04/18 16:34:24:  #RES: f1:68.852, precision:68.852, recall:68.852, loss:0.972 at 14
04/18 16:34:24:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:34:24:  ---------
04/18 16:35:43:  #RES: f1:70.974, precision:70.974, recall:70.974, loss:1.023 at 16
04/18 16:35:43:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:35:43:  ---------
04/18 16:37:03:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.022 at 18
04/18 16:37:03:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:37:03:  ---------
04/18 16:38:22:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.117 at 20
04/18 16:38:22:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:38:22:  ---------
04/18 16:39:42:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.322 at 22
04/18 16:39:42:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:39:42:  ---------
04/18 16:41:02:  #RES: f1:68.852, precision:68.852, recall:68.852, loss:1.167 at 24
04/18 16:41:02:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:41:02:  ---------
04/18 16:42:22:  #RES: f1:69.720, precision:69.720, recall:69.720, loss:1.182 at 26
04/18 16:42:22:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:42:22:  ---------
04/18 16:43:41:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.308 at 28
04/18 16:43:41:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:43:41:  ---------
04/18 16:45:01:  #RES: f1:72.710, precision:72.710, recall:72.710, loss:1.301 at 30
04/18 16:45:01:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:45:01:  ---------
04/18 16:46:21:  #RES: f1:72.324, precision:72.324, recall:72.324, loss:1.277 at 32
04/18 16:46:21:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:46:21:  ---------
04/18 16:47:40:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.326 at 34
04/18 16:47:40:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:47:40:  ---------
04/18 16:48:59:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.497 at 36
04/18 16:48:59:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:48:59:  ---------
04/18 16:50:19:  #RES: f1:72.035, precision:72.035, recall:72.035, loss:1.395 at 38
04/18 16:50:19:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:50:19:  ---------
04/18 16:51:39:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.829 at 40
04/18 16:51:39:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:51:39:  ---------
04/18 16:52:59:  #RES: f1:71.553, precision:71.553, recall:71.553, loss:1.457 at 42
04/18 16:52:59:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:52:59:  ---------
04/18 16:54:18:  #RES: f1:70.203, precision:70.203, recall:70.203, loss:1.466 at 44
04/18 16:54:18:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:54:18:  ---------
04/18 16:55:38:  #RES: f1:68.949, precision:68.949, recall:68.949, loss:1.408 at 46
04/18 16:55:38:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:55:38:  ---------
04/18 16:56:57:  #RES: f1:71.167, precision:71.167, recall:71.167, loss:1.614 at 48
04/18 16:56:57:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:56:57:  ---------
04/18 16:58:17:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.418 at 50
04/18 16:58:17:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:58:17:  ---------
04/18 16:59:36:  #RES: f1:71.456, precision:71.456, recall:71.456, loss:1.588 at 52
04/18 16:59:36:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 16:59:36:  ---------
04/18 17:00:56:  #RES: f1:70.492, precision:70.492, recall:70.492, loss:1.491 at 54
04/18 17:00:56:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:00:56:  ---------
04/18 17:02:15:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.594 at 56
04/18 17:02:15:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:02:15:  ---------
04/18 17:03:34:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.561 at 58
04/18 17:03:34:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:03:34:  ---------
04/18 17:04:54:  #RES: f1:70.974, precision:70.974, recall:70.974, loss:1.642 at 60
04/18 17:04:54:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:04:54:  ---------
04/18 17:06:13:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.645 at 62
04/18 17:06:13:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:06:13:  ---------
04/18 17:07:32:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.721 at 64
04/18 17:07:32:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:07:32:  ---------
04/18 17:08:52:  #RES: f1:71.456, precision:71.456, recall:71.456, loss:1.644 at 66
04/18 17:08:52:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:08:52:  ---------
04/18 17:10:12:  #RES: f1:70.395, precision:70.395, recall:70.395, loss:1.742 at 68
04/18 17:10:12:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:10:12:  ---------
04/18 17:11:31:  #RES: f1:70.588, precision:70.588, recall:70.588, loss:1.556 at 70
04/18 17:11:31:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:11:31:  ---------
04/18 17:12:51:  #RES: f1:69.624, precision:69.624, recall:69.624, loss:1.667 at 72
04/18 17:12:51:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:12:51:  ---------
04/18 17:14:10:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.491 at 74
04/18 17:14:10:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:14:10:  ---------
04/18 17:15:29:  #RES: f1:70.395, precision:70.395, recall:70.395, loss:1.640 at 76
04/18 17:15:29:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:15:29:  ---------
04/18 17:16:49:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.540 at 78
04/18 17:16:49:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:16:49:  ---------
04/18 17:18:09:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.488 at 80
04/18 17:18:09:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:18:09:  ---------
04/18 17:19:29:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.622 at 82
04/18 17:19:29:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:19:29:  ---------
04/18 17:20:49:  #RES: f1:71.553, precision:71.553, recall:71.553, loss:1.573 at 84
04/18 17:20:49:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:20:49:  ---------
04/18 17:22:08:  #RES: f1:70.974, precision:70.974, recall:70.974, loss:1.607 at 86
04/18 17:22:08:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:22:08:  ---------
04/18 17:23:27:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:1.664 at 88
04/18 17:23:27:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:23:27:  ---------
04/18 17:24:47:  #RES: f1:70.395, precision:70.395, recall:70.395, loss:1.662 at 90
04/18 17:24:47:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:24:47:  ---------
04/18 17:26:06:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.610 at 92
04/18 17:26:06:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:26:06:  ---------
04/18 17:27:26:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:1.561 at 94
04/18 17:27:26:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:27:26:  ---------
04/18 17:28:45:  #RES: f1:70.974, precision:70.974, recall:70.974, loss:1.577 at 96
04/18 17:28:45:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:28:45:  ---------
04/18 17:30:05:  #RES: f1:70.588, precision:70.588, recall:70.588, loss:1.617 at 98
04/18 17:30:05:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/18 17:30:05:  ---------
04/21 16:26:30:  ======================== New Round =============================
04/21 16:26:30:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 16:26:30:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=True, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 16:27:47:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/21 16:27:47:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/21 16:27:47:  ---------
04/21 16:29:06:  #RES: f1:68.563, precision:68.563, recall:68.563, loss:0.808 at 3
04/21 16:29:06:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/21 16:29:06:  ---------
04/21 16:30:25:  #RES: f1:68.949, precision:68.949, recall:68.949, loss:0.825 at 5
04/21 16:30:25:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/21 16:30:25:  ---------
04/21 16:31:44:  #RES: f1:70.010, precision:70.010, recall:70.010, loss:0.791 at 7
04/21 16:31:44:  Best: f1:72.999, precision:72.999, recall:72.999, loss:0.634 at 1
04/21 16:31:44:  ---------
04/21 16:35:00:  ======================== New Round =============================
04/21 16:35:00:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 16:35:00:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 16:36:29:  #RES: f1:70.203, precision:70.203, recall:70.203, loss:0.623 at 1
04/21 16:36:29:  Best: f1:70.203, precision:70.203, recall:70.203, loss:0.623 at 1
04/21 16:36:29:  ---------
04/21 16:37:58:  #RES: f1:72.420, precision:72.420, recall:72.420, loss:0.658 at 3
04/21 16:37:58:  Best: f1:72.420, precision:72.420, recall:72.420, loss:0.658 at 3
04/21 16:37:58:  ---------
04/21 16:39:28:  #RES: f1:72.131, precision:72.131, recall:72.131, loss:0.739 at 5
04/21 16:39:28:  Best: f1:72.420, precision:72.420, recall:72.420, loss:0.658 at 3
04/21 16:39:28:  ---------
04/21 16:40:58:  #RES: f1:69.045, precision:69.045, recall:69.045, loss:0.911 at 7
04/21 16:40:58:  Best: f1:72.420, precision:72.420, recall:72.420, loss:0.658 at 3
04/21 16:40:58:  ---------
04/21 16:42:28:  #RES: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:42:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:42:28:  ---------
04/21 16:43:58:  #RES: f1:69.045, precision:69.045, recall:69.045, loss:0.914 at 12
04/21 16:43:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:43:58:  ---------
04/21 16:45:28:  #RES: f1:70.588, precision:70.588, recall:70.588, loss:1.218 at 14
04/21 16:45:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:45:28:  ---------
04/21 16:46:58:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:1.211 at 16
04/21 16:46:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:46:58:  ---------
04/21 16:48:28:  #RES: f1:73.192, precision:73.192, recall:73.192, loss:1.011 at 18
04/21 16:48:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:48:28:  ---------
04/21 16:49:58:  #RES: f1:72.710, precision:72.710, recall:72.710, loss:1.290 at 20
04/21 16:49:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:49:58:  ---------
04/21 16:51:28:  #RES: f1:71.456, precision:71.456, recall:71.456, loss:1.642 at 22
04/21 16:51:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:51:28:  ---------
04/21 16:52:58:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:1.574 at 24
04/21 16:52:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:52:58:  ---------
04/21 16:54:28:  #RES: f1:73.674, precision:73.674, recall:73.674, loss:1.613 at 26
04/21 16:54:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:54:28:  ---------
04/21 16:55:58:  #RES: f1:72.710, precision:72.710, recall:72.710, loss:1.816 at 28
04/21 16:55:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:55:58:  ---------
04/21 16:57:28:  #RES: f1:73.385, precision:73.385, recall:73.385, loss:1.882 at 30
04/21 16:57:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:57:28:  ---------
04/21 16:58:58:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.110 at 32
04/21 16:58:58:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 16:58:58:  ---------
04/21 17:00:28:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.134 at 34
04/21 17:00:28:  Best: f1:73.770, precision:73.770, recall:73.770, loss:0.712 at 10
04/21 17:00:28:  ---------
04/21 17:01:58:  #RES: f1:73.867, precision:73.867, recall:73.867, loss:2.115 at 36
04/21 17:01:58:  Best: f1:73.867, precision:73.867, recall:73.867, loss:2.115 at 36
04/21 17:01:58:  ---------
04/21 17:03:28:  #RES: f1:73.770, precision:73.770, recall:73.770, loss:2.250 at 38
04/21 17:03:28:  Best: f1:73.867, precision:73.867, recall:73.867, loss:2.115 at 36
04/21 17:03:28:  ---------
04/21 17:04:58:  #RES: f1:74.735, precision:74.735, recall:74.735, loss:2.310 at 40
04/21 17:04:58:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.310 at 40
04/21 17:04:58:  ---------
04/21 17:06:28:  #RES: f1:73.192, precision:73.192, recall:73.192, loss:2.524 at 42
04/21 17:06:28:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.310 at 40
04/21 17:06:28:  ---------
04/21 17:07:58:  #RES: f1:74.735, precision:74.735, recall:74.735, loss:2.331 at 44
04/21 17:07:58:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.331 at 44
04/21 17:07:58:  ---------
04/21 17:09:28:  #RES: f1:73.963, precision:73.963, recall:73.963, loss:2.291 at 46
04/21 17:09:28:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.331 at 44
04/21 17:09:28:  ---------
04/21 17:10:58:  #RES: f1:74.349, precision:74.349, recall:74.349, loss:2.309 at 48
04/21 17:10:58:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.331 at 44
04/21 17:10:58:  ---------
04/21 17:12:28:  #RES: f1:75.024, precision:75.024, recall:75.024, loss:2.458 at 50
04/21 17:12:28:  Best: f1:75.024, precision:75.024, recall:75.024, loss:2.458 at 50
04/21 17:12:28:  ---------
04/21 17:13:58:  #RES: f1:74.446, precision:74.446, recall:74.446, loss:2.261 at 52
04/21 17:13:58:  Best: f1:75.024, precision:75.024, recall:75.024, loss:2.458 at 50
04/21 17:13:58:  ---------
04/21 17:15:27:  #RES: f1:74.638, precision:74.638, recall:74.638, loss:2.257 at 54
04/21 17:15:27:  Best: f1:75.024, precision:75.024, recall:75.024, loss:2.458 at 50
04/21 17:15:27:  ---------
04/21 17:16:57:  #RES: f1:75.699, precision:75.699, recall:75.699, loss:2.466 at 56
04/21 17:16:57:  Best: f1:75.699, precision:75.699, recall:75.699, loss:2.466 at 56
04/21 17:16:57:  ---------
04/21 17:18:27:  #RES: f1:75.121, precision:75.121, recall:75.121, loss:2.398 at 58
04/21 17:18:27:  Best: f1:75.699, precision:75.699, recall:75.699, loss:2.466 at 56
04/21 17:18:27:  ---------
04/21 17:19:57:  #RES: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:19:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:19:57:  ---------
04/21 17:21:27:  #RES: f1:74.928, precision:74.928, recall:74.928, loss:2.697 at 62
04/21 17:21:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:21:27:  ---------
04/21 17:22:57:  #RES: f1:74.156, precision:74.156, recall:74.156, loss:2.252 at 64
04/21 17:22:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:22:57:  ---------
04/21 17:24:27:  #RES: f1:74.349, precision:74.349, recall:74.349, loss:2.629 at 66
04/21 17:24:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:24:27:  ---------
04/21 17:25:57:  #RES: f1:74.638, precision:74.638, recall:74.638, loss:2.714 at 68
04/21 17:25:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:25:57:  ---------
04/21 17:27:27:  #RES: f1:75.410, precision:75.410, recall:75.410, loss:2.412 at 70
04/21 17:27:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:27:27:  ---------
04/21 17:28:56:  #RES: f1:75.024, precision:75.024, recall:75.024, loss:2.495 at 72
04/21 17:28:56:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:28:56:  ---------
04/21 17:30:26:  #RES: f1:75.121, precision:75.121, recall:75.121, loss:2.446 at 74
04/21 17:30:26:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:30:26:  ---------
04/21 17:31:56:  #RES: f1:75.603, precision:75.603, recall:75.603, loss:2.438 at 76
04/21 17:31:56:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:31:56:  ---------
04/21 17:33:27:  #RES: f1:75.313, precision:75.313, recall:75.313, loss:2.476 at 78
04/21 17:33:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:33:27:  ---------
04/21 17:34:57:  #RES: f1:74.638, precision:74.638, recall:74.638, loss:2.536 at 80
04/21 17:34:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:34:57:  ---------
04/21 17:36:27:  #RES: f1:75.121, precision:75.121, recall:75.121, loss:2.598 at 82
04/21 17:36:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:36:27:  ---------
04/21 17:37:57:  #RES: f1:74.928, precision:74.928, recall:74.928, loss:2.604 at 84
04/21 17:37:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:37:57:  ---------
04/21 17:39:27:  #RES: f1:75.506, precision:75.506, recall:75.506, loss:2.582 at 86
04/21 17:39:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:39:27:  ---------
04/21 17:40:56:  #RES: f1:75.506, precision:75.506, recall:75.506, loss:2.572 at 88
04/21 17:40:56:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:40:56:  ---------
04/21 17:42:27:  #RES: f1:74.831, precision:74.831, recall:74.831, loss:2.493 at 90
04/21 17:42:27:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:42:27:  ---------
04/21 17:43:57:  #RES: f1:75.121, precision:75.121, recall:75.121, loss:2.474 at 92
04/21 17:43:57:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:43:57:  ---------
04/21 17:45:26:  #RES: f1:75.217, precision:75.217, recall:75.217, loss:2.471 at 94
04/21 17:45:26:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:45:26:  ---------
04/21 17:46:56:  #RES: f1:75.313, precision:75.313, recall:75.313, loss:2.501 at 96
04/21 17:46:56:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:46:56:  ---------
04/21 17:48:26:  #RES: f1:75.506, precision:75.506, recall:75.506, loss:2.514 at 98
04/21 17:48:26:  Best: f1:75.892, precision:75.892, recall:75.892, loss:2.482 at 60
04/21 17:48:26:  ---------
04/21 17:54:03:  ======================== New Round =============================
04/21 17:54:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 17:54:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=10, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 17:54:15:  ======================== New Round =============================
04/21 17:54:15:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 17:54:15:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=20, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 17:54:26:  ======================== New Round =============================
04/21 17:54:26:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 17:54:26:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 17:55:38:  ======================== New Round =============================
04/21 17:55:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 17:55:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 17:57:08:  #RES: f1:70.203, precision:70.203, recall:70.203, loss:0.623 at 1
04/21 17:57:08:  Best: f1:70.203, precision:70.203, recall:70.203, loss:0.623 at 1
04/21 17:57:08:  ---------
04/21 17:58:38:  ======================== New Round =============================
04/21 17:58:38:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 17:58:38:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 18:00:07:  #RES: f1:68.177, precision:68.177, recall:68.177, loss:0.651 at 1
04/21 18:00:07:  Best: f1:68.177, precision:68.177, recall:68.177, loss:0.651 at 1
04/21 18:00:07:  ---------
04/21 18:01:36:  #RES: f1:69.913, precision:69.913, recall:69.913, loss:0.742 at 3
04/21 18:01:36:  Best: f1:69.913, precision:69.913, recall:69.913, loss:0.742 at 3
04/21 18:01:36:  ---------
04/21 18:03:06:  #RES: f1:70.685, precision:70.685, recall:70.685, loss:nan at 5
04/21 18:03:06:  Best: f1:70.685, precision:70.685, recall:70.685, loss:nan at 5
04/21 18:03:06:  ---------
04/21 18:04:36:  #RES: f1:69.624, precision:69.624, recall:69.624, loss:0.805 at 7
04/21 18:04:36:  Best: f1:70.685, precision:70.685, recall:70.685, loss:nan at 5
04/21 18:04:36:  ---------
04/21 18:52:22:  ======================== New Round =============================
04/21 18:52:22:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/21 18:52:22:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=True, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/21 18:53:52:  #RES: f1:71.070, precision:71.070, recall:71.070, loss:0.619 at 1
04/21 18:53:52:  Best: f1:71.070, precision:71.070, recall:71.070, loss:0.619 at 1
04/21 18:53:52:  ---------
04/21 18:55:21:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:0.662 at 3
04/21 18:55:21:  Best: f1:71.360, precision:71.360, recall:71.360, loss:0.662 at 3
04/21 18:55:21:  ---------
04/21 18:56:52:  #RES: f1:72.324, precision:72.324, recall:72.324, loss:0.706 at 6
04/21 18:56:52:  Best: f1:72.324, precision:72.324, recall:72.324, loss:0.706 at 6
04/21 18:56:52:  ---------
04/21 18:58:21:  #RES: f1:72.613, precision:72.613, recall:72.613, loss:0.767 at 8
04/21 18:58:21:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.767 at 8
04/21 18:58:21:  ---------
04/21 18:59:51:  #RES: f1:70.588, precision:70.588, recall:70.588, loss:0.827 at 10
04/21 18:59:51:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.767 at 8
04/21 18:59:51:  ---------
04/21 19:01:22:  #RES: f1:70.203, precision:70.203, recall:70.203, loss:0.771 at 12
04/21 19:01:22:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.767 at 8
04/21 19:01:22:  ---------
04/21 19:02:52:  #RES: f1:70.781, precision:70.781, recall:70.781, loss:0.787 at 14
04/21 19:02:52:  Best: f1:72.613, precision:72.613, recall:72.613, loss:0.767 at 8
04/21 19:02:52:  ---------
04/21 19:04:22:  #RES: f1:73.963, precision:73.963, recall:73.963, loss:0.930 at 16
04/21 19:04:22:  Best: f1:73.963, precision:73.963, recall:73.963, loss:0.930 at 16
04/21 19:04:22:  ---------
04/21 19:05:52:  #RES: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:05:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:05:52:  ---------
04/21 19:07:22:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:1.495 at 20
04/21 19:07:22:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:07:22:  ---------
04/21 19:08:52:  #RES: f1:74.156, precision:74.156, recall:74.156, loss:1.275 at 22
04/21 19:08:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:08:52:  ---------
04/21 19:10:22:  #RES: f1:71.360, precision:71.360, recall:71.360, loss:1.529 at 24
04/21 19:10:22:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:10:22:  ---------
04/21 19:11:52:  #RES: f1:71.263, precision:71.263, recall:71.263, loss:1.721 at 26
04/21 19:11:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:11:52:  ---------
04/21 19:13:22:  #RES: f1:67.888, precision:67.888, recall:67.888, loss:1.970 at 28
04/21 19:13:22:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:13:22:  ---------
04/21 19:14:52:  #RES: f1:72.131, precision:72.131, recall:72.131, loss:1.842 at 30
04/21 19:14:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:14:52:  ---------
04/21 19:16:22:  #RES: f1:71.263, precision:71.263, recall:71.263, loss:2.035 at 32
04/21 19:16:22:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:16:22:  ---------
04/21 19:17:52:  #RES: f1:71.842, precision:71.842, recall:71.842, loss:2.015 at 34
04/21 19:17:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:17:52:  ---------
04/21 19:19:22:  #RES: f1:72.228, precision:72.228, recall:72.228, loss:2.169 at 36
04/21 19:19:22:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:19:22:  ---------
04/21 19:20:52:  #RES: f1:73.867, precision:73.867, recall:73.867, loss:2.187 at 38
04/21 19:20:52:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:20:52:  ---------
04/21 19:22:21:  #RES: f1:73.867, precision:73.867, recall:73.867, loss:2.502 at 40
04/21 19:22:21:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:22:21:  ---------
04/21 19:23:51:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:2.392 at 42
04/21 19:23:51:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:23:51:  ---------
04/21 19:25:21:  #RES: f1:73.192, precision:73.192, recall:73.192, loss:2.510 at 44
04/21 19:25:21:  Best: f1:74.542, precision:74.542, recall:74.542, loss:1.039 at 18
04/21 19:25:21:  ---------
04/21 19:26:52:  #RES: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:26:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:26:52:  ---------
04/21 19:28:21:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.504 at 48
04/21 19:28:21:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:28:21:  ---------
04/21 19:29:51:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:2.541 at 50
04/21 19:29:51:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:29:51:  ---------
04/21 19:31:22:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:2.691 at 52
04/21 19:31:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:31:22:  ---------
04/21 19:32:52:  #RES: f1:71.745, precision:71.745, recall:71.745, loss:2.818 at 54
04/21 19:32:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:32:52:  ---------
04/21 19:34:22:  #RES: f1:74.253, precision:74.253, recall:74.253, loss:2.841 at 56
04/21 19:34:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:34:22:  ---------
04/21 19:35:52:  #RES: f1:73.385, precision:73.385, recall:73.385, loss:2.956 at 58
04/21 19:35:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:35:52:  ---------
04/21 19:37:22:  #RES: f1:74.156, precision:74.156, recall:74.156, loss:2.986 at 60
04/21 19:37:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:37:22:  ---------
04/21 19:38:52:  #RES: f1:73.770, precision:73.770, recall:73.770, loss:3.084 at 62
04/21 19:38:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:38:52:  ---------
04/21 19:40:22:  #RES: f1:74.253, precision:74.253, recall:74.253, loss:2.984 at 64
04/21 19:40:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:40:22:  ---------
04/21 19:41:52:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:2.744 at 66
04/21 19:41:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:41:52:  ---------
04/21 19:43:23:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:3.039 at 68
04/21 19:43:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:43:23:  ---------
04/21 19:44:52:  #RES: f1:73.095, precision:73.095, recall:73.095, loss:2.902 at 70
04/21 19:44:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:44:52:  ---------
04/21 19:46:22:  #RES: f1:71.938, precision:71.938, recall:71.938, loss:2.855 at 72
04/21 19:46:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:46:22:  ---------
04/21 19:47:52:  #RES: f1:72.517, precision:72.517, recall:72.517, loss:2.871 at 74
04/21 19:47:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:47:52:  ---------
04/21 19:49:23:  #RES: f1:72.903, precision:72.903, recall:72.903, loss:2.898 at 76
04/21 19:49:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:49:23:  ---------
04/21 19:50:53:  #RES: f1:73.288, precision:73.288, recall:73.288, loss:2.947 at 78
04/21 19:50:53:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:50:53:  ---------
04/21 19:52:23:  #RES: f1:72.806, precision:72.806, recall:72.806, loss:2.915 at 80
04/21 19:52:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:52:23:  ---------
04/21 19:53:53:  #RES: f1:73.481, precision:73.481, recall:73.481, loss:2.832 at 82
04/21 19:53:53:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:53:53:  ---------
04/21 19:55:23:  #RES: f1:73.192, precision:73.192, recall:73.192, loss:2.875 at 84
04/21 19:55:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:55:23:  ---------
04/21 19:56:53:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.891 at 86
04/21 19:56:53:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:56:53:  ---------
04/21 19:58:22:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.903 at 88
04/21 19:58:22:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:58:22:  ---------
04/21 19:59:52:  #RES: f1:73.578, precision:73.578, recall:73.578, loss:2.943 at 90
04/21 19:59:52:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 19:59:52:  ---------
04/21 20:01:23:  #RES: f1:72.903, precision:72.903, recall:72.903, loss:2.903 at 92
04/21 20:01:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 20:01:23:  ---------
04/21 20:02:53:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.898 at 94
04/21 20:02:53:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 20:02:53:  ---------
04/21 20:04:23:  #RES: f1:72.999, precision:72.999, recall:72.999, loss:2.910 at 96
04/21 20:04:23:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 20:04:23:  ---------
04/21 20:05:53:  #RES: f1:73.095, precision:73.095, recall:73.095, loss:2.900 at 98
04/21 20:05:53:  Best: f1:74.735, precision:74.735, recall:74.735, loss:2.527 at 46
04/21 20:05:53:  ---------
04/22 22:29:23:  ======================== New Round =============================
04/22 22:29:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/22 22:29:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', warmup_steps=100, weight_decay=0.01)
04/22 22:30:51:  #RES: f1:66.152, precision:66.152, recall:66.152, loss:0.681 at 1
04/22 22:30:51:  Best: f1:66.152, precision:66.152, recall:66.152, loss:0.681 at 1
04/22 22:30:51:  ---------
04/23 10:50:06:  ======================== New Round =============================
04/23 10:50:06:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 10:50:06:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 10:51:13:  ======================== New Round =============================
04/23 10:51:13:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 10:51:13:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 10:54:16:  ======================== New Round =============================
04/23 10:54:16:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 10:54:16:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 10:56:27:  ======================== New Round =============================
04/23 10:56:27:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 10:56:27:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:00:33:  ======================== New Round =============================
04/23 11:00:33:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:00:33:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:02:13:  ======================== New Round =============================
04/23 11:02:13:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:02:13:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:04:59:  ======================== New Round =============================
04/23 11:04:59:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:04:59:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:06:11:  ======================== New Round =============================
04/23 11:06:11:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:06:11:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:06:18:  #RES: f1:10.825, precision:10.825, recall:10.825, loss:2.109 at 0
04/23 11:06:18:  Best: f1:10.825, precision:10.825, recall:10.825, loss:2.109 at 0
04/23 11:06:18:  ---------
04/23 11:06:25:  #RES: f1:10.825, precision:10.825, recall:10.825, loss:2.076 at 0
04/23 11:06:25:  Best: f1:10.825, precision:10.825, recall:10.825, loss:2.076 at 0
04/23 11:06:25:  ---------
04/23 11:06:53:  ======================== New Round =============================
04/23 11:06:53:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:06:53:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=64, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:08:22:  #RES: f1:69.175, precision:69.175, recall:69.175, loss:0.636 at 1
04/23 11:08:22:  Best: f1:69.175, precision:69.175, recall:69.175, loss:0.636 at 1
04/23 11:08:22:  ---------
04/23 11:09:51:  #RES: f1:70.309, precision:70.309, recall:70.309, loss:0.723 at 3
04/23 11:09:51:  Best: f1:70.309, precision:70.309, recall:70.309, loss:0.723 at 3
04/23 11:09:51:  ---------
04/23 11:11:20:  #RES: f1:67.526, precision:67.526, recall:67.526, loss:0.783 at 5
04/23 11:11:20:  Best: f1:70.309, precision:70.309, recall:70.309, loss:0.723 at 3
04/23 11:11:20:  ---------
04/23 11:12:50:  #RES: f1:71.649, precision:71.649, recall:71.649, loss:0.752 at 8
04/23 11:12:50:  Best: f1:71.649, precision:71.649, recall:71.649, loss:0.752 at 8
04/23 11:12:50:  ---------
04/23 11:14:19:  #RES: f1:71.340, precision:71.340, recall:71.340, loss:0.703 at 10
04/23 11:14:19:  Best: f1:71.649, precision:71.649, recall:71.649, loss:0.752 at 8
04/23 11:14:19:  ---------
04/23 11:15:49:  #RES: f1:72.268, precision:72.268, recall:72.268, loss:0.795 at 12
04/23 11:15:49:  Best: f1:72.268, precision:72.268, recall:72.268, loss:0.795 at 12
04/23 11:15:49:  ---------
04/23 11:17:18:  #RES: f1:71.546, precision:71.546, recall:71.546, loss:0.969 at 14
04/23 11:17:18:  Best: f1:72.268, precision:72.268, recall:72.268, loss:0.795 at 12
04/23 11:17:18:  ---------
04/23 11:18:48:  #RES: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:18:48:  Best: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:18:48:  ---------
04/23 11:20:17:  #RES: f1:70.309, precision:70.309, recall:70.309, loss:1.152 at 18
04/23 11:20:17:  Best: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:20:17:  ---------
04/23 11:21:47:  #RES: f1:71.753, precision:71.753, recall:71.753, loss:1.207 at 20
04/23 11:21:47:  Best: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:21:47:  ---------
04/23 11:23:16:  #RES: f1:68.454, precision:68.454, recall:68.454, loss:1.575 at 22
04/23 11:23:16:  Best: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:23:16:  ---------
04/23 11:24:46:  #RES: f1:69.381, precision:69.381, recall:69.381, loss:1.472 at 24
04/23 11:24:46:  Best: f1:72.887, precision:72.887, recall:72.887, loss:0.975 at 16
04/23 11:24:46:  ---------
04/23 11:26:16:  #RES: f1:72.990, precision:72.990, recall:72.990, loss:1.707 at 26
04/23 11:26:16:  Best: f1:72.990, precision:72.990, recall:72.990, loss:1.707 at 26
04/23 11:26:16:  ---------
04/23 11:27:46:  #RES: f1:72.268, precision:72.268, recall:72.268, loss:1.703 at 28
04/23 11:27:46:  Best: f1:72.990, precision:72.990, recall:72.990, loss:1.707 at 26
04/23 11:27:46:  ---------
04/23 11:29:15:  #RES: f1:72.680, precision:72.680, recall:72.680, loss:1.826 at 30
04/23 11:29:15:  Best: f1:72.990, precision:72.990, recall:72.990, loss:1.707 at 26
04/23 11:29:15:  ---------
04/23 11:31:10:  #RES: f1:72.990, precision:72.990, recall:72.990, loss:2.065 at 32
04/23 11:31:10:  Best: f1:72.990, precision:72.990, recall:72.990, loss:2.065 at 32
04/23 11:31:10:  ---------
04/23 11:31:47:  ======================== New Round =============================
04/23 11:31:47:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 11:31:47:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=3), device_id='cuda:3', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 11:33:18:  #RES: f1:73.608, precision:73.608, recall:73.608, loss:2.135 at 34
04/23 11:33:18:  Best: f1:73.608, precision:73.608, recall:73.608, loss:2.135 at 34
04/23 11:33:18:  ---------
04/23 11:34:08:  #RES: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:34:08:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:34:08:  ---------
04/23 11:35:01:  #RES: f1:72.990, precision:72.990, recall:72.990, loss:2.176 at 36
04/23 11:35:01:  Best: f1:73.608, precision:73.608, recall:73.608, loss:2.135 at 34
04/23 11:35:01:  ---------
04/23 11:36:29:  #RES: f1:70.326, precision:70.326, recall:70.326, loss:1.137 at 9
04/23 11:36:29:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:36:29:  ---------
04/23 11:36:44:  #RES: f1:73.814, precision:73.814, recall:73.814, loss:2.287 at 38
04/23 11:36:44:  Best: f1:73.814, precision:73.814, recall:73.814, loss:2.287 at 38
04/23 11:36:44:  ---------
04/23 11:38:28:  #RES: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:38:28:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:38:28:  ---------
04/23 11:38:53:  #RES: f1:72.255, precision:72.255, recall:72.255, loss:1.398 at 13
04/23 11:38:53:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:38:53:  ---------
04/23 11:40:12:  #RES: f1:73.608, precision:73.608, recall:73.608, loss:2.376 at 42
04/23 11:40:12:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:40:12:  ---------
04/23 11:41:16:  #RES: f1:70.178, precision:70.178, recall:70.178, loss:1.928 at 18
04/23 11:41:16:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:41:16:  ---------
04/23 11:41:55:  #RES: f1:73.093, precision:73.093, recall:73.093, loss:2.536 at 44
04/23 11:41:55:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:41:55:  ---------
04/23 11:43:40:  #RES: f1:74.330, precision:74.330, recall:74.330, loss:2.591 at 46
04/23 11:43:40:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:43:40:  ---------
04/23 11:43:41:  #RES: f1:68.249, precision:68.249, recall:68.249, loss:2.224 at 22
04/23 11:43:41:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 11:43:41:  ---------
04/23 11:45:24:  #RES: f1:74.124, precision:74.124, recall:74.124, loss:2.603 at 48
04/23 11:45:24:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:45:24:  ---------
04/23 11:46:04:  #RES: f1:75.074, precision:75.074, recall:75.074, loss:1.619 at 27
04/23 11:46:04:  Best: f1:75.074, precision:75.074, recall:75.074, loss:1.619 at 27
04/23 11:46:04:  ---------
04/23 11:47:09:  #RES: f1:74.227, precision:74.227, recall:74.227, loss:2.685 at 50
04/23 11:47:09:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:47:09:  ---------
04/23 11:48:27:  #RES: f1:74.629, precision:74.629, recall:74.629, loss:2.081 at 31
04/23 11:48:27:  Best: f1:75.074, precision:75.074, recall:75.074, loss:1.619 at 27
04/23 11:48:27:  ---------
04/23 11:48:53:  #RES: f1:72.474, precision:72.474, recall:72.474, loss:2.464 at 52
04/23 11:48:53:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:48:53:  ---------
04/23 11:50:36:  #RES: f1:74.845, precision:74.845, recall:74.845, loss:2.619 at 54
04/23 11:50:36:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:50:36:  ---------
04/23 11:50:50:  #RES: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 11:50:50:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 11:50:50:  ---------
04/23 11:52:20:  #RES: f1:73.299, precision:73.299, recall:73.299, loss:2.577 at 56
04/23 11:52:20:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:52:20:  ---------
04/23 11:53:14:  #RES: f1:75.074, precision:75.074, recall:75.074, loss:2.161 at 41
04/23 11:53:14:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 11:53:14:  ---------
04/23 11:54:03:  #RES: f1:73.608, precision:73.608, recall:73.608, loss:2.429 at 58
04/23 11:54:03:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:54:03:  ---------
04/23 11:55:37:  #RES: f1:75.668, precision:75.668, recall:75.668, loss:2.222 at 45
04/23 11:55:37:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 11:55:37:  ---------
04/23 11:55:49:  #RES: f1:73.505, precision:73.505, recall:73.505, loss:2.517 at 60
04/23 11:55:49:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:55:49:  ---------
04/23 11:57:31:  #RES: f1:73.814, precision:73.814, recall:73.814, loss:2.504 at 62
04/23 11:57:31:  Best: f1:74.948, precision:74.948, recall:74.948, loss:2.280 at 40
04/23 11:57:31:  ---------
04/23 11:58:00:  #RES: f1:75.519, precision:75.519, recall:75.519, loss:2.244 at 50
04/23 11:58:00:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 11:58:00:  ---------
04/23 11:59:15:  #RES: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 11:59:15:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 11:59:15:  ---------
04/23 12:00:24:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.331 at 54
04/23 12:00:24:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:00:24:  ---------
04/23 12:01:00:  #RES: f1:73.711, precision:73.711, recall:73.711, loss:2.628 at 66
04/23 12:01:00:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:01:00:  ---------
04/23 12:02:44:  #RES: f1:74.124, precision:74.124, recall:74.124, loss:2.598 at 68
04/23 12:02:44:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:02:44:  ---------
04/23 12:02:47:  #RES: f1:74.481, precision:74.481, recall:74.481, loss:2.290 at 59
04/23 12:02:47:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:02:47:  ---------
04/23 12:04:28:  #RES: f1:74.536, precision:74.536, recall:74.536, loss:2.687 at 70
04/23 12:04:28:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:04:28:  ---------
04/23 12:05:10:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.354 at 63
04/23 12:05:10:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:05:10:  ---------
04/23 12:06:11:  #RES: f1:74.021, precision:74.021, recall:74.021, loss:2.652 at 72
04/23 12:06:11:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:06:11:  ---------
04/23 12:07:33:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.376 at 68
04/23 12:07:33:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:07:33:  ---------
04/23 12:07:55:  #RES: f1:74.948, precision:74.948, recall:74.948, loss:2.768 at 74
04/23 12:07:55:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:07:55:  ---------
04/23 12:09:40:  #RES: f1:74.536, precision:74.536, recall:74.536, loss:2.639 at 76
04/23 12:09:40:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:09:40:  ---------
04/23 12:09:57:  #RES: f1:75.371, precision:75.371, recall:75.371, loss:2.342 at 72
04/23 12:09:57:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:09:57:  ---------
04/23 12:11:24:  #RES: f1:73.711, precision:73.711, recall:73.711, loss:2.698 at 78
04/23 12:11:24:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:11:24:  ---------
04/23 12:12:21:  #RES: f1:75.964, precision:75.964, recall:75.964, loss:2.294 at 77
04/23 12:12:21:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:12:21:  ---------
04/23 12:13:08:  #RES: f1:73.711, precision:73.711, recall:73.711, loss:2.773 at 80
04/23 12:13:08:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:13:08:  ---------
04/23 12:14:43:  #RES: f1:75.816, precision:75.816, recall:75.816, loss:2.242 at 82
04/23 12:14:43:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:14:43:  ---------
04/23 12:14:51:  #RES: f1:73.402, precision:73.402, recall:73.402, loss:2.794 at 82
04/23 12:14:51:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:14:51:  ---------
04/23 12:16:35:  #RES: f1:74.536, precision:74.536, recall:74.536, loss:2.817 at 84
04/23 12:16:35:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:16:35:  ---------
04/23 12:17:07:  #RES: f1:75.964, precision:75.964, recall:75.964, loss:2.284 at 86
04/23 12:17:07:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:17:07:  ---------
04/23 12:18:19:  #RES: f1:73.918, precision:73.918, recall:73.918, loss:2.793 at 86
04/23 12:18:19:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:18:19:  ---------
04/23 12:19:29:  #RES: f1:75.964, precision:75.964, recall:75.964, loss:2.289 at 91
04/23 12:19:29:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.085 at 36
04/23 12:19:29:  ---------
04/23 12:20:03:  #RES: f1:73.505, precision:73.505, recall:73.505, loss:2.827 at 88
04/23 12:20:03:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:20:03:  ---------
04/23 12:21:46:  #RES: f1:73.505, precision:73.505, recall:73.505, loss:2.785 at 90
04/23 12:21:46:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:21:46:  ---------
04/23 12:21:55:  #RES: f1:76.261, precision:76.261, recall:76.261, loss:2.287 at 95
04/23 12:21:55:  Best: f1:76.261, precision:76.261, recall:76.261, loss:2.287 at 95
04/23 12:21:55:  ---------
04/23 12:23:32:  #RES: f1:74.227, precision:74.227, recall:74.227, loss:2.824 at 92
04/23 12:23:32:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:23:32:  ---------
04/23 12:25:06:  #RES: f1:74.227, precision:74.227, recall:74.227, loss:2.880 at 94
04/23 12:25:06:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:25:06:  ---------
04/23 12:26:36:  #RES: f1:74.639, precision:74.639, recall:74.639, loss:2.843 at 96
04/23 12:26:36:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:26:36:  ---------
04/23 12:28:05:  #RES: f1:74.433, precision:74.433, recall:74.433, loss:2.840 at 98
04/23 12:28:05:  Best: f1:75.258, precision:75.258, recall:75.258, loss:2.686 at 64
04/23 12:28:05:  ---------
04/23 14:25:25:  ======================== New Round =============================
04/23 14:25:25:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 14:25:25:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=3), device_id='cuda:3', enable_log=True, epochs=400, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 14:27:38:  #RES: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 14:27:38:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 14:27:38:  ---------
04/23 14:29:53:  #RES: f1:68.398, precision:68.398, recall:68.398, loss:1.292 at 9
04/23 14:29:53:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 14:29:53:  ---------
04/23 14:32:08:  #RES: f1:72.404, precision:72.404, recall:72.404, loss:1.189 at 13
04/23 14:32:08:  Best: f1:72.849, precision:72.849, recall:72.849, loss:0.566 at 4
04/23 14:32:08:  ---------
04/23 14:34:24:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:1.669 at 18
04/23 14:34:24:  Best: f1:72.997, precision:72.997, recall:72.997, loss:1.669 at 18
04/23 14:34:24:  ---------
04/23 14:36:40:  #RES: f1:71.068, precision:71.068, recall:71.068, loss:2.111 at 22
04/23 14:36:40:  Best: f1:72.997, precision:72.997, recall:72.997, loss:1.669 at 18
04/23 14:36:40:  ---------
04/23 14:38:55:  #RES: f1:64.392, precision:64.392, recall:64.392, loss:2.197 at 27
04/23 14:38:55:  Best: f1:72.997, precision:72.997, recall:72.997, loss:1.669 at 18
04/23 14:38:55:  ---------
04/23 14:41:11:  #RES: f1:71.810, precision:71.810, recall:71.810, loss:2.133 at 31
04/23 14:41:11:  Best: f1:72.997, precision:72.997, recall:72.997, loss:1.669 at 18
04/23 14:41:11:  ---------
04/23 14:43:27:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.294 at 36
04/23 14:43:27:  Best: f1:74.036, precision:74.036, recall:74.036, loss:2.294 at 36
04/23 14:43:27:  ---------
04/23 14:45:44:  #RES: f1:74.629, precision:74.629, recall:74.629, loss:1.885 at 41
04/23 14:45:44:  Best: f1:74.629, precision:74.629, recall:74.629, loss:1.885 at 41
04/23 14:45:44:  ---------
04/23 14:47:59:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:47:59:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:47:59:  ---------
04/23 14:50:14:  #RES: f1:72.552, precision:72.552, recall:72.552, loss:2.279 at 50
04/23 14:50:14:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:50:14:  ---------
04/23 14:52:30:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.412 at 54
04/23 14:52:30:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:52:30:  ---------
04/23 14:54:45:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.359 at 59
04/23 14:54:45:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:54:45:  ---------
04/23 14:57:00:  #RES: f1:72.700, precision:72.700, recall:72.700, loss:2.366 at 63
04/23 14:57:00:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:57:00:  ---------
04/23 14:59:15:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.207 at 68
04/23 14:59:15:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 14:59:15:  ---------
04/23 15:01:30:  #RES: f1:73.294, precision:73.294, recall:73.294, loss:2.275 at 72
04/23 15:01:30:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 15:01:30:  ---------
04/23 15:03:45:  #RES: f1:73.739, precision:73.739, recall:73.739, loss:2.151 at 77
04/23 15:03:45:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 15:03:45:  ---------
04/23 15:06:00:  #RES: f1:74.629, precision:74.629, recall:74.629, loss:2.210 at 82
04/23 15:06:00:  Best: f1:74.926, precision:74.926, recall:74.926, loss:2.021 at 45
04/23 15:06:00:  ---------
04/23 15:08:15:  #RES: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:08:15:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:08:15:  ---------
04/23 15:10:30:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.049 at 91
04/23 15:10:30:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:10:30:  ---------
04/23 15:12:46:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:2.307 at 95
04/23 15:12:46:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:12:46:  ---------
04/23 15:15:02:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.026 at 100
04/23 15:15:02:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:15:02:  ---------
04/23 15:17:17:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:1.922 at 104
04/23 15:17:17:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:17:17:  ---------
04/23 15:19:32:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.394 at 109
04/23 15:19:32:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:19:32:  ---------
04/23 15:21:47:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.172 at 113
04/23 15:21:47:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:21:47:  ---------
04/23 15:24:02:  #RES: f1:74.481, precision:74.481, recall:74.481, loss:2.187 at 118
04/23 15:24:02:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:24:02:  ---------
04/23 15:26:18:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.739 at 123
04/23 15:26:18:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:26:18:  ---------
04/23 15:28:33:  #RES: f1:74.629, precision:74.629, recall:74.629, loss:2.002 at 127
04/23 15:28:33:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:28:33:  ---------
04/23 15:30:47:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.292 at 132
04/23 15:30:47:  Best: f1:75.074, precision:75.074, recall:75.074, loss:2.141 at 86
04/23 15:30:47:  ---------
04/23 15:33:02:  #RES: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:33:02:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:33:02:  ---------
04/23 15:35:18:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.335 at 141
04/23 15:35:18:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:35:18:  ---------
04/23 15:37:33:  #RES: f1:72.552, precision:72.552, recall:72.552, loss:2.035 at 145
04/23 15:37:33:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:37:33:  ---------
04/23 15:39:48:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.086 at 150
04/23 15:39:48:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:39:48:  ---------
04/23 15:42:03:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.350 at 154
04/23 15:42:03:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:42:03:  ---------
04/23 15:44:18:  #RES: f1:73.294, precision:73.294, recall:73.294, loss:2.394 at 159
04/23 15:44:18:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:44:18:  ---------
04/23 15:46:34:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.478 at 164
04/23 15:46:34:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:46:34:  ---------
04/23 15:48:49:  #RES: f1:72.107, precision:72.107, recall:72.107, loss:2.738 at 168
04/23 15:48:49:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:48:49:  ---------
04/23 15:51:05:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:2.200 at 173
04/23 15:51:05:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:51:05:  ---------
04/23 15:53:20:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:2.440 at 177
04/23 15:53:20:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:53:20:  ---------
04/23 15:55:35:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.370 at 182
04/23 15:55:35:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:55:35:  ---------
04/23 15:57:50:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.318 at 186
04/23 15:57:50:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 15:57:50:  ---------
04/23 16:00:05:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.242 at 191
04/23 16:00:05:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:00:05:  ---------
04/23 16:02:21:  #RES: f1:74.481, precision:74.481, recall:74.481, loss:2.366 at 196
04/23 16:02:21:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:02:21:  ---------
04/23 16:04:36:  #RES: f1:72.404, precision:72.404, recall:72.404, loss:2.224 at 200
04/23 16:04:36:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:04:36:  ---------
04/23 16:06:52:  #RES: f1:73.294, precision:73.294, recall:73.294, loss:2.271 at 205
04/23 16:06:52:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:06:52:  ---------
04/23 16:09:08:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.226 at 209
04/23 16:09:08:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:09:08:  ---------
04/23 16:11:23:  #RES: f1:74.481, precision:74.481, recall:74.481, loss:2.056 at 214
04/23 16:11:23:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:11:23:  ---------
04/23 16:13:38:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.267 at 218
04/23 16:13:38:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:13:38:  ---------
04/23 16:15:52:  #RES: f1:72.700, precision:72.700, recall:72.700, loss:2.450 at 223
04/23 16:15:52:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:15:52:  ---------
04/23 16:18:07:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.399 at 227
04/23 16:18:07:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:18:07:  ---------
04/23 16:20:22:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.413 at 232
04/23 16:20:22:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:20:22:  ---------
04/23 16:22:37:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.515 at 237
04/23 16:22:37:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:22:37:  ---------
04/23 16:24:54:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.545 at 241
04/23 16:24:54:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:24:54:  ---------
04/23 16:27:09:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.659 at 246
04/23 16:27:09:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:27:09:  ---------
04/23 16:29:24:  #RES: f1:74.629, precision:74.629, recall:74.629, loss:2.443 at 250
04/23 16:29:24:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:29:24:  ---------
04/23 16:31:41:  #RES: f1:72.552, precision:72.552, recall:72.552, loss:2.621 at 255
04/23 16:31:41:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:31:41:  ---------
04/23 16:33:56:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.417 at 260
04/23 16:33:56:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:33:56:  ---------
04/23 16:36:12:  #RES: f1:72.255, precision:72.255, recall:72.255, loss:2.385 at 264
04/23 16:36:12:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:36:12:  ---------
04/23 16:38:28:  #RES: f1:72.700, precision:72.700, recall:72.700, loss:2.432 at 269
04/23 16:38:28:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:38:28:  ---------
04/23 16:40:43:  #RES: f1:73.294, precision:73.294, recall:73.294, loss:2.459 at 273
04/23 16:40:43:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:40:43:  ---------
04/23 16:42:59:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.451 at 278
04/23 16:42:59:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:42:59:  ---------
04/23 16:45:15:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.527 at 283
04/23 16:45:15:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:45:15:  ---------
04/23 16:47:31:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.635 at 287
04/23 16:47:31:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:47:31:  ---------
04/23 16:49:46:  #RES: f1:72.700, precision:72.700, recall:72.700, loss:2.641 at 292
04/23 16:49:46:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:49:46:  ---------
04/23 16:52:02:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.447 at 296
04/23 16:52:02:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:52:02:  ---------
04/23 16:54:17:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.475 at 301
04/23 16:54:17:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:54:17:  ---------
04/23 16:56:32:  #RES: f1:73.739, precision:73.739, recall:73.739, loss:2.501 at 305
04/23 16:56:32:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:56:32:  ---------
04/23 16:58:47:  #RES: f1:74.332, precision:74.332, recall:74.332, loss:2.503 at 310
04/23 16:58:47:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 16:58:47:  ---------
04/23 17:01:03:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.520 at 315
04/23 17:01:03:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:01:03:  ---------
04/23 17:03:19:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.605 at 319
04/23 17:03:19:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:03:19:  ---------
04/23 17:05:34:  #RES: f1:73.591, precision:73.591, recall:73.591, loss:2.601 at 324
04/23 17:05:34:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:05:34:  ---------
04/23 17:07:49:  #RES: f1:73.145, precision:73.145, recall:73.145, loss:2.653 at 328
04/23 17:07:49:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:07:49:  ---------
04/23 17:10:04:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:2.666 at 333
04/23 17:10:04:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:10:04:  ---------
04/23 17:12:19:  #RES: f1:72.849, precision:72.849, recall:72.849, loss:2.673 at 337
04/23 17:12:19:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:12:19:  ---------
04/23 17:14:34:  #RES: f1:72.997, precision:72.997, recall:72.997, loss:2.681 at 342
04/23 17:14:34:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:14:34:  ---------
04/23 17:16:49:  #RES: f1:72.849, precision:72.849, recall:72.849, loss:2.689 at 346
04/23 17:16:49:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:16:49:  ---------
04/23 17:19:05:  #RES: f1:73.294, precision:73.294, recall:73.294, loss:2.820 at 351
04/23 17:19:05:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:19:05:  ---------
04/23 17:21:19:  #RES: f1:74.481, precision:74.481, recall:74.481, loss:2.728 at 356
04/23 17:21:19:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:21:19:  ---------
04/23 17:23:34:  #RES: f1:73.442, precision:73.442, recall:73.442, loss:2.507 at 360
04/23 17:23:34:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:23:34:  ---------
04/23 17:25:49:  #RES: f1:73.887, precision:73.887, recall:73.887, loss:2.524 at 365
04/23 17:25:49:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:25:49:  ---------
04/23 17:28:05:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.535 at 369
04/23 17:28:05:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:28:05:  ---------
04/23 17:30:20:  #RES: f1:74.184, precision:74.184, recall:74.184, loss:2.545 at 374
04/23 17:30:20:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:30:20:  ---------
04/23 17:32:35:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.533 at 378
04/23 17:32:35:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:32:35:  ---------
04/23 17:34:51:  #RES: f1:74.036, precision:74.036, recall:74.036, loss:2.539 at 383
04/23 17:34:51:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:34:51:  ---------
04/23 17:37:06:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.498 at 388
04/23 17:37:06:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:37:06:  ---------
04/23 17:39:21:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.504 at 392
04/23 17:39:21:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:39:21:  ---------
04/23 17:41:37:  #RES: f1:74.926, precision:74.926, recall:74.926, loss:2.509 at 397
04/23 17:41:37:  Best: f1:75.223, precision:75.223, recall:75.223, loss:2.161 at 136
04/23 17:41:37:  ---------
04/23 17:43:03:  ======================== New Round =============================
04/23 17:43:03:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 17:43:03:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 17:44:23:  ======================== New Round =============================
04/23 17:44:23:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 17:44:23:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 17:50:37:  ======================== New Round =============================
04/23 17:50:37:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 17:50:37:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 17:51:45:  ======================== New Round =============================
04/23 17:51:45:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 17:51:45:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:09:13:  ======================== New Round =============================
04/23 18:09:13:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:09:13:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:12:05:  ======================== New Round =============================
04/23 18:12:05:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:12:05:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:13:07:  ======================== New Round =============================
04/23 18:13:07:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:13:07:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:15:50:  ======================== New Round =============================
04/23 18:15:50:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:15:50:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:16:53:  ======================== New Round =============================
04/23 18:16:53:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:16:53:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:18:40:  ======================== New Round =============================
04/23 18:18:40:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:18:40:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 18:56:04:  ======================== New Round =============================
04/23 18:56:04:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 18:56:04:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=1, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 19:09:18:  ======================== New Round =============================
04/23 19:09:18:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 19:09:18:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 19:11:26:  #RES: f1:6746.388, precision:6979.381, recall:6528.447, loss:0.603 at 2
04/23 19:11:26:  Best: f1:6746.388, precision:6979.381, recall:6528.447, loss:0.603 at 2
04/23 19:11:26:  ---------
04/23 19:12:49:  ======================== New Round =============================
04/23 19:12:49:  2015, add_gan:True, add_gan_loss: False, add_gpt: False, text_model deberta
04/23 19:12:49:  Namespace(adam_epsilon=1e-08, add_cycle=False, add_gan=True, add_gan_loss=False, add_gpt=False, add_llm=False, alpha=0.6, aspect_epochs=100, batch_size=96, beta=0.6, cache_dir='cache', data_image_dir='../data/ImgData/twitter2015', data_text_dir='../data/twitter2015', dataset_type='2015', device=device(type='cuda', index=0), device_id='cuda:0', enable_log=True, epochs=100, gradient_accumulation_steps=1, image_model_name='clip', log_dir='log.log', logging_steps=500, lr=2e-05, max_grad_norm=1.0, max_steps=-1, name_path_dict={'bert': '../_weight/deberta', 'roberta': '../_weight/roberta-base', 'deberta': '../_weight/deberta', 'clip': 'openai/clip-vit-base-patch32', 'robertat': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'vit': '../_weight/vit-base-patch16-224-in21k', 'xlm': 'xlm-roberta-base', 'debertal': 'microsoft/deberta-v3-large', 'laion': 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'}, only_text_loss=True, output_dir='/data/results', output_result_file='/data/result.txt', random_seed=42, refresh_aspect=False, refresh_data=False, save_steps=100, task_name='dualc', text_model_name='deberta', text_num_labels=3, warmup_steps=100, weight_decay=0.01)
04/23 19:14:59:  #RES: f1:67.464, precision:69.794, recall:65.284, loss:0.603 at 2
04/23 19:14:59:  Best: f1:67.464, precision:69.794, recall:65.284, loss:0.603 at 2
04/23 19:14:59:  ---------
04/23 19:17:09:  #RES: f1:69.058, precision:71.443, recall:66.827, loss:0.661 at 5
04/23 19:17:09:  Best: f1:69.058, precision:71.443, recall:66.827, loss:0.661 at 5
04/23 19:17:09:  ---------
04/23 19:19:19:  #RES: f1:67.763, precision:70.103, recall:65.574, loss:0.745 at 8
04/23 19:19:19:  Best: f1:69.058, precision:71.443, recall:66.827, loss:0.661 at 5
04/23 19:19:19:  ---------
04/23 19:21:29:  #RES: f1:67.663, precision:70.000, recall:65.477, loss:0.778 at 11
04/23 19:21:29:  Best: f1:69.058, precision:71.443, recall:66.827, loss:0.661 at 5
04/23 19:21:29:  ---------
04/23 19:23:39:  #RES: f1:63.777, precision:65.979, recall:61.716, loss:1.070 at 14
04/23 19:23:39:  Best: f1:69.058, precision:71.443, recall:66.827, loss:0.661 at 5
04/23 19:23:39:  ---------
04/23 19:25:48:  #RES: f1:69.158, precision:71.546, recall:66.924, loss:1.009 at 17
04/23 19:25:48:  Best: f1:69.158, precision:71.546, recall:66.924, loss:1.009 at 17
04/23 19:25:48:  ---------
04/23 19:27:58:  #RES: f1:70.254, precision:72.680, recall:67.985, loss:1.048 at 20
04/23 19:27:58:  Best: f1:70.254, precision:72.680, recall:67.985, loss:1.048 at 20
04/23 19:27:58:  ---------
04/23 19:30:07:  #RES: f1:70.154, precision:72.577, recall:67.888, loss:1.245 at 23
04/23 19:30:07:  Best: f1:70.254, precision:72.680, recall:67.985, loss:1.048 at 20
04/23 19:30:07:  ---------
04/23 19:32:16:  #RES: f1:70.055, precision:72.474, recall:67.792, loss:1.319 at 26
04/23 19:32:16:  Best: f1:70.254, precision:72.680, recall:67.985, loss:1.048 at 20
04/23 19:32:16:  ---------
04/23 19:34:25:  #RES: f1:70.852, precision:73.299, recall:68.563, loss:1.494 at 29
04/23 19:34:25:  Best: f1:70.852, precision:73.299, recall:68.563, loss:1.494 at 29
04/23 19:34:25:  ---------
04/23 19:36:35:  #RES: f1:69.357, precision:71.753, recall:67.117, loss:1.698 at 32
04/23 19:36:35:  Best: f1:70.852, precision:73.299, recall:68.563, loss:1.494 at 29
04/23 19:36:35:  ---------
04/23 19:38:46:  #RES: f1:69.457, precision:71.856, recall:67.213, loss:1.607 at 35
04/23 19:38:46:  Best: f1:70.852, precision:73.299, recall:68.563, loss:1.494 at 29
04/23 19:38:46:  ---------
04/23 19:40:56:  #RES: f1:69.258, precision:71.649, recall:67.020, loss:1.669 at 38
04/23 19:40:56:  Best: f1:70.852, precision:73.299, recall:68.563, loss:1.494 at 29
04/23 19:40:56:  ---------
