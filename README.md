https://arxiv.org/abs/2406.00017

Our method based on public datasets: Twitter15 and Twitter17. All the data and checkpoint could be accessed by [GoogleDrive](https://drive.google.com/drive/folders/1naG4UEymBiVYGHQR9yGMVNa7fXyE_7HO?usp=drive_link) Download the checkpoint, including sentiment_2015.pt and predict_2015.pt, and put it at PTA/cache/2015 for usage.

The encoder of text and image are Deberta-base and ViT, which could be accessed by huggingface.

Any problem, feel free to contact us betterszsong@gmail.com.


Citation
```
@ARTICLE{2024arXiv240600017S,
       author = {{Song}, Shezheng and {Li}, Shasha and {Zhao}, Shan and {Wang}, Chengyu and {Li}, Xiaopeng and {Yu}, Jie and {Wan}, Qian and {Ma}, Jun and {Yan}, Tianwei and {Ma}, Wentao and {Mao}, Xiaoguang},
        title = "{PTA: Enhancing Multimodal Sentiment Analysis through Pipelined Prediction and Translation-based Alignment}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multimedia},
         year = 2024,
        month = may,
          eid = {arXiv:2406.00017},
        pages = {arXiv:2406.00017},
          doi = {10.48550/arXiv.2406.00017},
archivePrefix = {arXiv},
       eprint = {2406.00017},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240600017S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


```
